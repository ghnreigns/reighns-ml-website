{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18de9b4e-6f8d-4a2a-a016-5cfd4bb81de8",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\F}{\\mathbb{F}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\v}{\\mathbf{v}}\n",
    "\\newcommand{\\a}{\\mathbf{a}}\n",
    "\\newcommand{\\b}{\\mathbf{b}}\n",
    "\\newcommand{\\c}{\\mathbf{c}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\u}{\\mathbf{u}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\1}{\\mathbf{1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfcb049-5b1c-4f2f-aa7e-6eb2f198fbfb",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Learning Objectives](#1)\n",
    "* [Vector Multiplications](#2)\n",
    "    * [Dot Product](#11)\n",
    "        * [Algebraic Definition](#11)\n",
    "        * [Geometric Definition](#11)\n",
    "        * [Properties of Dot Product](#11)\n",
    "        * [Law of Cosine](#11)\n",
    "        * [Cauchy-Schwarz Inequality](#11)\n",
    "    * [Unit Vector](#11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c57caa-a66a-421c-a637-ea9bbd354557",
   "metadata": {},
   "source": [
    "!!! summary \"Learning Objectives\"\n",
    "    - Definition of a Field\n",
    "    - Definition of a Vector\n",
    "        - Vector Operations with both Algebraic and Geometric understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f53d8-f995-404e-b51d-2e978d0d7b0f",
   "metadata": {},
   "source": [
    "## Vector Multiplications\n",
    "\n",
    "This section introduces one of the most important idea in Linear Algebra, the **Dot Product**. Since [Wikipedia](https://en.wikipedia.org/wiki/Dot_product)^[Dot_product] has a wholesome introduction, we will be copying over some definitions from it.\n",
    "\n",
    "\n",
    "[^Dot_product]: https://en.wikipedia.org/wiki/Dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9251313-d005-4423-9b53-42a1bb716591",
   "metadata": {},
   "source": [
    "### Dot Product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead711b9-062f-42a0-b47c-d812083e3031",
   "metadata": {},
   "source": [
    "#### Algebraic Definition (Dot Product)\n",
    "\n",
    "!!! success \"Definition\"\n",
    "    The dot product of two vectors $\\color{red}{\\a =  \\begin{bmatrix} a_1  \\; a_2  \\; \\dots \\; a_n \\end{bmatrix}^{\\rm T}}$ and \n",
    "    $\\color{blue}{\\b =  \\begin{bmatrix} b_1 & b_2  & \\dots & b_n \\end{bmatrix}^{\\rm T}}$ is defined as:\n",
    "\n",
    "    $$\\mathbf{\\color{red}\\a}\\cdot\\mathbf{\\color{blue}\\b}=\\sum_{i=1}^n {\\color{red}a}_i{\\color{blue}b}_i={\\color{red}a}_1{\\color{blue}b}_1+{\\color{red}a}_2{\\color{blue}b}_2+\\cdots+{\\color{red}a}_n{\\color{blue}b}_n$$\n",
    "\n",
    "    where $\\sum$ denotes summation and $n$ is the dimension of the vector space. Since **vector spaces** have not been introduced, we just think of it as the $\\R^n$ dimensional space. \n",
    "\n",
    "##### Example of Dot Product\n",
    "\n",
    "!!! example\n",
    "    For instance, in 3-dimensional space, the **dot product** of column vectors $\\begin{bmatrix}1 & 3 & -5\\end{bmatrix}^{\\rm T}$ and $\\begin{bmatrix}4 & -2 & -2\\end{bmatrix}^{\\rm T}$\n",
    "\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\ [{\\color{red}1, 3, -5}] \\cdot  [{\\color{blue}4, -2, -1}] &= ({\\color{red}1} \\times {\\color{blue}4}) + ({\\color{red}3}\\times{\\color{blue}-2}) + ({\\color{red}-5}\\times{\\color{blue}-1}) \\\\\n",
    "    &= 4 - 6 + 5 \\\\\n",
    "    &= 3\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "!!! info \"Vector as Matrices\"\n",
    "\n",
    "    We are a little ahead in terms of the definition of Matrices, but for people familiar with it, or have worked with `numpy` before, we know that we can interpret a row vector of dimension $n$ as a matrix of dimension $1 \\times n$. Similarly, we can interpret a column vector of dimension $n$ as a matrix of dimension $n \\times 1$. With this interpretation, we can perform a so called \"matrix multiplication\" of the row vector and column vector. The result is the dot product. We will go in details when we get to it.\n",
    "\n",
    "    If vectors are treated like row matrices, the dot product can also be written as a matrix multiplication.\n",
    "\n",
    "    $$\\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} = \\mathbf{\\color{red}a}^\\mathsf T \\mathbf{\\color{blue}b}$$\n",
    "\n",
    "    Expressing the above example in this way, a 1 × 3 matrix **row vector** is multiplied by a 3 × 1 matrix **column vector** to get a 1 × 1 matrix that is identified with its unique entry:\n",
    "    $$\n",
    "      \\begin{bmatrix}\n",
    "       \\color{red}1 & \\color{red}3 & \\color{red}-5\n",
    "      \\end{bmatrix}\n",
    "      \\begin{bmatrix}\n",
    "       \\color{blue}4 \\\\ \\color{blue}-2 \\\\ \\color{blue}-1\n",
    "      \\end{bmatrix} = \\color{purple}3\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8eabd1-5093-49ea-b810-7b3cb8c46eda",
   "metadata": {},
   "source": [
    "#### DOT Product (Geometric definition)\n",
    "\n",
    "!!! success \"Definition\"\n",
    "    In [Euclidean space](Euclidean_space \"wikilink\"), a [Euclidean vector](Euclidean_vector \"wikilink\") is a geometric object that possesses both a magnitude and a direction. A vector can be pictured as\n",
    "    an arrow. Its magnitude is its length, and its direction is the direction to which the arrow points. The magnitude of a vector **a** is denoted by $\\left\\| \\mathbf{a} \\right\\|$. The dot product of two\n",
    "    Euclidean vectors **a** and **b** is defined by $$\\mathbf{a}\\cdot\\mathbf{b}=\\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta ,$$\n",
    "    where $\\theta$ is the angle between $\\a$ and $\\b$.\n",
    "\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/linear_algebra/linear_algebra_theory_intuition_code_chap3_fig_3.1_scalar_projection_and_dot_product.PNG\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Fig 3.11: Diagram of Scalar Projection and DOT Product; By Hongnan G.</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41836dac-e5fb-4d7c-8052-820124a8cebc",
   "metadata": {},
   "source": [
    "##### Scalar projections\n",
    "\n",
    "TODO: To motivate the geometric interpretation, we should see the example on scalar projections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e103b15b-4d4d-4d89-9658-3a4a845a5f1c",
   "metadata": {},
   "source": [
    "##### Sign of the DOT Product is determined by the Angle in between the two vectors\n",
    "\n",
    "!!! note \"Refactor Geometric Formula\"\n",
    "\n",
    "The geometric definition can be re-written as follows:\n",
    "\n",
    "\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "\\mathbf{a}\\cdot\\mathbf{b} &=\\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta \\implies \\cos(\\theta) = \\frac{\\a^\\top \\b}{\\|\\a\\| \\|\\b\\|} \\implies \\theta = \\cos^{-1}\\left(\\frac{\\a^\\top \\b}{\\|\\a\\| \\|\\b\\|}\\right)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "which essentially means that one can find the angle between two known vectors in any dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de6bd08-e99c-4f16-86c7-181c7de828e6",
   "metadata": {},
   "source": [
    Mike X Cohen explains in **Linear Algebra: Theory, Intuition, Code, 2021. (pp. 51-52)** how the **sign** of the dot product is determined solely by the angle between the two vectors. **By definition**, $\\mathbf{a}\\cdot\\mathbf{b} = \\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta$, we know that the sign (positive or negative) of the dot product $\\a \\cdot \\b$ is solely determined by $\\cos \\theta$ since $\\|\\a\\| \\|\\b\\|$ is always positive.\n",
    "\n",
    "- **Case 1 ($0< \\theta < 90$): This implies that $\\cos \\theta > 0 \\implies \\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta > 0 \\implies \\mathbf{a}\\cdot\\mathbf{b} > 0$.**\n",
    "- **Case 2 ($90 < \\theta < 180$): This implies that $\\cos \\theta < 0 \\implies \\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta < 0 \\implies \\mathbf{a}\\cdot\\mathbf{b} < 0$.**\n",
    "- **Case 3 ($\\theta = 90$): This is an important property, for now, we just need to know that since $\\cos \\theta = 0$, then $\\a \\cdot \\b = \\0$. These two vectors are orthogonal.**\n",
    "- **Case 4 ($\\theta = 0$ or $\\theta = 180$): This implies that $\\cos \\theta = 1 \\implies \\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta = \\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|$. We say these two vectors are collinear.**\n",
    "\n",
    "!!! note \"Consequence of Case 4\"\n",
    "    A simple consequence of case 4 is that if a vector $\\a$ dot product with itself, then by case 4, we have $\\a \\cdot \\a = \\|\\a\\|^2 \\implies \\|\\a\\| = \\sqrt{\\a \\cdot \\a}$ which is the formula of the  [Euclidean length](Euclidean_length \"wikilink\") of\n",
    "the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b5a24-242b-4477-9064-f85e2c02ab33",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/linear_algebra/linear_algebra_theory_intuition_code_chap3_fig_3.2.svg\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Fig 3.2: Sign of Dot Product and Angle between two vectors; By Hongnan G.</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b03ca-98b7-4e80-93d9-8e15f2850379",
   "metadata": {},
   "source": [
    "#### Properties of Dot Product\n",
    "\n",
    "!!! success \"info\" \"Properties of Dot Product\"\n",
    "    The **dot product**[^dot_product] fulfills the following properties if **a**, **b**, and **c** are real [vectors](vector_(geometry) \"wikilink\") and $\\lambda$ is a [scalar](scalar_(mathematics) \"wikilink\").\n",
    "\n",
    "    1.  **[Commutative](Commutative \"wikilink\"):**\n",
    "\n",
    "        $\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{b} \\cdot \\mathbf{a} ,$\n",
    "        which follows from the definition (*θ* is the angle between **a** and **b**): $\\mathbf{a} \\cdot \\mathbf{b} = \\left\\| \\mathbf{a} \\right\\| \\left\\| \\mathbf{b} \\right\\| \\cos \\theta = \\left\\| \\mathbf{b} \\right\\| \\left\\| \\mathbf{a} \\right\\| \\cos \\theta = \\mathbf{b} \\cdot \\mathbf{a} .$\n",
    "\n",
    "    2.  **[Distributive](Distributive_property \"wikilink\") over vector\n",
    "        addition:**\n",
    "\n",
    "        $\\mathbf{a} \\cdot (\\mathbf{b} + \\mathbf{c}) = \\mathbf{a} \\cdot \\mathbf{b} + \\mathbf{a} \\cdot \\mathbf{c} .$\n",
    "\n",
    "    3.  **[Bilinear](bilinear_form \"wikilink\")**:\n",
    "\n",
    "        $\\mathbf{a} \\cdot ( \\lambda \\mathbf{b} + \\mathbf{c} ) = \\lambda ( \\mathbf{a} \\cdot \\mathbf{b} ) + ( \\mathbf{a} \\cdot \\mathbf{c} ) .$\n",
    "\n",
    "    4.  **[Scalar multiplication](Scalar_multiplication \"wikilink\"):**\n",
    "\n",
    "        $( \\lambda_1 \\mathbf{a} ) \\cdot ( \\lambda_2 \\mathbf{b} ) = \\lambda_1 \\lambda_2 ( \\mathbf{a} \\cdot \\mathbf{b} ) .$\n",
    "\n",
    "    5.  **Not [associative](associative \"wikilink\")** because the dot\n",
    "        product between a scalar (**a ⋅ b**) and a vector (**c**) is not\n",
    "        defined, which means that the expressions involved in the\n",
    "        associative property, (**a ⋅ b**) ⋅ **c** or **a** ⋅ (**b ⋅ c**) are\n",
    "        both ill-defined.[^11] Note however that the previously mentioned\n",
    "        scalar multiplication property is sometimes called the \\\"associative\n",
    "        law for scalar and dot product\\\"[^12] or one can say that \\\"the dot\n",
    "        product is associative with respect to scalar multiplication\\\"\n",
    "        because \\lambda $(\\a \\cdot \\b) = (\\lambda \\a) \\cdot \\b = \\a \\cdot (\\lambda\n",
    "        \\b)$.\n",
    "\n",
    "    6.  **[Orthogonal](Orthogonal \"wikilink\"):**\n",
    "\n",
    "        Two non-zero vectors **a** and **b** are *orthogonal* if and only if $\\a \\cdot \\b = \\0$.\n",
    "\n",
    "    7.  **No [cancellation](cancellation_law \"wikilink\"):**\n",
    "    Unlike multiplication of ordinary numbers, where if $ab=ac$  then *b* always equals *c* unless *a* is zero, the dot product does not obey the [cancellation law](cancellation_law \"wikilink\").\n",
    "\n",
    "    8.  **[Product Rule](Product_Rule \"wikilink\"):**\n",
    "\n",
    "         If **a** and **b** are (vector-valued) [differentiable functions](differentiable_function \"wikilink\"), then the derivative, denoted by a prime ' of $\\a \\cdot \\b$ is given by the rule $(\\a \\cdot \\b)' = \\a' \\cdot \\b + \\a \\cdot \\b'$.\n",
    "\n",
    "    [^dot_product]: https://en.wikipedia.org/wiki/Dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815589e-bf42-4a0d-9562-2af147d2db02",
   "metadata": {},
   "source": [
    "#### Application to the law of cosines {#application_to_the_law_of_cosines}\n",
    "\n",
    "A triangle with lines a, b and c is presented in figure 3.31, a and b are separated by angle *θ*, then the **law of cosine** states that $$c^2 = a^2 + b^2 - 2ab\\cos(\\theta)$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\color{orange}c} \\cdot \\mathbf{\\color{orange}c}  & = ( \\mathbf{\\color{red}a} - \\mathbf{\\color{blue}b}) \\cdot ( \\mathbf{\\color{red}a} - \\mathbf{\\color{blue}b} ) \\\\\n",
    " & = \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{red}a} - \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} - \\mathbf{\\color{blue}b} \\cdot \\mathbf{\\color{red}a} + \\mathbf{\\color{blue}b} \\cdot \\mathbf{\\color{blue}b} \\\\\n",
    " & = \\mathbf{\\color{red}a}^2 - \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} - \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} + \\mathbf{\\color{blue}b}^2 \\\\\n",
    " & = \\mathbf{\\color{red}a}^2 - 2 \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} + \\mathbf{\\color{blue}b}^2 \\\\\n",
    "\\mathbf{\\color{orange}c}^2 & = \\mathbf{\\color{red}a}^2 + \\mathbf{\\color{blue}b}^2 - 2 \\mathbf{\\color{red}a} \\mathbf{\\color{blue}b} \\cos \\mathbf{\\color{purple}\\theta} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "which is the [law of cosines](law_of_cosines \"wikilink\").\n",
    "`{{clear}}`{=mediawiki}\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/linear_algebra/linear_algebra_theory_intuition_code_chap3_fig_3.31_law_of_cosine.PNG' width=\"500\" height=\"350\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig 3.31; Law of Cosine</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce08eb-47e1-4900-bc58-c4ad4df27d69",
   "metadata": {},
   "source": [
    "### Cauchy-Schwarz Inequality\n",
    "\n",
    "Let two vectors $\\v$ and $\\w$ be in a field $\\F^n$, then the inequality\n",
    "\n",
    "$$|\\v^\\top \\w| \\leq \\Vert \\v \\Vert \\Vert \\w \\Vert$$ holds. \n",
    "\n",
    "---\n",
    "\n",
    "> This inequality provides an **upper bound** for the dot product between two vectors; in other words, the absolute value of the dot product between two vectors cannot be larger than the product of the norms of the individual vectors. Note carefully that in order for the inequality to become an equality if and only if both vectors are the zero vector $\\0$ or if one vector (either one) is scaled by the other vector $\\v = \\lambda \\w$.  - **Mike X Cohen, Linear Algebra: Theory, Intuition, Code**\n",
    "\n",
    "If you wonder why when $\\v = \\lambda \\w$ implies equality, it is apparent if you do a substitution as such $$|\\v^\\top \\w| = |\\lambda \\w^\\top \\w| = \\lambda |\\w^\\top \\w| = \\lambda \\|\\w\\|^2 = \\lambda \\|\\w\\| \\|\\w\\| = \\|\\v\\| \\|\\w\\|$$\n",
    "where we used the fact that $\\w^\\top \\w = \\|\\w\\|^2$ by definition.\n",
    "\n",
    "The author decided to include this inequality here because this theorem is always used in many proofs. He then shows a use case in the Geometric Interpretation of the Dot Product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fbe90-a4e7-4eee-a3a6-68bc28b24c38",
   "metadata": {},
   "source": [
    "#### Proof of Algebraic and Geometric Equivalence of DOT Product\n",
    "\n",
    "Intuition: \n",
    "\n",
    "- https://flexbooks.ck12.org/cbook/ck-12-college-precalculus/section/9.6/primary/lesson/scalar-and-vector-projections-c-precalc/\n",
    "- https://www.quora.com/What-are-the-geometrical-meanings-of-a-dot-product-and-cross-product-of-a-vector\n",
    "- https://math.stackexchange.com/questions/805954/what-does-the-dot-product-of-two-vectors-represent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8df1d-270b-4db0-9b84-68c1526a67d4",
   "metadata": {},
   "source": [
    "### Unit Vector\n",
    "\n",
    "Though not apparent now, **unit vectors** are important in linear algebra and have wide spread applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e1b42-f7a1-4686-83fe-5155e42571b6",
   "metadata": {},
   "source": [
    "#### Geometric Interpretation of Unit Vector\n",
    "\n",
    "Consider vector $\\v = [3, 4]$ which has a norm of $\\|\\v_1\\| = \\sqrt{3^2 + 4^2} = 5$. We want to find a vector $\\u$ that is in the same direction of $\\v$ (assume all has starting coordinates at origin), but has norm of $1$. Geometrically, we know that we just need to divide the vector $\\v$ by $5$ (a.k.a the norm of the vector itself) to get a vector that has norm $1$ and also lies on the same direction. But how to recover the exact vector $\\u$ with its coordinates? We have learnt about **Vector-Scalar Multiplication** and visually, we need to multiply the vector $\\v$ by $\\frac{1}{5}$ to get the vector $\\u = [0.6, 0.8]$.\n",
    "\n",
    "Thus, given a vector $\\v$, we know that if we divide it by the length of itself, we can recover back a vector $\\u = \\frac{1}{\\|\\v\\|}\\v$ such that $\\|\\u\\| = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8155b6-a20c-49a8-83f7-5816576e21e8",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/linear_algebra/linear_algebra_theory_intuition_code_chap3_fig_3.5.svg\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Fig 3.5: Unit Vectors; By Hongnan G.</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff630916-c7eb-450e-b196-761e26891ae8",
   "metadata": {},
   "source": [
    "#### Algebraic Interpretation of Unit Vector\n",
    "\n",
    "Given a vector $\\v$, can we find a vector $\\u$ such that:\n",
    "- $\\|\\u\\| = 1$\n",
    "- $\\u$ is in the same direction as $\\v$.\n",
    "\n",
    "---\n",
    "\n",
    "We need to fulfill the above two conditions, and since we know that $\\u$ must be in the same direction as $\\v$, then $\\u = \\lambda \\v$ by definition. Thus, our problem is reduced to finding a vector $\\u = \\lambda \\v$ such that $\\|\\u\\| = 1$. And since $\\v$ is known, it suffices for us to find $\\lambda$ only. We also know that $\\|\\u\\|$ must be $1$, as a result, let us take the norm of both sides to get $$\\|\\u\\| = \\|\\lambda \\v\\| \\implies \\|\\u\\| = \\lambda \\|\\v\\| \\implies \\lambda = \\dfrac{\\|\\u\\|}{\\|\\v\\|}$$\n",
    "\n",
    "Substituting back, we have $$\\u = \\lambda \\v = \\dfrac{\\|\\u\\|}{\\|\\v\\|} \\v = \\dfrac{1}{\\|\\v\\|} \\v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43cfb3-441c-4fa2-8d8e-4d5233f456c8",
   "metadata": {},
   "source": [
    "### Exercise 1: Linear Combination\n",
    "\n",
    "Given a set of weights and vectors, write a python function that outputs the linear combination of the vectors with the respective weights.\n",
    "\n",
    "---\n",
    "\n",
    "The code to the solution is presented below, it is important to realize that the number of elements in the weights vector should be the same as the number of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e12b37-0b6e-4b16-943a-1161d9597ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Union\n",
    "\n",
    "# as col vector\n",
    "v1 = np.asarray([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "v2 = np.asarray([2, 4, 6, 8, 10]).reshape(-1, 1)\n",
    "v3 = np.asarray([3, 6, 9, 12, 15]).reshape(-1, 1)\n",
    "\n",
    "weights = [10, 20, 30]\n",
    "\n",
    "\n",
    "def linear_combination_vectors(\n",
    "    weights: List[float], *args: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Computes the linear combination of vectors.\n",
    "\n",
    "    Args:\n",
    "        weights (List[float]): The set of weights corresponding to each vector.\n",
    "        *args (np.ndarray): The set of vectors.\n",
    "\n",
    "    Returns:\n",
    "        linear_weighted_sum (np.ndarray): The linear combination of vectors.\n",
    "\n",
    "    Examples:\n",
    "        >>> v1 = np.asarray([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "        >>> v2 = np.asarray([2, 4, 6, 8, 10]).reshape(-1, 1)\n",
    "        >>> v3 = np.asarray([3, 6, 9, 12, 15]).reshape(-1, 1)\n",
    "        >>> weights = [10, 20, 30]\n",
    "        >>> linear_combination_vectors([10, 20, 30], v1, v2, v3)\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_weighted_sum = np.zeros(shape=args[0].shape)\n",
    "    for weight, vec in zip(weights, args):\n",
    "        linear_weighted_sum += weight * vec\n",
    "    return linear_weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96512de3-8a63-4d3e-b63e-34c863630a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[140.],\n",
       "       [280.],\n",
       "       [420.],\n",
       "       [560.],\n",
       "       [700.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_combination_vectors(weights, v1, v2, v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb31c77-62ad-4ecb-9d51-67b2ef88ccc7",
   "metadata": {},
   "source": [
    "### Exercise 2: Dot Product and Average\n",
    "\n",
    "Develop a method to use the dot product to compute the average of a set of numbers in a vector.\n",
    "\n",
    "---\n",
    "\n",
    "Since we want to compute the average of all elements in a vector $\\v \\in \\R^n$, we can first see the formula of average to be: $$\\bar{\\v} = \\frac{v_1 + v_2 + ... + v_n}{n}$$\n",
    "\n",
    "To make use of dot product, we can define $\\1$ and perform $\\v^\\top \\cdot \\1$ which returns the sum of all elements in $\\v$ by the definition of dot product. Lastly, divide this answer by the total number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dae6d1c-f974-45e6-948b-a7b9a7fb0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "    \"\"\"Computes the dot product of two vectors.\n",
    "\n",
    "    We assume both vectors are flattened, i.e. they are 1D arrays.\n",
    "\n",
    "    Args:\n",
    "        v1 (np.ndarray): The first vector.\n",
    "        v2 (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "        dot_product_v1_v2 (float): The dot product of two vectors.\n",
    "\n",
    "    Examples:\n",
    "        >>> v1 = np.asarray([1, 2, 3, 4, 5])\n",
    "        >>> v2 = np.asarray([2, 4, 6, 8, 10])\n",
    "        >>> dot_product(v1, v2)\n",
    "    \"\"\"\n",
    "\n",
    "    v1, v2 = np.asarray(v1).flatten(), np.asarray(v2).flatten()\n",
    "\n",
    "    dot_product_v1_v2 = 0\n",
    "    for element_1, element_2 in zip(v1, v2):\n",
    "        dot_product_v1_v2 += element_1 * element_2\n",
    "\n",
    "    # same as np.dot but does not take into the orientation of vectors\n",
    "    assert dot_product_v1_v2 == np.dot(v1.T, v2)\n",
    "\n",
    "    return dot_product_v1_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf1b4c3e-7184-4b6d-817e-6ed7ffa9f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_set(vec: Union[np.ndarray, set]) -> float:\n",
    "    \"\"\"Average a set of numbers using dot product.\n",
    "\n",
    "    Given a set of numbers {v1, v2, ..., vn}, the average is defined as:\n",
    "    avg = (v1 + v2 + ... + vn) / n\n",
    "\n",
    "    To use the dot product, we can convert the set to a col/row vector (array) `vec` and\n",
    "    perform the dot product with the vector of ones to get `sum(set)`. Lastly, we divide by the number of elements in the set.\n",
    "\n",
    "    Args:\n",
    "        vec (Union[np.ndarray, set]): A set of numbers.\n",
    "\n",
    "    Returns:\n",
    "        average (float): The average of the set.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(vec, set):\n",
    "        vec = np.asarray(list(vec)).flatten()\n",
    "\n",
    "    ones = np.ones(shape=vec.shape)\n",
    "    total_sum = dot_product(vec, ones)\n",
    "    average = total_sum / vec.shape[0]\n",
    "\n",
    "    assert average == np.mean(vec)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d30336f-0d05-45f9-ba64-b27f68c7a3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average of all vectors in v1 is 3.0\n"
     ]
    }
   ],
   "source": [
    "# as col vector\n",
    "v = np.asarray([1, 2, 3, 4, 5])\n",
    "v_set = {1,2,3,4,5}\n",
    "average = average_set(v_set)\n",
    "print(f\"average of all vectors in v_set is {average}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70f7f0-9130-4585-9cf8-928a4e16df3d",
   "metadata": {},
   "source": [
    "### Exercise 3: Dot Product and Weighted Average\n",
    "\n",
    "What if some numbers were more important than other numbers? Modify your answer to the previous question to devise a method to use the dot product to compute a weighted mean of a set of numbers.\n",
    "\n",
    "---\n",
    "\n",
    "We assume weighted mean to be normalized such that the weights of all the vectors must sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "501ccc2a-bb0d-4233-bed8-dba2d5836f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted average is 0.23372627800855864\n"
     ]
    }
   ],
   "source": [
    "# as col vector\n",
    "v1 = np.asarray([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "shape_v1 = v1.shape\n",
    "num_elements = shape_v1[0]\n",
    "\n",
    "random_weights = np.random.rand(*shape_v1)\n",
    "normalized_random_weights = random_weights / num_elements\n",
    "\n",
    "total_sum = dot_product(v1, normalized_random_weights)\n",
    "\n",
    "weighted_average = total_sum / v1.shape[0]\n",
    "\n",
    "print(f\"weighted average is {weighted_average}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
