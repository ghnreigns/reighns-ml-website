{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18de9b4e-6f8d-4a2a-a016-5cfd4bb81de8",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\F}{\\mathbb{F}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\v}{\\mathbf{v}}\n",
    "\\newcommand{\\a}{\\mathbf{a}}\n",
    "\\newcommand{\\b}{\\mathbf{b}}\n",
    "\\newcommand{\\c}{\\mathbf{c}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\u}{\\mathbf{u}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\1}{\\mathbf{1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb09e9f-e4bd-47ae-a18a-5902f7be24cf",
   "metadata": {},
   "source": [
    "The following notes use reference from **Mike X Cohen: Linear Algebra: Theory, Intuition, Code, 2021.**, **Sheldon Axler: Linear Algebra Done Right, 2015.** and **Wikipedia** for intuitions, examples, formal definitions and theorems. Since I am at the introductory chapters, I will breeze through them until vector spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfcb049-5b1c-4f2f-aa7e-6eb2f198fbfb",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Learning Objectives](#1)\n",
    "* [Field](#1)\n",
    "    * [Definition (Field)](#Import_modules_offline)\n",
    "    * [Examples of Fields](#Import_other_modules)\n",
    "    * [Notation of Fields](#11)\n",
    "    * [Summary of Fields](#11)\n",
    "* [Vectors](#2)\n",
    "    * [Geometric Definition (Vectors)](#11)\n",
    "        * [Vector is Invariant under Coordinates](#11)\n",
    "    * [Algebraic Definition (Vectors)](#11)\n",
    "    * [Equality of Vectors](#11)\n",
    "    * [Vector Orientation](#11)\n",
    "        * [Example of Column and Row Vectors](#11)\n",
    "    * [Transposed Vector](#11)\n",
    "        * [Definition (Transposed Vector)](#11)\n",
    "    * [Vector Addition and Subtraction](#11)\n",
    "        * [Algebraic Definition](#11)\n",
    "        * [Geometric Definition](#11)\n",
    "        * [Vector Addition is Commutative](#11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c57caa-a66a-421c-a637-ea9bbd354557",
   "metadata": {},
   "source": [
    "!!! summary \"Learning Objectives\"\n",
    "    - Definition of a Field\n",
    "    - Definition of a Vector\n",
    "        - Vector Operations with both Algebraic and Geometric understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f53d8-f995-404e-b51d-2e978d0d7b0f",
   "metadata": {},
   "source": [
    "## Vector Multiplications\n",
    "\n",
    "This section introduces one of the most important idea in Linear Algebra, the **Dot Product**. Since [Wikipedia](https://en.wikipedia.org/wiki/Dot_product)^[Dot_product] has a wholesome introduction, we will be copying over some definitions from it.\n",
    "\n",
    "\n",
    "[^Dot_product]: https://en.wikipedia.org/wiki/Dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead711b9-062f-42a0-b47c-d812083e3031",
   "metadata": {},
   "source": [
    "### DOT Product (Algebraic definition)\n",
    "\n",
    "The dot product of two vectors $\\color{red}{\\a =  \\begin{bmatrix} a_1  \\; a_2  \\; \\dots \\; a_n \\end{bmatrix}^{\\rm T}}$ and \n",
    "$\\color{blue}{\\b =  \\begin{bmatrix} b_1 & b_2  & \\dots & b_n \\end{bmatrix}^{\\rm T}}$ is defined as:\n",
    "\n",
    "$$\\mathbf{\\color{red}\\a}\\cdot\\mathbf{\\color{blue}\\b}=\\sum_{i=1}^n {\\color{red}a}_i{\\color{blue}b}_i={\\color{red}a}_1{\\color{blue}b}_1+{\\color{red}a}_2{\\color{blue}b}_2+\\cdots+{\\color{red}a}_n{\\color{blue}b}_n$$\n",
    "\n",
    "where $\\sum$ denotes summation and $n$ is the dimension of the vector space. Since **vector space** has not been introduced, we just think of it as the $\\R^n$ dimensional space. \n",
    "\n",
    "#### Example\n",
    "\n",
    "For instance, in 3-dimensional space, the **dot product** of column vectors $\\begin{bmatrix}1 & 3 & -5\\end{bmatrix}^{\\rm T}$ and $\\begin{bmatrix}4 & -2 & -2\\end{bmatrix}^{\\rm T}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ [{\\color{red}1, 3, -5}] \\cdot  [{\\color{blue}4, -2, -1}] &= ({\\color{red}1} \\times {\\color{blue}4}) + ({\\color{red}3}\\times{\\color{blue}-2}) + ({\\color{red}-5}\\times{\\color{blue}-1}) \\\\\n",
    "&= 4 - 6 + 5 \\\\\n",
    "&= 3\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Vector as Matrices\n",
    "\n",
    "We are a little ahead in terms of the definition of Matrices, but for people familiar with it, or have worked with `numpy` before, we know that we can interpret a row vector of dimension $n$ as a matrix of dimension $1 \\times n$, similarly, we can interpret a column vector of dimension $n$ as a matrix of dimension $n \\times 1$. With this interpretation, we can perform a so called \"matrix multiplication\" of the row vector and column vector. The result is the dot product. We will go in details when we get to it.\n",
    "\n",
    "If vectors are identified with row matrix, the dot product can also be written as a matrix multiplication.\n",
    "\n",
    "$$\\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} = \\mathbf{\\color{red}a}^\\mathsf T \\mathbf{\\color{blue}b}$$\n",
    "\n",
    "Expressing the above example in this way, a 1 × 3 matrix **row vector** is multiplied by a 3 × 1 matrix **column vector** to get a 1 × 1 matrix that is identified with its unique entry:\n",
    "$$\n",
    "  \\begin{bmatrix}\n",
    "   \\color{red}1 & \\color{red}3 & \\color{red}-5\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "   \\color{blue}4 \\\\ \\color{blue}-2 \\\\ \\color{blue}-1\n",
    "  \\end{bmatrix} = \\color{purple}3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174cfb48-db93-48f7-b36f-c4dbae04bb60",
   "metadata": {},
   "source": [
    "### Properties\n",
    "\n",
    "Extracted from Wikipedia:\n",
    "\n",
    "The dot product fulfills the following properties if **a**, **b**, and\n",
    "**c** are real [vectors](vector_(geometry) \"wikilink\") and *r* is a\n",
    "[scalar](scalar_(mathematics) \"wikilink\").[^8][^9]\n",
    "\n",
    "1.  **[Commutative](Commutative \"wikilink\"):**\n",
    "\n",
    "    $\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{b} \\cdot \\mathbf{a} ,$\n",
    "    which follows from the definition (*θ* is the angle between **a** and **b**): $\\mathbf{a} \\cdot \\mathbf{b} = \\left\\| \\mathbf{a} \\right\\| \\left\\| \\mathbf{b} \\right\\| \\cos \\theta = \\left\\| \\mathbf{b} \\right\\| \\left\\| \\mathbf{a} \\right\\| \\cos \\theta = \\mathbf{b} \\cdot \\mathbf{a} .$\n",
    "    \n",
    "2.  **[Distributive](Distributive_property \"wikilink\") over vector\n",
    "    addition:**\n",
    "\n",
    "    $\\mathbf{a} \\cdot (\\mathbf{b} + \\mathbf{c}) = \\mathbf{a} \\cdot \\mathbf{b} + \\mathbf{a} \\cdot \\mathbf{c} .$\n",
    "    \n",
    "3.  **[Bilinear](bilinear_form \"wikilink\")**:\n",
    "\n",
    "    $\\mathbf{a} \\cdot ( r \\mathbf{b} + \\mathbf{c} ) = r ( \\mathbf{a} \\cdot \\mathbf{b} ) + ( \\mathbf{a} \\cdot \\mathbf{c} ) .$\n",
    "    \n",
    "4.  **[Scalar multiplication](Scalar_multiplication \"wikilink\"):**\n",
    "\n",
    "    $( c_1 \\mathbf{a} ) \\cdot ( c_2 \\mathbf{b} ) = c_1 c_2 ( \\mathbf{a} \\cdot \\mathbf{b} ) .$\n",
    "    \n",
    "5.  **Not [associative](associative \"wikilink\")** because the dot\n",
    "    product between a scalar (**a ⋅ b**) and a vector (**c**) is not\n",
    "    defined, which means that the expressions involved in the\n",
    "    associative property, (**a ⋅ b**) ⋅ **c** or **a** ⋅ (**b ⋅ c**) are\n",
    "    both ill-defined.[^11] Note however that the previously mentioned\n",
    "    scalar multiplication property is sometimes called the \\\"associative\n",
    "    law for scalar and dot product\\\"[^12] or one can say that \\\"the dot\n",
    "    product is associative with respect to scalar multiplication\\\"\n",
    "    because *c* (**a** ⋅ **b**) = (*c* **a**) ⋅ **b** = **a** ⋅ (*c*\n",
    "    **b**).[^13]\n",
    "    \n",
    "6.  **[Orthogonal](Orthogonal \"wikilink\"):**\n",
    "\n",
    "    Two non-zero vectors **a** and **b** are *orthogonal* if and only if $\\a \\cdot \\b = \\0$.\n",
    "        \n",
    "7.  **No [cancellation](cancellation_law \"wikilink\"):**\n",
    "Unlike multiplication of ordinary numbers, where if $ab=ac$  then *b* always equals *c* unless *a* is zero, the dot product does not obey the [cancellation law](cancellation_law \"wikilink\").\n",
    "\n",
    "8.  **[Product Rule](Product_Rule \"wikilink\"):**\n",
    "\n",
    "     If **a** and **b** are (vector-valued) [differentiable functions](differentiable_function \"wikilink\"), then the derivative, denoted by a prime ' of $\\a \\cdot \\b$ is given by the rule $(\\a \\cdot \\b)' = \\a' \\cdot \\b + \\a \\cdot \\b'$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce08eb-47e1-4900-bc58-c4ad4df27d69",
   "metadata": {},
   "source": [
    "### Cauchy-Schwarz Inequality\n",
    "\n",
    "Let two vectors $\\v$ and $\\w$ be in field $\\F^n$, then the inequality\n",
    "\n",
    "$$|\\v^\\top \\w| \\leq \\Vert \\v \\Vert \\Vert \\w \\Vert$$ holds. \n",
    "\n",
    "---\n",
    "\n",
    "> This inequality provides an **upper bound** for the dot product between two vectors; in other words, the absolute value of the dot product between two vectors cannot be larger than the product of the norms of the individual vectors. Note carefully that in order for the inequality to become an equality if and only if both vectors are the zero vector $\\0$ or if one vector (either one) is scaled by the other vector $\\v = \\lambda \\w$.  - **Mike X Cohen, Linear Algebra: Theory, Intuition, Code**\n",
    "\n",
    "If you wonder why when $\\v = \\lambda \\w$ implies equality, it is apparent if you do a substitution as such $$|\\v^\\top \\w| = |\\lambda \\w^\\top \\w| = \\lambda |\\w^\\top \\w| = \\lambda \\|\\w\\|^2 = \\lambda \\|\\w\\| \\|\\w\\| = \\|\\v\\| \\|\\w\\|$$\n",
    "where we used the fact that $\\w^\\top \\w = \\|\\w\\|^2$ by definition.\n",
    "\n",
    "The author decided to include this inequality here because this theorem is always used in many proofs. He then shows a use case in the Geometric Interpretation of the Dot Product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdad35a-67f2-4bfb-b0c1-eca500f024d1",
   "metadata": {},
   "source": [
    "### DOT Product (Geometric definition)\n",
    "\n",
    "In [Euclidean space](Euclidean_space \"wikilink\"), a [Euclidean vector](Euclidean_vector \"wikilink\") is a geometric object that possesses both a magnitude and a direction. A vector can be pictured as\n",
    "an arrow. Its magnitude is its length, and its direction is the direction to which the arrow points. The magnitude of a vector **a** is denoted by $\\left\\| \\mathbf{a} \\right\\|$. The dot product of two\n",
    "Euclidean vectors **a** and **b** is defined by $$\\mathbf{a}\\cdot\\mathbf{b}=\\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta ,$$\n",
    "where $\\theta$ is the angle between $\\a$ and $\\b$.\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/linear_algebra/linear_algebra_theory_intuition_code_chap3_fig_3.1_scalar_projection_and_dot_product.PNG' width=\"500\" height=\"350\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig 3.11; Diagram of Scalar Projection and DOT Product.</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2cb18-ab03-4b0b-b91c-17af422c5173",
   "metadata": {},
   "source": [
    "In particular, if the vectors $\\a$ and $\\b$ are [orthogonal](orthogonal \"wikilink\")\n",
    "(i.e., their angle is $\\frac{\\pi}{2}$), then $\\cos \\frac \\pi 2 = 0$, which implies that\n",
    "\n",
    "$$\\mathbf a \\cdot \\mathbf b = 0 .$$ At the other extreme, if they are\n",
    "codirectional, then the angle between them is zero with $\\cos 0 = 1$ and\n",
    "\n",
    "$$\\mathbf a \\cdot \\mathbf b = \\left\\| \\mathbf a \\right\\| \\, \\left\\| \\mathbf b \\right\\|$$\n",
    "This implies that the dot product of a vector **a** with itself is\n",
    "\n",
    "$$\\mathbf a \\cdot \\mathbf a = \\left\\| \\mathbf a \\right\\| ^2 ,$$ which\n",
    "gives $$\\left\\| \\mathbf a \\right\\| = \\sqrt{\\mathbf a \\cdot \\mathbf a}$$\n",
    "\n",
    "the formula for the [Euclidean length](Euclidean_length \"wikilink\") of\n",
    "the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d0081-bd84-479f-b276-94725ac0a3f4",
   "metadata": {},
   "source": [
    "#### Scalar projections\n",
    "\n",
    "#TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c30fbf8-2084-45e3-9bfc-f60823387669",
   "metadata": {},
   "source": [
    "#### Sign of the DOT Product is determined by the Angle in between the two vectors\n",
    "\n",
    "The geometric definition can be re-written as follows:\n",
    "\n",
    "\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "\\mathbf{a}\\cdot\\mathbf{b} &=\\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta \\implies \\cos(\\theta) = \\frac{\\a^\\top \\b}{\\|\\a\\| \\|\\b\\|} \\implies \\theta = \\cos^{-1}\\left(\\frac{\\a^\\top \\b}{\\|\\a\\| \\|\\b\\|}\\right)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "which essentially means that one can find the angle between two known vectors in any dimensional space.\n",
    "\n",
    "---\n",
    "\n",
    "The author pays particular attention to this part and he explained how the **sign** of the dot product is determined solely by the angle between the two vectors. One can read it in more details on page 51-52 of his book **Linear Algebra: Theory, Intuition, Code**. I will just give a summary:\n",
    "\n",
    "!!! note\n",
    "    By the definition $\\mathbf{a}\\cdot\\mathbf{b} = \\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta$, we know that the sign (positive or negative) of the dot product $\\a \\cdot \\b$ is solely determined by $\\cos \\theta$ since $\\|\\a\\| \\|\\b\\|$ is always positive. And thus if $0<\\theta < 90$ then $\\cos \\theta > 0 \\implies \\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta > 0 \\implies \\mathbf{a}\\cdot\\mathbf{b} > 0$. If $90<\\theta < 180$ then $\\cos \\theta < 0 \\implies \\|\\mathbf{a}\\|\\ \\|\\mathbf{b}\\|\\cos\\theta < 0 \\implies \\mathbf{a}\\cdot\\mathbf{b} < 0$. The author also pin point that when $\\theta$ is 90 degrees, then the dot product is necessarily zero, implying the two vectors are orthogonal. This is one of the most important properties of dot product and should be committed to memory!\n",
    "    \n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/linear_algebra/linear_algebra_theory_intuition_code_chap3_fig_3.2.PNG' width=\"500\" height=\"350\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig 3.2; Courtesy of Linear Algebra: Theory,<br> Intuition, Code by Mike X Cohen</b></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d28d4e-7fed-4bab-bd9b-bd5381abc9bf",
   "metadata": {},
   "source": [
    "#### Application to the law of cosines {#application_to_the_law_of_cosines}\n",
    "\n",
    "A triangle with lines a, b and c is presented in figure 3.31, a and b are separated by angle *θ*, then the **law of cosine** states that $$c^2 = a^2 + b^2 - 2ab\\cos(\\theta)$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\color{orange}c} \\cdot \\mathbf{\\color{orange}c}  & = ( \\mathbf{\\color{red}a} - \\mathbf{\\color{blue}b}) \\cdot ( \\mathbf{\\color{red}a} - \\mathbf{\\color{blue}b} ) \\\\\n",
    " & = \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{red}a} - \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} - \\mathbf{\\color{blue}b} \\cdot \\mathbf{\\color{red}a} + \\mathbf{\\color{blue}b} \\cdot \\mathbf{\\color{blue}b} \\\\\n",
    " & = \\mathbf{\\color{red}a}^2 - \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} - \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} + \\mathbf{\\color{blue}b}^2 \\\\\n",
    " & = \\mathbf{\\color{red}a}^2 - 2 \\mathbf{\\color{red}a} \\cdot \\mathbf{\\color{blue}b} + \\mathbf{\\color{blue}b}^2 \\\\\n",
    "\\mathbf{\\color{orange}c}^2 & = \\mathbf{\\color{red}a}^2 + \\mathbf{\\color{blue}b}^2 - 2 \\mathbf{\\color{red}a} \\mathbf{\\color{blue}b} \\cos \\mathbf{\\color{purple}\\theta} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "which is the [law of cosines](law_of_cosines \"wikilink\").\n",
    "`{{clear}}`{=mediawiki}\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/linear_algebra/linear_algebra_theory_intuition_code_chap3_fig_3.31_law_of_cosine.PNG' width=\"500\" height=\"350\" align=\"center\"/>\n",
    "<figcaption align = \"center\"><b>Fig 3.31; Law of Cosine</b></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fbe90-a4e7-4eee-a3a6-68bc28b24c38",
   "metadata": {},
   "source": [
    "#### Proof of Algebraic and Geometric Equivalence of DOT Product\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43cfb3-441c-4fa2-8d8e-4d5233f456c8",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "The code to the solution is presented below, it is important to realize that the number of elements in the weights vector should be the same as the number of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19e12b37-0b6e-4b16-943a-1161d9597ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# as col vector\n",
    "v1 = np.asarray([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "v2 = np.asarray([2, 4, 6, 8, 10]).reshape(-1, 1)\n",
    "v3 = np.asarray([3, 6, 9, 12, 15]).reshape(-1, 1)\n",
    "\n",
    "weights = [10, 20, 30]\n",
    "\n",
    "\n",
    "def linear_combination_vectors(\n",
    "    weights: List[float], *args: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Computes the linear combination of vectors.\n",
    "\n",
    "    Args:\n",
    "        weights (List[float]): The set of weights corresponding to each vector.\n",
    "\n",
    "    Returns:\n",
    "        linear_weighted_sum (np.ndarray): The linear combination of vectors.\n",
    "\n",
    "    Examples:\n",
    "        >>> v1 = np.asarray([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "        >>> v2 = np.asarray([2, 4, 6, 8, 10]).reshape(-1, 1)\n",
    "        >>> v3 = np.asarray([3, 6, 9, 12, 15]).reshape(-1, 1)\n",
    "        >>> weights = [10, 20, 30]\n",
    "        >>> linear_combination_vectors([10, 20, 30], v1, v2, v3)\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_weighted_sum = np.zeros(shape=args[0].shape)\n",
    "    for weight, vec in zip(weights, args):\n",
    "        linear_weighted_sum += weight * vec\n",
    "    return linear_weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96512de3-8a63-4d3e-b63e-34c863630a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[140.],\n",
       "       [280.],\n",
       "       [420.],\n",
       "       [560.],\n",
       "       [700.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_combination_vectors(weights, v1, v2, v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb31c77-62ad-4ecb-9d51-67b2ef88ccc7",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Since we want to compute the average of all elements in a vector $\\v \\in \\R^n$, we can first see the formula of average to be: $$\\bar{\\v} = \\frac{v_1 + v_2 + ... + v_n}{n}$$\n",
    "\n",
    "To make use of dot product, we can define $\\1$ and perform $\\v^\\top \\cdot \\1$ which returns the sum of all elements in $\\v$ by the definition of dot product. Lastly, divide this answer by the total number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dae6d1c-f974-45e6-948b-a7b9a7fb0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "    \"\"\"Computes the dot product of two vectors.\n",
    "\n",
    "    Args:\n",
    "        v1 (np.ndarray): The first vector.\n",
    "        v2 (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "        dot_product_v1_v2 (float): The dot product of two vectors.\n",
    "\n",
    "    Examples:\n",
    "        >>> v1 = np.asarray([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "        >>> v2 = np.asarray([2, 4, 6, 8, 10]).reshape(-1, 1)\n",
    "        >>> dot_product(v1, v2)\n",
    "    \"\"\"\n",
    "\n",
    "    dot_product_v1_v2 = 0\n",
    "    for element_1, element_2 in zip(v1, v2):\n",
    "        dot_product_v1_v2 += element_1 * element_2\n",
    "\n",
    "    return dot_product_v1_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e93625fc-80a8-40d8-9a8d-7a449b7d6926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product(v1, v2) == np.dot(v1.T, v2) # same as np.dot but does not take into the orientation of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d30336f-0d05-45f9-ba64-b27f68c7a3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average is [3.]\n"
     ]
    }
   ],
   "source": [
    "# as col vector\n",
    "v1 = np.asarray([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "shape_v1 = v1.shape\n",
    "\n",
    "ones = np.ones(shape=shape_v1)\n",
    "\n",
    "total_sum = dot_product(v1, ones)\n",
    "\n",
    "average = total_sum / v1.shape[0]\n",
    "\n",
    "print(f\"average is {average}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70f7f0-9130-4585-9cf8-928a4e16df3d",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "We assume weighted mean to be normalized such that the weights of all the vectors must sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "501ccc2a-bb0d-4233-bed8-dba2d5836f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted average is [0.22985858]\n"
     ]
    }
   ],
   "source": [
    "# as col vector\n",
    "v1 = np.asarray([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "shape_v1 = v1.shape\n",
    "num_elements = shape_v1[0]\n",
    "\n",
    "random_weights = np.random.rand(*shape_v1)\n",
    "normalized_random_weights = random_weights / num_elements\n",
    "\n",
    "total_sum = dot_product(v1, normalized_random_weights)\n",
    "\n",
    "weighted_average = total_sum / v1.shape[0]\n",
    "\n",
    "print(f\"weighted average is {weighted_average}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
