{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "145df933-1c09-483d-b26a-239a29af9be1",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "<a id = '1.0'></a>\n",
    "<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal;background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" >Quick Navigation</h1>\n",
    "\n",
    "    \n",
    "* [Dependencies and Configuration](#1)\n",
    "* [Stage 4: Modelling](#2)\n",
    "    * [How EDA helped us?](#31)\n",
    "    * [Modelling](#31)\n",
    "        * [Spot Checking Algorithms](#31)\n",
    "            * [Make Basic Pipeline (Say No to Data Leakage!)](#31)\n",
    "            * [Define Metrics](#31)\n",
    "            * [Comparison of Cross-Validated Models](#31)\n",
    "            * [Out-of-Fold Confusion Matrix](#31)\n",
    "            * [Hypothesis Testing Across Models](#31)\n",
    "        * [Model Selection: Hyperparameter Tuning with GridSearchCV](#31)\n",
    "        * [Retrain on the whole training set](#31)\n",
    "            * [Retrain using Optimal Hyperparameters](#31)\n",
    "        * [Interpretation of Results](#31)\n",
    "            * [Interpretation of Coefficients](#31)\n",
    "            * [Interpretation of Metric Scores on Train Set](#31)\n",
    "    * [Evaluation on Test Set](#31)\n",
    "    * [Bias-Variance Tradeoff](#31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180a2a7-c933-4f5f-ae21-e38eb415b880",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "To use logging instead of print.\n",
    "\n",
    "---\n",
    "\n",
    "We define a `make_pipeline` function here to define our steps needed. \n",
    "\n",
    "**TODO: To allow the function to take in dynamic parameters, maybe a list of steps can be passed in so that it is less hardcoded within the function.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2fd28b-c218-495e-9b60-0c37d5060ba2",
   "metadata": {},
   "source": [
    "# Dependencies and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d078e09b-5833-4510-b8a2-644087d79510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gcloud == 0.18.3\n",
    "import csv\n",
    "import random\n",
    "from functools import wraps\n",
    "from time import time\n",
    "from typing import Callable, Dict, List, Union, Optional, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import mlxtend\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv, bias_variance_decomp\n",
    "from scipy import stats\n",
    "from sklearn import (base, decomposition, dummy, ensemble, feature_selection,\n",
    "                     linear_model, metrics, model_selection, neighbors,\n",
    "                     pipeline, preprocessing, svm, tree)\n",
    "\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "#from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b04142-7779-4b78-bab7-f02eddb6a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class config:\n",
    "    raw_data: str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\"\n",
    "    processed_data: str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\"\n",
    "    train_size: float = 0.9\n",
    "    seed: int = 1992\n",
    "    num_folds: int = 5\n",
    "    cv_schema: str = \"StratifiedKFold\"\n",
    "    classification_type: str = \"binary\"\n",
    "    \n",
    "    target_col: List[str] = field(default_factory = lambda: [\"diagnosis\"])\n",
    "    unwanted_cols : List[str] =  field(default_factory = lambda: [\"id\", \"Unnamed: 32\"])\n",
    "    \n",
    "    # Plotting\n",
    "    colors : List[str] =field(default_factory = lambda: [\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"])\n",
    "    cmap_reversed = plt.cm.get_cmap('mako_r')\n",
    "    \n",
    "#     spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\"\n",
    "#     oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\"\n",
    "#     final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\"\n",
    "#     precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\"\n",
    "#     roc_plot = \"../data/images/roc_plot.png\"\n",
    "#     feature_importance = \"../data/images/feature_importance.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f2d69f-a76a-47aa-a3a8-e26c5b96591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed: int = 1234) -> None:\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "def init_logger(log_file: str = \"info.log\"):\n",
    "    \"\"\"\n",
    "    Initialize logger.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\", datefmt= \"%Y-%m-%d,%H:%M:%S\"))\n",
    "    file_handler = logging.FileHandler(filename=log_file)\n",
    "    file_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\",  datefmt= \"%Y-%m-%d,%H:%M:%S\"))\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8791f825-7ee4-403f-93be-99fd2f0493e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config()\n",
    "logger = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d10cc4-01dd-4c2e-b419-3389c9dcd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeding for reproducibility\n",
    "_ = set_seeds(seed = config.seed)\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(config.processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0f8f0-ce09-4c85-b4dc-f11315ec84f4",
   "metadata": {},
   "source": [
    "# Cross-Validation Strategy\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Generalization:</b>     \n",
    "    <blockquote cite=\"https://www.huxley.net/bnw/four.html\">\n",
    "        <p>Ultimately, we are interested in the Generalization Error made by the model, that is, how well the model perform on <b>unseen data</b> that is not taken from our sample set $\\mathcal{D}$. In general, we use <b>validation set</b> for <b>Model Selection</b> and the <b>test set</b> for <b>an estimate of generalization error</b> on new data.\n",
    "            <br> <b>- Refactored from Elements of Statistical Learning, Chapter 7.2</b></p>\n",
    "    </blockquote>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <b>Step 1: Train-Test-Split:</b> Since this dataset is relatively small, we will not use the <b>train-validation-test</b> split and only split into train and test in a ratio of 9:1, whereby the split is stratified on our target, using <code>stratify=y</code> parameter in <code>train_test_split()</code> to ensure that our target has equal representation in both train and test. We note that this is a relatively small dataset and in practice, we need a large sample size to get a reliable/stable split, it is also recommended to retrain the whole dataset (without the \"unseen\" test set) after we have done the model selection process (eg. finding best hyperparameters). \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <b>Step 2: Resampling Stategy:</b> Note that we will be performing <code>StratifiedKFold</code> as our resampling strategy. After our split in Step 1, we have a training set $X_{\\text{train}}$, we will then perform our resampling strategy on this $X_{\\text{train}}$. We will choose our choice of $K = 5$. The choice of $K$ is somewhat arbitrary, and is derived <a href=\"https://stats.stackexchange.com/questions/61783/bias-and-variance-in-leave-one-out-vs-k-fold-cross-validation\">empirically</a>. \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "To recap, we have the following:\n",
    "\n",
    "- **Training Set ($X_{\\text{train}}$)**: This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis $h \\in \\mathcal{H}$.\n",
    "- **Validation Set ($X_{\\text{val}}$)**: This is split from our $X_{\\text{train}}$ during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis $g \\in \\mathcal{H}$).\n",
    "- **Test Set ($X_{\\text{test}}$)**: This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model $g$, we will use $g$ to predict on the test set to get an estimate of the generalization error (also called out-of-sample error).\n",
    "\n",
    "---\n",
    "\n",
    "<figure>\n",
    "<img src='https://scikit-learn.org/stable/_images/grid_search_workflow.png' width=\"500\"/>\n",
    "<figcaption align = \"center\"><b>Courtesy of scikit-learn on a typical Cross-Validation workflow.</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c4506e-b17c-4592-9690-15150a8d3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(\"diagnosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2e7450a-75bc-400b-a2bb-21b06fc4b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_cols = X.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32d165c0-5668-4127-a186-93f2c2676ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train - test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=config.train_size, shuffle=True, stratify= y, random_state=config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d2535b-f957-4c8d-aead-0fb30ca86668",
   "metadata": {},
   "source": [
    "We confirm that we have stratified properly. We do observe that the distribution of targets in both `y_train` and `y_test` are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e23048-2d7d-46fe-b327-5f942335d33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10,13:35:37 - Y Train Distribution is : {0: 0.626953125, 1: 0.373046875}\n",
      "2021-11-10,13:35:37 - Y Test Distribution is : {0: 0.631578947368421, 1: 0.3684210526315789}\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Y Train Distribution is : {y_train.value_counts(normalize=True).to_dict()}\")\n",
    "logger.info(f\"Y Test Distribution is : {y_test.value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a62783b-1def-4567-8f20-d6a81f987f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folds(\n",
    "    df: pd.DataFrame,\n",
    "    num_folds: int,\n",
    "    cv_schema: str,\n",
    "    seed: int,\n",
    "    predictor_col: List,\n",
    "    target_col: List,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Split the given dataframe into training folds.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): [description]\n",
    "        num_folds (int): [description]\n",
    "        cv_schema (str): [description]\n",
    "        seed (int): [description]\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: [description]\n",
    "    \"\"\"\n",
    "\n",
    "    if cv_schema == \"KFold\":\n",
    "        df_folds = df.copy()\n",
    "        kf = model_selection.KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(\n",
    "            kf.split(X=df_folds[predictor_col], y=df_folds[target_col])\n",
    "        ):\n",
    "            df_folds.loc[val_idx, \"fold\"] = int(fold + 1)\n",
    "\n",
    "        df_folds[\"fold\"] = df_folds[\"fold\"].astype(int)\n",
    "\n",
    "    elif cv_schema == \"StratifiedKFold\":\n",
    "        df_folds = df.copy()\n",
    "        skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(\n",
    "            skf.split(X=df_folds[predictor_col], y=df_folds[target_col])\n",
    "        ):\n",
    "            df_folds.loc[val_idx, \"fold\"] = int(fold + 1)\n",
    "\n",
    "        df_folds[\"fold\"] = df_folds[\"fold\"].astype(int)\n",
    "        print(df_folds.groupby([\"fold\", \"diagnosis\"]).size())\n",
    "\n",
    "    return df_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "035e7e10-5ce3-4209-8e61-664c72cfdc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  diagnosis\n",
      "1     0            64\n",
      "      1            39\n",
      "2     0            65\n",
      "      1            38\n",
      "3     0            64\n",
      "      1            38\n",
      "4     0            64\n",
      "      1            38\n",
      "5     0            64\n",
      "      1            38\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_y_train = pd.concat([X_train, y_train], axis = 1).reset_index(drop=True)\n",
    "df_folds = make_folds(X_y_train, num_folds=config.num_folds, cv_schema=config.cv_schema, seed=config.seed, predictor_col= predictor_cols, target_col = config.target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cefba9-401a-4603-b174-cd5e29188b38",
   "metadata": {},
   "source": [
    "Looks good! All our five folds are stratified!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba7397-7a78-495e-aef7-c7c0f1a8ebff",
   "metadata": {},
   "source": [
    "# Define Metrics\n",
    "\n",
    "**Disclaimer: For a more detailed understanding of different metrics, do navigate to my self-made notes on metrics.**\n",
    "\n",
    "---\n",
    "\n",
    "Choosing a metric to measure the classifier's (hypothesis) performance is important, as choosing the wrong one can lead to disastrous interpretations. One prime example is using the accuracy metric for imbalanced datasets; consider 1 mil data points, dichotomized by $99\\%$ benign and $1\\%$ malignant samples, even a baseline model zeroR model which predicts the majority class no matter the process will give a $99\\%$ \\textbf{accuracy}, completely missing out any positive samples, which unfortunately, is what we may be more interested in.\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Say No to Accuracy:</b> Consider an imbalanced set, where the training data set has 100 patients (data points), and the ground truth is 90 patients are of class = 0, which means that these patients do not have cancer, whereas the remaining 10 patients are in class 1, where they do have cancer. This is an example of class imbalance where the ratio of class 1 to class 0 is 1:9.\n",
    "</div>   \n",
    "    \n",
    "Consider **a baseline (almost trivial) classifier**:\n",
    "\n",
    "```python\n",
    "def zeroR(patient_data):\n",
    "        training...\n",
    "    return benign\n",
    "```\n",
    "        \n",
    "\n",
    "where we predict the patient's class as the most frequent class. Meaning, the most frequent class in this question is the class = 0, where patients do not have cancer, so we just assign this class to everyone in this set. By doing this, we will inevitably achieve a **in-sample** accuracy rate of $\\frac{90}{100} = 90\\%$. But unfortunately, this supposedly high accuracy value is completely useless, because this classifier did not label any of the cancer patients correctly.\n",
    "\n",
    "The consequence can be serious, assuming the test set has the same distribution as our training set, where if we have a test set of 1000 patients, there are 900 negative and 100 positive. Our model just literally predict every one of them as benign, yielding a $90\\%$ **out-of-sample** accuracy.\n",
    "\n",
    "What did we conclude? Well, for one, our `accuracy` can be 90% high and looks good to the laymen, but it failed to predict the most important class of people - yes, misclassifying true cancer patients as healthy people is very bad! \n",
    "\n",
    "---\n",
    "\n",
    "For the reasons mentioned above, we will use metric that can help us reduce False Negatives, and at the same time, outputs meaningful predictions. In order to achieve for both, we will use **Receiver operating characteristic (ROC)** as the primary metric for the model to maximize (which is our $\\mathcal{M}$, and **Brier Score**, a [proper scoring rule](https://en.wikipedia.org/wiki/Scoring_rule) to measure the performance of our probabilistic predictions. We will go into some details in the next two subsections to justify our choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80441f4f-1068-47be-98ec-40a78ff9a464",
   "metadata": {},
   "source": [
    "## Proper Scoring Rule\n",
    "\n",
    "The math behind the idea of Proper Scoring Rule is non-trivial. Here, we try to understand why a proper scoring rule is desired in the context of binary classification.\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<li> <b>Strictly Proper Scoring Rule:</b> Brier Score Loss, for example, tells us that the best possible score, 0 (lowest loss), is obtained if and only if, the probability prediction we get for a sample, is the true probability itself. In other words, if a selected sample is of class 1, our prediction for this must be 1, with 100% probability, in order to get a score loss of 0.\n",
    "    \n",
    "<li> <b>Proper Scoring Rule:</b> Read [here](https://stats.stackexchange.com/questions/339919/what-does-it-mean-that-auc-is-a-semi-proper-scoring-rule) for this.\n",
    "    \n",
    "<li> <b>Semi Proper Scoring Rule:</b> AUROC, as mentioned, does not help out in telling whether a prediction by a classifier is close to the true probability or not. In our example, we even see that we can obtain a full score of 1, even if the probabilities all lie within 0.51 and 0.52.\n",
    "\n",
    "<li> <b>Improper Scoring Rule:</b> Accuracy is a prime example, the accuracy score does not, whatsoever, tells us about how close our predicted probabilities are, to the true probability distribution of our samples.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea1a9e5-0c11-4413-ae01-103ca35481d1",
   "metadata": {},
   "source": [
    "## Receiver operating characteristic (ROC)\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <b>Definition:</b> The basic (non-probablistic intepretation) of ROC is graph that plots the True Positive Rate on the y-axis and False Positive Rate on the x-axis parametrized by a threshold vector $\\vec{t}$. We then look at the area under the ROC curve (AUROC) to get an overall performance measure.\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "The choice of ROC over other metrics such as Accuracy is detailed initially. **We also established we want to reduce False Negative (FN), since misclassifying a positive patient as benign is way more costly than the other way round.** One can choose to minimize **Recall** in order to reduce FN, but this is less than ideal during training because it is a thresholded metric, and does not provide at which threshold the recall is at minimum. This leads us to choose ROC for the following two main reasons:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f25c1a-8571-4fd1-99ab-421d9bce6d13",
   "metadata": {},
   "source": [
    "### Threshold Invariant\n",
    "\n",
    "By definition, ROC computes the pair $TPR \\times FPR$ over all thresholds $t$, consequently, the AUROC is threshold invariant, allowing us to look at the model's performance over all thresholds. We note that ROC may not be that reliable in the case of very imbalanced datasets where majority is in the negative class, as $FPR = \\dfrac{FP}{FP+TN}$ may seem deceptively low as denominator may be made small by the sheer amount of TN, in this case, we may also look at the Precision-Recall curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf428f-42e5-4723-abc5-bfb49834a7c7",
   "metadata": {},
   "source": [
    "### Scale Invariant\n",
    "\n",
    "Technically, this is not the desired property that we need, as this means that the ROC is non-proper in scoring, it can take in non-calibrated scores and still perform relatively well. A classic example I always use is the following:\n",
    "\n",
    "```python\n",
    "y1 = [1,0,1,0]\n",
    "y2 = [0.52,0.51,0.52,0.51]\n",
    "y3 = [52,51,52,51]\n",
    "uncalibrated_roc = roc(y1,y2) == roc(y1,y3)\n",
    "print(f\"{uncalibrated_roc}\") -> 1.0\n",
    "```\n",
    "\n",
    "The example tells us two things, as long as the ranking of predictions is preserved, the final AUROC score is the same, regardless of scale. We also notice that even though the model gives very unconfident predictions, the AUROC score is 1, which can be misleadingly over-optimistic. With that, we introduce Brier Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afd660-747d-4014-8b08-2aa6358e2294",
   "metadata": {},
   "source": [
    "### Common Pitfalls\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Careful when using ROC function!</b>   \n",
    "    \n",
    "We also note that when passing arguments to scikit-learn's <code>roc_auc_score</code> function, we should be careful not to pass <code>y_score=model.predict(X)</code> inside as we have to understand that we are passing in <b>non-thresholded</b> probabilities into <code>y_score</code>. If you pass the predicted values (full of 0 and 1s), then you are thresholding on 0 and 1 only, which is incorrect by definition. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea2946-0cba-48e4-bc68-f1d2d4df1249",
   "metadata": {},
   "source": [
    "## Brier Score\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <b>Definition:</b> Brier Score computes the squared difference between the probability of a prediction and its actual outcome. \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "[Brier Score](https://en.wikipedia.org/wiki/Brier_score) is a strictly proper scoring rule while ROC is [not](https://www.fharrell.com/post/class-damage/); the lower the Brier Score, the better the predictions are calibrated. We can first compute the AUROC score of the model, and compute Brier Score to give us how well calibrated (confident) the predictions are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7f29a-bac7-43cb-8a16-995b87db02b1",
   "metadata": {},
   "source": [
    "### Well Calibrated\n",
    "\n",
    "A intuitive way of understanding well calibrated probabilities is as follows, extracted from [cambridge's probability calibration](https://blog.cambridgespark.com/probability-calibration-c7252ac123f):\n",
    "\n",
    "> In very simple terms, these are probabilities which can be interpreted as a confidence interval. Furthermore, a classifier is said to produce well calibrated probabilities if for the instances (data points) receiving probability 0.5, 50% of those instances belongs to the positive class.\n",
    "\n",
    "---\n",
    "\n",
    "In my own words, if a classifier is well calibrated, say in our context where we predict binary target, and pretend that out of our test set, 100 of the samples have a probability of around 0.1, then this means 10% of these 100 samples actually belong to the positive class.\n",
    "\n",
    "The generic steps are as follows to calculate a calibrated plot:\n",
    "\n",
    "1. Sort all the samples by the classifier's predicted probabilities, in either ascending or descending order.\n",
    "2. Bin your diagram into N bins, usually we take 10, which means on the X-axis, note this does not mean we have 0-0.1, 0.1-0.2, ..., 0.9-1 as the 10 bins.\n",
    "3. What step 2 means is let's say you have 100 predictions, if you bin by 10 bins, and since the predictions are ***sorted***, we can easily divide the 100 predictions into 10 intervals: for illustration, assume the 100 predictions are as follows, where we sort by ascending order and the prediction 0.1 has 10 of them, 0.2 have 10 of them, so on and so forth.\n",
    "    ```python\n",
    "    y_pred = [0.1, 0.1, ....., 0.2, 0.2, ..., 0.9, 0.9, ..., 1, 1, ...1]\n",
    "    ```\n",
    "4. Since we can divide the above into 10 bins, bin 1 will have 10 samples of predictions 0.1, bin 2 will have 10 samples of predictions 0.2, etc. We then take the mean of the **predictions of each bin**, that is for the first bin, we calculate $\\dfrac{1}{10}\\sum_{i=1}^{10}0.1 = 0.1$, and second bin, $\\dfrac{1}{10}\\sum_{i=1}^{10}0.2 = 0.2$. Note that this may not be such a nice number in reality, I made this example for the ease of illustration!\n",
    "5. Now, we have our X-axis from step 4, that is, we turned 10 bins, into 10 numbers, 0.1, 0.2, 0.3, ..., 1, and then we need to find the corresponding points for each of the 10 numbers! This is easy, for 0.1, the corresponding y-axis is just the **fraction of positives**, which means, out of the 10 samples in the first bin, how many of these 10 samples were actually positive? We do this for all 10 bins (points), and plot a line graph as seen in scikit-learn.  \n",
    "\n",
    "---\n",
    "\n",
    "Now this should be apparent now that a well calibrated model should lie close to the $y = x$ line. That is, if the mean predicted probability is 0.1, then the y-axis should also be 0.1, meaning to say that out of all the samples that were predicted as 0.1, we should really only have about 10% of them being positive. The same logic applies to the rest!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d66a4f1-c643-4189-a7a6-0320deeda71b",
   "metadata": {},
   "source": [
    "### Brier Score Loss\n",
    "\n",
    "Brier Score Loss is a handy metric to measure whether a classifier is well calibrated, as quoted from [scikit-learn](https://scikit-learn.org/stable/modules/calibration.html):\n",
    "\n",
    "> Brier Score Loss may be used to assess how well a classifier is calibrated. However, this metric should be used with care because a lower Brier score does not always mean a better calibrated model. This is because the Brier score metric is a combination of calibration loss and refinement loss. Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve. As refinement loss can change independently from calibration loss, a lower Brier score does not necessarily mean a better calibrated model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20bf8a-79ee-43e1-98ea-278e10b352a8",
   "metadata": {},
   "source": [
    "### Common Pitfalls\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Class Imbalance:</b> The good ol' class imbalance issue almost always pop up anywhere and everywhere. Intuitively, if we have a super rare positive/negative class, then if the model is very confident in its predictions for the majority class, but not so confident on the rare class, the overall Brier Score Loss may not be sufficient in discriminating the classifier's inability in correctly classifying the minority class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317251f-2d2a-4859-a15c-c9a2bda7f8d5",
   "metadata": {},
   "source": [
    "# Spot Checking Algorithms\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <b>Terminology Alert!</b> This method is advocated by <a href=\"https://machinelearningmastery.com/\">Jason Brownlee PhD</a> and this serves as the first stage of my modelling process. We will rapidly test (spot check) different classifier algorithms, from <code>DummyClassifier</code>, to <code>LinearModel</code> to more sophisticated ensemble trees like <code>RandomForest</code>. \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "I also note to the readers that we need to think of a few things when choosing the \"optimal\" machine learning algorithm:\n",
    "\n",
    "- [No Lunch Free Theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) intuitively says that no single optimization algorithm can work best in all situations. Therefore, spot checking can help us form a basis of which algorithm might work better in this particular scenario.\n",
    "- [Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor) often appears in many Machine Learning textbook, and the narrative is that a simpler model more often times generalizes better than a complex model. This is not unfamiliar when we think of the bias-variance tradeoff, and that is why there is always a tradeoff that we must make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f00969-f1d4-47c3-a04d-6c9f877c8245",
   "metadata": {},
   "source": [
    "## Say No to Data Leakage!\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Say No to Data Leakage:</b> This has been emphasized throughout and we must be careful as we should never touch the test set when fitting the model. In fact, we should try our best to not <b>contaminate</b> our validation set as well.\n",
    "    <li> This means that preprocessing steps such as <code>StandardScaling()</code> should only be fitted on the training data, and then apply the same transformation (mean and std) on the test data. In other words, do not apply scaling on the whole dataset before splitting. \n",
    "    <li> However, it is also equally important to take note <b>not to contaminate</b> our validation set, which is often overlooked, resulting in over optimistic results from model selection phase, but perform badly on unseen test set. As a result, when we use a 5 fold cross validation, we should be careful during fitting that the preprocessing steps are only applied on the training folds, and not on all 5 folds.\n",
    "    <li> The same idea is also applied to our <code>ReduceVIF()</code> preprocessing step. We should also include this in our pipeline and not select the features outside the cross-validation loop.</li>\n",
    "</div>   \n",
    "\n",
    "---\n",
    "\n",
    "Quoting from **[scikit-learn](https://scikit-learn.org/stable/common_pitfalls.html)**:\n",
    "\n",
    "> Data leakage occurs when information that would not be available at prediction time is used when building the model. This results in overly optimistic performance estimates, for example from [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation), and thus poorer performance when the model is used on actually novel data, for example during production.\n",
    "\n",
    "> A common cause is not keeping the test and train data subsets separate. Test data should never be used to make choices about the model. **The general rule is to never call fit on the test data.** While this may sound obvious, this is easy to miss in some cases, for example when applying certain pre-processing steps.\n",
    "\n",
    "> Although both train and test data subsets should receive the same preprocessing transformation (as described in the previous section), it is important that these transformations are only learnt from the training data. For example, if you have a normalization step where you divide by the average value, the average should be the average of the train subset, not the average of all the data. If the test subset is included in the average calculation, information from the test subset is influencing the model.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <b>How to avoid Data Leakage?</b> We know the pitfalls of fitting on validation/test data, the natural question is how can we avoid it completely? You can code it up yourself, but as a starter, we can use scikit-learn's <code>Pipeline</code> object. My tips are as follows:\n",
    "<li> Any preprocessing step must be done after splitting the whole dataset into train and test. If you are also using cross-validation, then we should only apply the preprocessing steps on the train set, and then use the metrics obtained from the train set to transform the validation set. You can see my pseudo-code below for a rough outline.\n",
    "    <li> The <code>Pipeline</code> object of Scikit-Learn can help prevent data leakage.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99732b-56c3-491d-8666-c5e7677380f2",
   "metadata": {},
   "source": [
    "## Pseudo-Code of Cross-Validation and Pipeline\n",
    "\n",
    "The below outlines a pseudo code of the cross-validation scheme using `Pipeline` object. Note that I included the most outer loop, which is searching for hyperparameters.\n",
    "\n",
    "- Define $G$ as the set of combination of hyperparamters. Define number of splits to be $K$.\n",
    "- For each set of hyperparameter $z \\in Z$:\n",
    "    - for fold $j$ in K:\n",
    "        - Set $F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}$\n",
    "        - Set $F_{\\text{val}} = F_{j}$ as the validation set\n",
    "        - Perform Standard Scaling on $F_{\\text{train}}$ and find the mean and std\n",
    "        - Perform VIF recursively on $F_{\\text{train}}$ and find the selected features\n",
    "        - Transform $F_{\\text{val}}$ using the mean and std found using $F_{\\text{train}}$\n",
    "        - Transform $F_{\\text{val}}$ to have only the selected features from $F_{\\text{train}}$\n",
    "        - Train and fit on $F_{\\text{train}}$ \n",
    "    - Evaluate the fitted parameters on $F_{\\text{val}}$ to obtain $\\mathcal{M}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aecef8e-238d-4f43-81a5-7f3baafec7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_inflation_factor(exog, idx_kept, vif_idx):\n",
    "    \"\"\"Compute VIF for one feature.\n",
    "    \n",
    "    Args:\n",
    "        exog (np.ndarray): Observations\n",
    "        idx_kept (List[int]): Indices of features to consider\n",
    "        vif_idx (int): Index of feature for which to compute VIF\n",
    "    \n",
    "    Returns:\n",
    "        float: VIF for the selected feature\n",
    "    \"\"\"\n",
    "    exog = np.asarray(exog)\n",
    "    \n",
    "    x_i = exog[:, vif_idx]\n",
    "    mask = [col for col in idx_kept if col != vif_idx]\n",
    "    x_noti = exog[:, mask]\n",
    "    \n",
    "    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n",
    "    vif = 1. / (1. - r_squared_i)\n",
    "    \n",
    "    return vif\n",
    "\n",
    "class ReduceVIF(base.BaseEstimator, base.TransformerMixin):\n",
    "    \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class;\n",
    "    I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, thresh=10, max_drop=20):\n",
    "        self.thresh = thresh\n",
    "        self.max_drop = max_drop\n",
    "        self.column_indices_kept_ = []\n",
    "        self.feature_names_kept_ = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the state of predictor columns after each fold.\"\"\"\n",
    "\n",
    "        self.column_indices_kept_ = []\n",
    "        self.feature_names_kept_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names\n",
    "\n",
    "        Args:\n",
    "            X ([type]): [description]\n",
    "            y ([type], optional): [description]. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.column_indices_kept_, self.feature_names_kept_ = self.calculate_vif(X)     \n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Transforms the Validation Set according to the selected feature names.\n",
    "\n",
    "        Args:\n",
    "            X ([type]): [description]\n",
    "            y ([type], optional): [description]. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "\n",
    "        return X[:, self.column_indices_kept_]\n",
    "\n",
    "    def calculate_vif(self, X: Union[np.ndarray, pd.DataFrame]):\n",
    "        \"\"\"Implements a VIF function that recursively eliminates features.\n",
    "\n",
    "        Args:\n",
    "            X (Union[np.ndarray, pd.DataFrame]): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        feature_names = None\n",
    "        column_indices_kept = list(range(X.shape[1]))\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            feature_names = X.columns\n",
    "\n",
    "        dropped = True\n",
    "        count = 0\n",
    "        \n",
    "        while dropped and count <= self.max_drop:\n",
    "            dropped = False\n",
    "            \n",
    "            max_vif, max_vif_col = None, None\n",
    "            \n",
    "            for col in column_indices_kept:\n",
    "                \n",
    "                vif = variance_inflation_factor(X, column_indices_kept, col)\n",
    "                \n",
    "                if max_vif is None or vif > max_vif:\n",
    "                    max_vif = vif\n",
    "                    max_vif_col = col\n",
    "            \n",
    "            if max_vif > self.thresh:\n",
    "                # print(f\"Dropping {max_vif_col} with vif={max_vif}\")\n",
    "                column_indices_kept.remove(max_vif_col)\n",
    "                \n",
    "                if feature_names is not None:\n",
    "                    feature_names.pop(max_vif_col)\n",
    "                    \n",
    "                dropped = True\n",
    "                count += 1\n",
    "                \n",
    "        return column_indices_kept, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "186a6a2b-8e75-4e1d-adf7-4d55ea8d8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature preparation pipeline for a model\n",
    "def make_pipeline(model):\n",
    "    \"\"\"Make a Pipeline for Training.\n",
    "\n",
    "    Args:\n",
    "        model ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    \n",
    "    steps = list()\n",
    "    # standardization\n",
    "    steps.append(('standardize', preprocessing.StandardScaler()))\n",
    "    # reduce VIF\n",
    "    steps.append((\"remove_multicollinearity\", ReduceVIF(thresh=10)))\n",
    "    # the model\n",
    "    steps.append(('model', model))\n",
    "    # create pipeline\n",
    "    _pipeline = pipeline.Pipeline(steps=steps)\n",
    "    return _pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82edb44c-1c71-4c0e-88e6-ab08860e16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    # baseline model\n",
    "    dummy.DummyClassifier(random_state=config.seed, strategy=\"stratified\"),\n",
    "    # linear model\n",
    "    linear_model.LogisticRegression(\n",
    "        random_state=config.seed, solver=\"liblinear\"\n",
    "    ),\n",
    "#     # nearest neighbours\n",
    "#     neighbors.KNeighborsClassifier(n_neighbors=8),\n",
    "#     # SVM\n",
    "#     svm.SVC(probability=True, random_state=config.seed),\n",
    "#     # tree\n",
    "#     tree.DecisionTreeClassifier(random_state=config.seed),\n",
    "#     # ensemble\n",
    "#     ensemble.RandomForestClassifier(random_state=config.seed),\n",
    "]\n",
    "\n",
    "classifiers = [make_pipeline(model) for model in classifiers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9dc484-c417-46af-b6e9-06249700025d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Careful when using ROC function!</b>   \n",
    "    \n",
    "We also note that when passing arguments to scikit-learn's <code>roc_auc_score</code> function, we should be careful not to pass <code>y_score=model.predict(X)</code> inside as we have to understand that we are passing in <b>non-thresholded</b> probabilities into <code>y_score</code>. If you pass the predicted values (full of 0 and 1s), then you are thresholding on 0 and 1 only, which is incorrect by definition. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d92ec-1d00-4af3-819b-2c9cf8e60c60",
   "metadata": {},
   "source": [
    "### Define Metrics\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Say No to Accuracy:</b> Consider an imbalanced set, where the training data set has 100 patients (data points), and the ground truth is 90 patients are of class = 0, which means that these patients do not have cancer, whereas the remaining 10 patients are in class 1, where they do have cancer. This is an example of class imbalance where the ratio of class 1 to class 0 is 1:9.\n",
    "</div>   \n",
    "    \n",
    "Consider **a baseline (almost trivial) classifier**:\n",
    "\n",
    "```python\n",
    "def zeroR(patient_data):\n",
    "        training...\n",
    "    return benign\n",
    "```\n",
    "        \n",
    "\n",
    "where we predict the patient's class as the most frequent class. Meaning, the most frequent class in this question is the class = 0, where patients do not have cancer, so we just assign this class to everyone in this set. By doing this, we will inevitably achieve a **in-sample** accuracy rate of $\\frac{90}{100} = 90\\%$. But unfortunately, this supposedly high accuracy value is completely useless, because this classifier did not label any of the cancer patients correctly.\n",
    "\n",
    "The consequence can be serious, assuming the test set has the same distribution as our training set, where if we have a test set of 1000 patients, there are 900 negative and 100 positive. Our model just literally predict every one of them as benign, yielding a $90\\%$ **out-of-sample** accuracy.\n",
    "\n",
    "What did we conclude? Well, for one, our `accuracy` can be 90% high and looks good to the laymen, but it failed to predict the most important class of people - yes, misclassifying true cancer patients as healthy people is very bad! \n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Optimizing Recall over Precision!</b>   \n",
    "    \n",
    "This is often a choice to make in business, from the confusion matrix and the out of fold roc scores, we are indeed approaching quite high score even without hyperparameter tuning. In the context of cancer prediction, it is often more important to minimize the False Negatives (assuming malignant to be the positive class), more than anything else. This is because the <b>cost</b> of wrongly classifying a malignant patient as benign is way more than wrong classifying a benign patient as malignant. \n",
    "    However, one needs to be careful not to <b> optimize the recall score </b> by calling the <code>recall_score()</code> function. This is because the recall function is already thresholded and you won't be able to optimize it any further. We will use ROC score as the metric to measure our performance, we will also show how we can choose a particular threshold from the ROC curve later. We may also introduce Precision-Recall curve to compare with ROC curve.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4048fe8-e047-4a3f-9913-cc500d2cc7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_result_names = [\"y_true\", \"y_pred\", \"y_prob\"]\n",
    "\n",
    "default_logit_names = [\"y_true\", \"y_pred\", \"y_prob\"]\n",
    "default_score_names = [\n",
    "    \"accuracy_score\",\n",
    "    \"precision_recall_fscore_support\",\n",
    "    \"confusion_matrix\",\n",
    "    # \"average_precision_score\",\n",
    "    \"multiclass_roc_auc_score\",\n",
    "    \"brier_score_loss\"\n",
    "]\n",
    "\n",
    "custom_score_names = [\"multiclass_roc_auc_score\", \"brier_score_loss\"]\n",
    "\n",
    "use_preds = [\n",
    "    \"accuracy_score\",\n",
    "    \"precision_recall_fscore_support\",\n",
    "    \"confusion_matrix\",\n",
    "]\n",
    "use_probs = [\"average_precision_score\"]\n",
    "custom_score_names = [\"multiclass_roc_auc_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6129e2e-23d6-4f03-8f9c-67926f8c0ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results:\n",
    "    \"\"\"Stores results for model training in columnwise format.\"\"\"\n",
    "    \n",
    "    _result_dict: Dict\n",
    "        \n",
    "    logit_names: List[str]\n",
    "    score_names: List[str]\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        logit_names: List[str] = default_logit_names,\n",
    "        score_names: List[str] = default_score_names,\n",
    "        existing_dict: Optional[Dict] = None,\n",
    "    ):\n",
    "        \"\"\"Construct a new results store.\"\"\"       \n",
    "        self.logit_names = logit_names\n",
    "        self.score_names = score_names\n",
    "        \n",
    "        if existing_dict is not None:\n",
    "            self._result_dict = copy.deepcopy(existing_dict)\n",
    "            return\n",
    "        \n",
    "        dict_keys = [\"identifier\", *logit_names, *score_names]\n",
    "        \n",
    "        self._result_dict = {\n",
    "            key: [] for key in dict_keys\n",
    "        }\n",
    "    \n",
    "    def add(self, identifier: str, results: Dict, in_place=False):\n",
    "        \"\"\"Add a new results row.\"\"\"        \n",
    "        if not in_place:\n",
    "            return Results(\n",
    "                self.logit_names,\n",
    "                self.score_names,\n",
    "                self._result_dict\n",
    "            ).add(identifier, results, in_place=True)\n",
    "        \n",
    "        self._result_dict[\"identifier\"].append(identifier)\n",
    "        \n",
    "        for result_name in set([*results.keys(), *self.logit_names, *self.score_names]):\n",
    "            \n",
    "            result_value = results.get(result_name, np.nan)\n",
    "            \n",
    "            self._result_dict[result_name].append(result_value)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_result(self, result_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get a map of identifiers to result values for a result.\"\"\"\n",
    "        return {\n",
    "            identifier: result_value for\n",
    "            identifier, result_value in\n",
    "            zip(self._result_dict[\"identifier\"], self._result_dict[result_name])\n",
    "        }\n",
    "    \n",
    "    def get_result_values(self, result_name: str) -> List[Any]:\n",
    "        \"\"\"Get a list of values for a result.\"\"\"\n",
    "        return self._result_dict[result_name]\n",
    "    \n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Get a Data Frame containing the results.\"\"\"\n",
    "        return pd.DataFrame.from_dict(self._result_dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Get a dictionary containing the results.\n",
    "        \n",
    "        Returns:\n",
    "             Dict[str, List[Any]]: Dictionary of result columns \n",
    "        \"\"\"\n",
    "        return self._result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55152d7e-9e5d-412b-86ab-58464b51f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_label_binarize(\n",
    "    y: np.ndarray, class_labels: List[int], pos_label=1, neg_label=0\n",
    "):\n",
    "    \"\"\"Binarize labels in one-vs-all fashion.\n",
    "    # TODO: to replace with the above vstack method.\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray) Sequence of integer labels to encode\n",
    "        class_labels (array-like) Labels for each class\n",
    "        pos_label (int) Value for positive labels\n",
    "        neg_label (int) Value for negative labels\n",
    "    Returns:\n",
    "        np.ndarray of shape (n_samples, n_classes) Encoded dataset\n",
    "    \"\"\"\n",
    "    if isinstance(y, list):\n",
    "        y = np.asarray(y)\n",
    "\n",
    "    columns = [\n",
    "        np.where(y == label, pos_label, neg_label) for label in class_labels\n",
    "    ]\n",
    "\n",
    "    return np.column_stack(columns)\n",
    "\n",
    "\n",
    "def multiclass_roc_auc_score(y_true, y_score, classes=None):\n",
    "    \"\"\"Compute ROC-AUC score for each class in a multiclass dataset.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray of shape (n_samples, n_classes)) True labels\n",
    "        y_score (np.ndarray of shape (n_samples, n_classes)) Target scores\n",
    "        classes (array-like of shape (n_classes,)) List of dataset classes. If `None`,\n",
    "            the lexicographical order of the labels in `y_true` is used.\n",
    "    \n",
    "    Returns:\n",
    "        array-like: ROC-AUC score for each class, in the same order as `classes`\n",
    "    \"\"\"\n",
    "    classes = (\n",
    "        np.unique(y_true) if classes is None\n",
    "        else classes\n",
    "    )\n",
    "    \n",
    "    y_true_multiclass = multiclass_label_binarize(\n",
    "        y_true,\n",
    "        class_labels=classes\n",
    "    )\n",
    "    \n",
    "    def oneclass_roc_auc_score(class_id):\n",
    "        y_true_class = y_true_multiclass[:, class_id]\n",
    "        y_score_class = y_score[:, class_id]\n",
    "        \n",
    "        fpr, tpr, _ = metrics.roc_curve(\n",
    "            y_true=y_true_class,\n",
    "            y_score=y_score_class,\n",
    "            pos_label=1\n",
    "        )\n",
    "        \n",
    "        return metrics.auc(\n",
    "            fpr,\n",
    "            tpr\n",
    "        )\n",
    "    \n",
    "    return [\n",
    "        oneclass_roc_auc_score(class_id)\n",
    "        for class_id in range(len(classes))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc47dca3-18a0-45e2-a108-7e791db3684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_y(y):\n",
    "    return (\n",
    "        y.ravel()\n",
    "        if config.classification_type == \"binary\"\n",
    "        else y\n",
    "    )\n",
    "\n",
    "def mean_score(score_values) -> Union[float, np.ndarray]:\n",
    "    \"\"\"Compute the mean score.\"\"\"\n",
    "    \n",
    "    score_values = np.array(score_values)\n",
    "    \n",
    "    shape = score_values.shape\n",
    "    \n",
    "    if len(shape) == 1:\n",
    "        return score_values.mean()\n",
    "    \n",
    "    return score_values.mean(axis=0)\n",
    "\n",
    "def mean_cv_results(model_results: Results) -> Dict:\n",
    "    \"\"\"Add mean cross-validation results.\n",
    "    \n",
    "    This method computes the mean value for all\n",
    "    score types in the model_results, including\n",
    "    for scores (e.g., confusion matrix) where\n",
    "    the mean value may contain decimal places.\n",
    "    \"\"\"\n",
    "    cv_logits = {\n",
    "        y_result: np.concatenate(model_results.get_result_values(y_result))\n",
    "        for y_result in\n",
    "        model_results.logit_names\n",
    "    }\n",
    "    \n",
    "    cv_scores = {\n",
    "        score: mean_score(\n",
    "            model_results.get_result_values(score)\n",
    "        )\n",
    "        for score in model_results.score_names\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        **cv_logits,\n",
    "        **cv_scores,\n",
    "    }\n",
    "\n",
    "def oof_cv_results(model_results: Results) -> Dict:\n",
    "    \"\"\"Add OOF cross-validation results.\"\"\"\n",
    "    \n",
    "    cv_logits = {\n",
    "        y_result: np.concatenate(\n",
    "            model_results.get_result_values(y_result)\n",
    "        )\n",
    "        for y_result in\n",
    "        model_results.logit_names\n",
    "    }\n",
    "    \n",
    "    cv_scores = compute_metrics(cv_logits)\n",
    "    \n",
    "    return {\n",
    "        **cv_logits,\n",
    "        **cv_scores,\n",
    "    }\n",
    "\n",
    "def add_cv_results(model_results: Results):\n",
    "    \"\"\"Add cross-validation results.\n",
    "    \n",
    "    This method returns a copy of the given model results\n",
    "    with summary columns for mean and CV cross-validation.\n",
    "    \"\"\"\n",
    "    mean_cv = mean_cv_results(model_results)\n",
    "    oof_cv = oof_cv_results(model_results)\n",
    "    \n",
    "    return (\n",
    "        model_results\n",
    "        .add(\"mean_cv\", mean_cv)\n",
    "        .add(\"oof_cv\", oof_cv)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "662bbaa5-ad1c-4a7c-8d24-7baa77d73619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits: Dict[str, np.ndarray]) -> Dict[str, Any]:\n",
    "    \"\"\"Compute metrics from logits.\n",
    "    use_probs: all metrics that use probabilities.\n",
    "    use_preds: all metrics that use thresholded predictions.\n",
    "    \n",
    "    # TODO add this\n",
    "    precision, recall, fbeta_score, _ = metrics.precision_recall_fscore_support(\n",
    "        y_true=y_val,\n",
    "        y_pred = y_val_pred,\n",
    "        labels=np.unique(y_val),\n",
    "        average=None\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    y_true, y_pred, y_prob = (\n",
    "        logits[\"y_true\"],\n",
    "        logits[\"y_pred\"],\n",
    "        logits[\"y_prob\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    use_preds = [\n",
    "        \"accuracy_score\",\n",
    "        \"precision_recall_fscore_support\",\n",
    "        \"confusion_matrix\",\n",
    "    ]\n",
    "    use_probs = [\"average_precision_score\"]\n",
    "\n",
    "    default_metrics_dict: Dict[str, float] = {}\n",
    "    custom_metrics_dict: Dict[str, float] = {}\n",
    "\n",
    "    for metric_name in default_score_names:\n",
    "        if hasattr(metrics, metric_name):\n",
    "            # TODO: get metric score with default parameters, consider adding kwargs if you want to configure parameters\n",
    "            if metric_name in use_preds:\n",
    "                metric_score = getattr(metrics, metric_name)(\n",
    "                    y_true, y_pred\n",
    "                )\n",
    "            elif metric_name in use_probs:\n",
    "                # logger.info(\"TODO: write custom scores for precision-recall as here is hardcoded\")\n",
    "                pass\n",
    "#                 metric_score = getattr(metrics, metric_name)(\n",
    "#                     y_true, y_prob\n",
    "#                 )\n",
    "        else:\n",
    "            # add custom metrics here\n",
    "            multiclass_roc_auc = multiclass_roc_auc_score(y_true, y_prob)\n",
    "            brier_score_loss = (\n",
    "                metrics.brier_score_loss(y_true=y_true, y_prob=y_prob[:, 1])\n",
    "                if config.classification_type == \"binary\"\n",
    "                else np.nan\n",
    "            )\n",
    "            custom_metrics_dict[\"multiclass_roc_auc_score\"] = multiclass_roc_auc\n",
    "            custom_metrics_dict[\"brier_score_loss\"] = brier_score_loss\n",
    "\n",
    "        if metric_name not in default_metrics_dict:\n",
    "            default_metrics_dict[metric_name] = metric_score\n",
    "\n",
    "        metrics_dict = {**default_metrics_dict, **custom_metrics_dict}\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f25cb461-dca0-43af-963a-59ae478a4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_fold(\n",
    "    df_folds: pd.DataFrame,\n",
    "    models: List[Callable],\n",
    "    num_folds: int,\n",
    "    predictor_col: List,\n",
    "    target_col: List,\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results.\n",
    "\n",
    "    Args:\n",
    "        df_folds (pd.DataFrame): [description]\n",
    "        model (Callable): [description]\n",
    "        num_folds (int): [description]\n",
    "        predictor_col (List): [description]\n",
    "        target_col (List): [description]\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List]: [description]\n",
    "    \"\"\"\n",
    "  \n",
    "    y_true = df_folds[target_col].values.flatten()\n",
    "\n",
    "    # test_pred_arr: np.ndarray = np.zeros(len(X_test))\n",
    "\n",
    "    model_dict = {}\n",
    "\n",
    "    for model in models:\n",
    "        model_results = Results()\n",
    "\n",
    "        if isinstance(model, pipeline.Pipeline):\n",
    "            model_name = model[\"model\"].__class__.__name__\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "\n",
    "        # out-of-fold validation predictions\n",
    "        oof_pred_arr: np.ndarray = np.zeros(len(df_folds))\n",
    "      \n",
    "        for fold in range(1, num_folds + 1):\n",
    "\n",
    "            train_df = df_folds[df_folds[\"fold\"] != fold].reset_index(drop=True)\n",
    "            val_df = df_folds[df_folds[\"fold\"] == fold].reset_index(drop=True)\n",
    "            val_idx = df_folds[df_folds[\"fold\"] == fold].index.values\n",
    "            X_train, y_train = train_df[predictor_col].values, prepare_y(train_df[target_col].values)\n",
    "            X_val, y_val = val_df[predictor_col].values, prepare_y(val_df[target_col].values)\n",
    "    \n",
    "            model.fit(X_train, y_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            y_val_prob = model.predict_proba(X_val)\n",
    "            \n",
    "            logits = {\n",
    "                \"y_true\": y_val,\n",
    "                \"y_pred\": y_val_pred,\n",
    "                \"y_prob\": y_val_prob,\n",
    "            }\n",
    "            \n",
    "            metrics = compute_metrics(logits)\n",
    "            \n",
    "            model_results.add(f\"fold {fold}\", {\n",
    "                **logits,\n",
    "                **metrics\n",
    "            }, in_place=True)\n",
    "            \n",
    "           \n",
    "        if model_name not in model_dict:\n",
    "            model_dict[model_name] = model_results\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "283e267b-9975-476a-8319-086ba16624f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = train_on_fold(\n",
    "    df_folds,\n",
    "    models = classifiers,\n",
    "    num_folds=5,\n",
    "    predictor_col=predictor_cols,\n",
    "    target_col = config.target_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0706c4a8-e825-4c30-8f8f-a673871ae6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_with_summary = {\n",
    "    model: add_cv_results(model_results)\n",
    "    for model, model_results in model_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57cc722c-73f4-4225-aef9-0117594aa578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>fold 5</th>\n",
       "      <th>mean_cv</th>\n",
       "      <th>oof_cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">DummyClassifier</th>\n",
       "      <th>identifier</th>\n",
       "      <td>fold 1</td>\n",
       "      <td>fold 2</td>\n",
       "      <td>fold 3</td>\n",
       "      <td>fold 4</td>\n",
       "      <td>fold 5</td>\n",
       "      <td>mean_cv</td>\n",
       "      <td>oof_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_true</th>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_pred</th>\n",
       "      <td>[1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_prob</th>\n",
       "      <td>[[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...</td>\n",
       "      <td>[[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...</td>\n",
       "      <td>[[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...</td>\n",
       "      <td>[[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...</td>\n",
       "      <td>[[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...</td>\n",
       "      <td>[[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...</td>\n",
       "      <td>[[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_score</th>\n",
       "      <td>0.524272</td>\n",
       "      <td>0.543689</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.513592</td>\n",
       "      <td>0.513672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_recall_fscore_support</th>\n",
       "      <td>([0.6229508196721312, 0.38095238095238093], [0...</td>\n",
       "      <td>([0.65, 0.3953488372093023], [0.6, 0.447368421...</td>\n",
       "      <td>([0.5932203389830508, 0.32558139534883723], [0...</td>\n",
       "      <td>([0.6779661016949152, 0.4418604651162791], [0....</td>\n",
       "      <td>([0.559322033898305, 0.27906976744186046], [0....</td>\n",
       "      <td>[[0.6206918588496804, 0.364562569213732], [0.5...</td>\n",
       "      <td>([0.6208053691275168, 0.3644859813084112], [0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion_matrix</th>\n",
       "      <td>[[38, 26], [23, 16]]</td>\n",
       "      <td>[[39, 26], [21, 17]]</td>\n",
       "      <td>[[35, 29], [24, 14]]</td>\n",
       "      <td>[[40, 24], [19, 19]]</td>\n",
       "      <td>[[33, 31], [26, 12]]</td>\n",
       "      <td>[[37.0, 27.2], [22.6, 15.6]]</td>\n",
       "      <td>[[185, 136], [113, 78]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multiclass_roc_auc_score</th>\n",
       "      <td>[0.5020032051282052, 0.5020032051282051]</td>\n",
       "      <td>[0.5236842105263158, 0.5236842105263158]</td>\n",
       "      <td>[0.4576480263157895, 0.4576480263157895]</td>\n",
       "      <td>[0.5625, 0.5625]</td>\n",
       "      <td>[0.41570723684210525, 0.4157072368421053]</td>\n",
       "      <td>[0.4923085357624831, 0.4923085357624831]</td>\n",
       "      <td>[0.49235047544486304, 0.4923504754448631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brier_score_loss</th>\n",
       "      <td>0.475728</td>\n",
       "      <td>0.456311</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.421569</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.486408</td>\n",
       "      <td>0.486328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">LogisticRegression</th>\n",
       "      <th>identifier</th>\n",
       "      <td>fold 1</td>\n",
       "      <td>fold 2</td>\n",
       "      <td>fold 3</td>\n",
       "      <td>fold 4</td>\n",
       "      <td>fold 5</td>\n",
       "      <td>mean_cv</td>\n",
       "      <td>oof_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_true</th>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_pred</th>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_prob</th>\n",
       "      <td>[[0.005260701104841559, 0.9947392988951584], [...</td>\n",
       "      <td>[[2.955594684439511e-06, 0.9999970444053156], ...</td>\n",
       "      <td>[[0.9986774529087579, 0.0013225470912421409], ...</td>\n",
       "      <td>[[0.9991782455619103, 0.0008217544380897519], ...</td>\n",
       "      <td>[[0.9941673240706257, 0.005832675929374318], [...</td>\n",
       "      <td>[[0.005260701104841559, 0.9947392988951584], [...</td>\n",
       "      <td>[[0.005260701104841559, 0.9947392988951584], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_score</th>\n",
       "      <td>0.980583</td>\n",
       "      <td>0.970874</td>\n",
       "      <td>0.95098</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.95098</td>\n",
       "      <td>0.964801</td>\n",
       "      <td>0.964844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_recall_fscore_support</th>\n",
       "      <td>([0.984375, 0.9743589743589743], [0.984375, 0....</td>\n",
       "      <td>([0.9558823529411765, 1.0], [1.0, 0.9210526315...</td>\n",
       "      <td>([0.9538461538461539, 0.9459459459459459], [0....</td>\n",
       "      <td>([0.9692307692307692, 0.972972972972973], [0.9...</td>\n",
       "      <td>([0.9402985074626866, 0.9714285714285714], [0....</td>\n",
       "      <td>[[0.9607265566961573, 0.9729412929412931], [0....</td>\n",
       "      <td>([0.9604863221884499, 0.9726775956284153], [0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion_matrix</th>\n",
       "      <td>[[63, 1], [1, 38]]</td>\n",
       "      <td>[[65, 0], [3, 35]]</td>\n",
       "      <td>[[62, 2], [3, 35]]</td>\n",
       "      <td>[[63, 1], [2, 36]]</td>\n",
       "      <td>[[63, 1], [4, 34]]</td>\n",
       "      <td>[[63.2, 1.0], [2.6, 35.6]]</td>\n",
       "      <td>[[316, 5], [13, 178]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multiclass_roc_auc_score</th>\n",
       "      <td>[0.9979967948717948, 0.9979967948717948]</td>\n",
       "      <td>[0.9971659919028341, 0.997165991902834]</td>\n",
       "      <td>[0.9950657894736843, 0.9950657894736842]</td>\n",
       "      <td>[0.9917763157894738, 0.9917763157894737]</td>\n",
       "      <td>[0.9925986842105263, 0.9925986842105263]</td>\n",
       "      <td>[0.9949207152496626, 0.9949207152496626]</td>\n",
       "      <td>[0.9949438110616365, 0.9949438110616365]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brier_score_loss</th>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.01789</td>\n",
       "      <td>0.029431</td>\n",
       "      <td>0.02339</td>\n",
       "      <td>0.037862</td>\n",
       "      <td>0.025351</td>\n",
       "      <td>0.025322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                               fold 1  \\\n",
       "DummyClassifier    identifier                                                                  fold 1   \n",
       "                   y_true                           [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "                   y_pred                           [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "                   y_prob                           [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...   \n",
       "                   accuracy_score                                                            0.524272   \n",
       "                   precision_recall_fscore_support  ([0.6229508196721312, 0.38095238095238093], [0...   \n",
       "                   confusion_matrix                                              [[38, 26], [23, 16]]   \n",
       "                   multiclass_roc_auc_score                  [0.5020032051282052, 0.5020032051282051]   \n",
       "                   brier_score_loss                                                          0.475728   \n",
       "LogisticRegression identifier                                                                  fold 1   \n",
       "                   y_true                           [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "                   y_pred                           [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "                   y_prob                           [[0.005260701104841559, 0.9947392988951584], [...   \n",
       "                   accuracy_score                                                            0.980583   \n",
       "                   precision_recall_fscore_support  ([0.984375, 0.9743589743589743], [0.984375, 0....   \n",
       "                   confusion_matrix                                                [[63, 1], [1, 38]]   \n",
       "                   multiclass_roc_auc_score                  [0.9979967948717948, 0.9979967948717948]   \n",
       "                   brier_score_loss                                                          0.018182   \n",
       "\n",
       "                                                                                               fold 2  \\\n",
       "DummyClassifier    identifier                                                                  fold 2   \n",
       "                   y_true                           [1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "                   y_pred                           [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "                   y_prob                           [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...   \n",
       "                   accuracy_score                                                            0.543689   \n",
       "                   precision_recall_fscore_support  ([0.65, 0.3953488372093023], [0.6, 0.447368421...   \n",
       "                   confusion_matrix                                              [[39, 26], [21, 17]]   \n",
       "                   multiclass_roc_auc_score                  [0.5236842105263158, 0.5236842105263158]   \n",
       "                   brier_score_loss                                                          0.456311   \n",
       "LogisticRegression identifier                                                                  fold 2   \n",
       "                   y_true                           [1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "                   y_pred                           [1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "                   y_prob                           [[2.955594684439511e-06, 0.9999970444053156], ...   \n",
       "                   accuracy_score                                                            0.970874   \n",
       "                   precision_recall_fscore_support  ([0.9558823529411765, 1.0], [1.0, 0.9210526315...   \n",
       "                   confusion_matrix                                                [[65, 0], [3, 35]]   \n",
       "                   multiclass_roc_auc_score                   [0.9971659919028341, 0.997165991902834]   \n",
       "                   brier_score_loss                                                           0.01789   \n",
       "\n",
       "                                                                                               fold 3  \\\n",
       "DummyClassifier    identifier                                                                  fold 3   \n",
       "                   y_true                           [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, ...   \n",
       "                   y_pred                           [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "                   y_prob                           [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...   \n",
       "                   accuracy_score                                                            0.480392   \n",
       "                   precision_recall_fscore_support  ([0.5932203389830508, 0.32558139534883723], [0...   \n",
       "                   confusion_matrix                                              [[35, 29], [24, 14]]   \n",
       "                   multiclass_roc_auc_score                  [0.4576480263157895, 0.4576480263157895]   \n",
       "                   brier_score_loss                                                          0.519608   \n",
       "LogisticRegression identifier                                                                  fold 3   \n",
       "                   y_true                           [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, ...   \n",
       "                   y_pred                           [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, ...   \n",
       "                   y_prob                           [[0.9986774529087579, 0.0013225470912421409], ...   \n",
       "                   accuracy_score                                                             0.95098   \n",
       "                   precision_recall_fscore_support  ([0.9538461538461539, 0.9459459459459459], [0....   \n",
       "                   confusion_matrix                                                [[62, 2], [3, 35]]   \n",
       "                   multiclass_roc_auc_score                  [0.9950657894736843, 0.9950657894736842]   \n",
       "                   brier_score_loss                                                          0.029431   \n",
       "\n",
       "                                                                                               fold 4  \\\n",
       "DummyClassifier    identifier                                                                  fold 4   \n",
       "                   y_true                           [0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, ...   \n",
       "                   y_pred                           [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "                   y_prob                           [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...   \n",
       "                   accuracy_score                                                            0.578431   \n",
       "                   precision_recall_fscore_support  ([0.6779661016949152, 0.4418604651162791], [0....   \n",
       "                   confusion_matrix                                              [[40, 24], [19, 19]]   \n",
       "                   multiclass_roc_auc_score                                          [0.5625, 0.5625]   \n",
       "                   brier_score_loss                                                          0.421569   \n",
       "LogisticRegression identifier                                                                  fold 4   \n",
       "                   y_true                           [0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, ...   \n",
       "                   y_pred                           [0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, ...   \n",
       "                   y_prob                           [[0.9991782455619103, 0.0008217544380897519], ...   \n",
       "                   accuracy_score                                                            0.970588   \n",
       "                   precision_recall_fscore_support  ([0.9692307692307692, 0.972972972972973], [0.9...   \n",
       "                   confusion_matrix                                                [[63, 1], [2, 36]]   \n",
       "                   multiclass_roc_auc_score                  [0.9917763157894738, 0.9917763157894737]   \n",
       "                   brier_score_loss                                                           0.02339   \n",
       "\n",
       "                                                                                               fold 5  \\\n",
       "DummyClassifier    identifier                                                                  fold 5   \n",
       "                   y_true                           [0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "                   y_pred                           [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "                   y_prob                           [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...   \n",
       "                   accuracy_score                                                            0.441176   \n",
       "                   precision_recall_fscore_support  ([0.559322033898305, 0.27906976744186046], [0....   \n",
       "                   confusion_matrix                                              [[33, 31], [26, 12]]   \n",
       "                   multiclass_roc_auc_score                 [0.41570723684210525, 0.4157072368421053]   \n",
       "                   brier_score_loss                                                          0.558824   \n",
       "LogisticRegression identifier                                                                  fold 5   \n",
       "                   y_true                           [0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "                   y_pred                           [0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "                   y_prob                           [[0.9941673240706257, 0.005832675929374318], [...   \n",
       "                   accuracy_score                                                             0.95098   \n",
       "                   precision_recall_fscore_support  ([0.9402985074626866, 0.9714285714285714], [0....   \n",
       "                   confusion_matrix                                                [[63, 1], [4, 34]]   \n",
       "                   multiclass_roc_auc_score                  [0.9925986842105263, 0.9925986842105263]   \n",
       "                   brier_score_loss                                                          0.037862   \n",
       "\n",
       "                                                                                              mean_cv  \\\n",
       "DummyClassifier    identifier                                                                 mean_cv   \n",
       "                   y_true                           [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "                   y_pred                           [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "                   y_prob                           [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...   \n",
       "                   accuracy_score                                                            0.513592   \n",
       "                   precision_recall_fscore_support  [[0.6206918588496804, 0.364562569213732], [0.5...   \n",
       "                   confusion_matrix                                      [[37.0, 27.2], [22.6, 15.6]]   \n",
       "                   multiclass_roc_auc_score                  [0.4923085357624831, 0.4923085357624831]   \n",
       "                   brier_score_loss                                                          0.486408   \n",
       "LogisticRegression identifier                                                                 mean_cv   \n",
       "                   y_true                           [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "                   y_pred                           [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "                   y_prob                           [[0.005260701104841559, 0.9947392988951584], [...   \n",
       "                   accuracy_score                                                            0.964801   \n",
       "                   precision_recall_fscore_support  [[0.9607265566961573, 0.9729412929412931], [0....   \n",
       "                   confusion_matrix                                        [[63.2, 1.0], [2.6, 35.6]]   \n",
       "                   multiclass_roc_auc_score                  [0.9949207152496626, 0.9949207152496626]   \n",
       "                   brier_score_loss                                                          0.025351   \n",
       "\n",
       "                                                                                               oof_cv  \n",
       "DummyClassifier    identifier                                                                  oof_cv  \n",
       "                   y_true                           [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "                   y_pred                           [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "                   y_prob                           [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0...  \n",
       "                   accuracy_score                                                            0.513672  \n",
       "                   precision_recall_fscore_support  ([0.6208053691275168, 0.3644859813084112], [0....  \n",
       "                   confusion_matrix                                           [[185, 136], [113, 78]]  \n",
       "                   multiclass_roc_auc_score                 [0.49235047544486304, 0.4923504754448631]  \n",
       "                   brier_score_loss                                                          0.486328  \n",
       "LogisticRegression identifier                                                                  oof_cv  \n",
       "                   y_true                           [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "                   y_pred                           [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "                   y_prob                           [[0.005260701104841559, 0.9947392988951584], [...  \n",
       "                   accuracy_score                                                            0.964844  \n",
       "                   precision_recall_fscore_support  ([0.9604863221884499, 0.9726775956284153], [0....  \n",
       "                   confusion_matrix                                             [[316, 5], [13, 178]]  \n",
       "                   multiclass_roc_auc_score                  [0.9949438110616365, 0.9949438110616365]  \n",
       "                   brier_score_loss                                                          0.025322  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.concat({\n",
    "    name: results.to_dataframe().T\n",
    "    for name, results\n",
    "    in model_dict_with_summary.items()\n",
    "}, axis=0)\n",
    "\n",
    "results_df.columns = ['fold 1', 'fold 2', 'fold 3', 'fold 4', 'fold 5', 'mean_cv', 'oof_cv']\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3989a8-29e2-4938-a3a4-aad63375c0d3",
   "metadata": {},
   "source": [
    "### Comparison of Cross-Validated Models\n",
    "\n",
    "The point of the following comparison is to check how different models are performing across folds. More specifically, if we have 5 folds, we will have a metric score for each fold, subsequently, we can find the standard error of model's performance. We need to be aware of models that have high variance across folds in terms of the metrics performance. This can indicate that the model is highly unstable, and may be a sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12323e8a-36e5-4b8e-9f71-84f4c9caf8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>fold</th>\n",
       "      <th>multiclass_roc_auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>fold 1</td>\n",
       "      <td>0.502003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>fold 2</td>\n",
       "      <td>0.523684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>fold 3</td>\n",
       "      <td>0.457648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>fold 4</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>fold 5</td>\n",
       "      <td>0.415707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>SE</td>\n",
       "      <td>0.025568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>fold 1</td>\n",
       "      <td>0.997997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>fold 2</td>\n",
       "      <td>0.997166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>fold 3</td>\n",
       "      <td>0.995066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>fold 4</td>\n",
       "      <td>0.991776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>fold 5</td>\n",
       "      <td>0.992599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>SE</td>\n",
       "      <td>0.001221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model    fold  multiclass_roc_auc_score\n",
       "0      DummyClassifier  fold 1                  0.502003\n",
       "1      DummyClassifier  fold 2                  0.523684\n",
       "2      DummyClassifier  fold 3                  0.457648\n",
       "3      DummyClassifier  fold 4                  0.562500\n",
       "4      DummyClassifier  fold 5                  0.415707\n",
       "5      DummyClassifier      SE                  0.025568\n",
       "6   LogisticRegression  fold 1                  0.997997\n",
       "7   LogisticRegression  fold 2                  0.997166\n",
       "8   LogisticRegression  fold 3                  0.995066\n",
       "9   LogisticRegression  fold 4                  0.991776\n",
       "10  LogisticRegression  fold 5                  0.992599\n",
       "11  LogisticRegression      SE                  0.001221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_metrics(model_dict: Dict, metric_name:str=\"roc\", pos_label:int=1):\n",
    "    \"\"\"\n",
    "    Summarize metrics of each fold with its standard error.\n",
    "    We also plot a boxplot to show the results.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    \n",
    "    for model_name, model_results in model_dict.items():\n",
    "        result_dict = model_results.get_result(result_name=metric_name)\n",
    "        \n",
    "        tmp_score = []\n",
    "        for fold, metric in result_dict.items():\n",
    "            pos_class_score = metric[pos_label]\n",
    "            results.append((model_name, fold, pos_class_score))\n",
    "            tmp_score.append(pos_class_score)\n",
    "        \n",
    "        # append the Standard Error of K folds\n",
    "        results.append((model_name, \"SE\", np.std(tmp_score, ddof=1) / len(tmp_score) ** 0.5))\n",
    "    \n",
    "    \n",
    "    summary_df = pd.DataFrame(results, columns=[\"model\", \"fold\", metric_name])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    _ = sns.boxplot(x=\"model\", y=metric_name, data=summary_df[(summary_df['model'] != 'DummyClassifier') & (summary_df['fold'] != 'SE')], ax=ax)\n",
    "    \n",
    "   # fig.savefig(config.spot_checking_boxplot, format='png', dpi=300)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "summary_df = summarize_metrics(model_dict = model_dict, metric_name=\"multiclass_roc_auc_score\")\n",
    "display(summary_df.tail(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d07e8-804b-46b7-8943-fb10ffdd8b5c",
   "metadata": {},
   "source": [
    "### Out-of-Fold Confusion Matrix\n",
    "\n",
    "We do have information on the performance of each folds, we now look at the performance of all 5 folds together. Typicall there are two ways to do it, one is to simply take the average of the score of five folds, the other is to take a look at out of folds predictions.\n",
    "\n",
    "---\n",
    "\n",
    "From the confusion matrix of the out of fold performance, Logistic Regression does seem to be a model we can explore on, although slightly lower in terms of overall AUROC score than SVC, it seems to have the quite low False Negatives amongst all. With further hyperparameter tuning and threshold optimization, we can make it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60173744-c169-4cfd-bb25-23152732592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [model for model in model_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1022fde-7449-4f68-9632-03b2f0bc5401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(10, 10))\n",
    "\n",
    "for axes, algo in zip(ax.ravel(), model_names):\n",
    "\n",
    "    cf_mat = results_df.oof_cv[algo].confusion_matrix\n",
    "\n",
    "    #### scores\n",
    "    positive_class_auroc = results_df.oof_cv[algo].multiclass_roc_auc_score[1]\n",
    "\n",
    "    #### annotations\n",
    "    labels = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"]\n",
    "    counts = [\"{0:0.0f}\".format(value) for value in cf_mat.flatten()]\n",
    "    percentages = [\"{0:.2%}\".format(value) for value in cf_mat.flatten() / np.sum(cf_mat)]\n",
    "\n",
    "    #### final annotations\n",
    "    label = (\n",
    "        np.array([f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(labels, counts, percentages)])\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(\n",
    "        data=cf_mat,\n",
    "        vmin=0,\n",
    "        vmax=330,\n",
    "        cmap=[\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"],\n",
    "        linewidth=2,\n",
    "        linecolor=\"white\",\n",
    "        square=True,\n",
    "        ax=axes,\n",
    "        annot=label,\n",
    "        fmt=\"\",\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": 10, \"color\": \"black\", \"weight\": \"bold\", \"alpha\": 0.8},\n",
    "        alpha=1,\n",
    "    )\n",
    "\n",
    "    axes.text(0, -0, \"{}\".format(algo), {\"size\": 12, \"color\": \"black\", \"weight\": \"bold\"})\n",
    "\n",
    "    axes.scatter(1, 1, s=3500, c=\"white\")\n",
    "    axes.text(\n",
    "        0.72,\n",
    "        1.0,\n",
    "        \"AUC: {}\".format(np.round(positive_class_auroc, 3)), \n",
    "        {\"size\": 10, \"color\": \"black\", \"weight\": \"bold\"},\n",
    "    )\n",
    "\n",
    "    ## ticks and labels\n",
    "    axes.set_xticklabels(\"\")\n",
    "    axes.set_yticklabels(\"\")\n",
    "\n",
    "\n",
    "## titles and text\n",
    "fig.text(0, 1.05, \"Out Of Fold Confusion Matrix\", {\"size\": 22, \"weight\": \"bold\"}, alpha=1)\n",
    "fig.text(\n",
    "    0,\n",
    "    1,\n",
    "    \"\"\"This Visualization show the results of various classifiers and there respective\n",
    "results.\"\"\",\n",
    "    {\"size\": 14, \"weight\": \"normal\"},\n",
    "    alpha=0.98,\n",
    ")\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=2.5, w_pad=2.5, h_pad=2.5)\n",
    "# fig.savefig(config.oof_confusion_matrix, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f521fd-054d-4b39-8971-8af2aa1c8dff",
   "metadata": {},
   "source": [
    "### Hypothesis Testing Across Models\n",
    "\n",
    "I am slightly shocked at the performance of plain LogisticRegression, I decide to use an idea from [Hypothesis Testing Across Models](http://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/) to check if the difference is really by chance or not. Note that I will be modifying his code as his code does not split using StratifiedKFold.\n",
    "\n",
    "---\n",
    "\n",
    "The basic idea is to test if two model's difference in scores (in this case roc), is statistically significant or not. However, we note that this method may violate an assumption of Student's t test.\n",
    "\n",
    "---\n",
    "\n",
    "- Null Hypothesis $H_0$: The difference in the performance score of two classifiers is Statistically Significant.\n",
    "- Alternate Hypothesis $H_1$: The difference in the performance score of two classifiers is **not** Statistically Significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25f5a2-9921-4371-a93a-0c5466eef811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_ttest_skfold_cv(\n",
    "    estimator1, estimator2, X, y, cv=10, scoring=None, shuffle=False, random_seed=None\n",
    "):\n",
    "    \"\"\"Modified from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/ttest.py to accomodate StratifiedKFold\"\"\"\n",
    "\n",
    "    if not shuffle:\n",
    "        skf = model_selection.StratifiedKFold(n_splits=cv, shuffle=shuffle)\n",
    "    else:\n",
    "        skf = model_selection.StratifiedKFold(\n",
    "            n_splits=cv, random_state=random_seed, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "    if scoring is None:\n",
    "        if estimator1._estimator_type == \"classifier\":\n",
    "            scoring = \"accuracy\"\n",
    "        elif estimator1._estimator_type == \"regressor\":\n",
    "            scoring = \"r2\"\n",
    "        else:\n",
    "            raise AttributeError(\"Estimator must \" \"be a Classifier or Regressor.\")\n",
    "    if isinstance(scoring, str):\n",
    "        scorer = metrics.get_scorer(scoring)\n",
    "    else:\n",
    "        scorer = scoring\n",
    "\n",
    "    score_diff = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X=X, y=y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        estimator1.fit(X_train, y_train)\n",
    "        estimator2.fit(X_train, y_train)\n",
    "\n",
    "        est1_score = scorer(estimator1, X_test, y_test)\n",
    "        est2_score = scorer(estimator2, X_test, y_test)\n",
    "        score_diff.append(est1_score - est2_score)\n",
    "\n",
    "    avg_diff = np.mean(score_diff)\n",
    "\n",
    "    numerator = avg_diff * np.sqrt(cv)\n",
    "    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (cv - 1))\n",
    "    t_stat = numerator / denominator\n",
    "\n",
    "    pvalue = stats.t.sf(np.abs(t_stat), cv - 1) * 2.0\n",
    "    return float(t_stat), float(pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e1daa-1175-42d3-b398-29e45b066e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if difference between algorithms is real\n",
    "X_tmp = X_y_train[predictor_cols].values\n",
    "y_tmp = X_y_train['diagnosis'].values\n",
    "\n",
    "t, p = paired_ttest_skfold_cv(estimator1=classifiers[1], estimator2=classifiers[-1],shuffle=True,cv=5, X=X_tmp, y=y_tmp, scoring='roc_auc', random_seed=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d7844-d8da-427d-a46f-9d0398e2bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('P-value: %.3f, t-Statistic: %.3f' % (p, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b67b3-2a6f-4aea-a777-5981c0968464",
   "metadata": {},
   "source": [
    "Since P value is quite high, and more the basic threshold of 0.05 or 0.1, we fail to reject the null hypothesis, and say that there is no significant difference between these two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9dcf7-9de6-4508-9f83-d97b7aa724f0",
   "metadata": {},
   "source": [
    "## Model Selection: Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06510a3-7aaf-4b45-982f-ce6703c3e602",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <b>Hyperparameter Tuning:</b>\n",
    "    <li> We have done a quick spot checking on algorithms and realized that <code>LogisticRegression</code> is doing well for this task. For this purpose, I will just perform hyperparameter tuning on this single algorithm. However, in practice and if resources are allowed, I will also tune other models such as <code>RandomForest()</code>, or gradient boosting algorithms such as <code>XGBoost</code>, as I believe they will perform no worse than our Logistic Regression model given the right hyperparameters.\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <b>Grid Search:</b>\n",
    "    <li> We will use an old-fashioned way to search for hyperparameters, which is brute force method. The time complexity of Grid Search is high and if you have many hyperparameters to tune, I recommend trying out <b>Random Grid Search</b> or libraries like <b>Optuna</b> that uses Bayesian Optimization.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9666a82-1983-45ba-904f-fd292785265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_finetuning_pipeline(model):\n",
    "    \"\"\"Make a Pipeline for Training.\n",
    "\n",
    "    Args:\n",
    "        model ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    \n",
    "    steps = list()\n",
    "    # standardization\n",
    "    steps.append(('standardize', preprocessing.StandardScaler()))\n",
    "    # reduce VIF\n",
    "    steps.append(('remove_multicollinearity', ReduceVIF(thresh=10)))\n",
    "    # the model\n",
    "    steps.append(('model', model))\n",
    "    # create pipeline\n",
    "    _pipeline = pipeline.Pipeline(steps=steps)\n",
    "    return _pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4a664-a7fb-4d0b-9778-a5380f6a0dd5",
   "metadata": {},
   "source": [
    "Reconstruct our pipeline but now only taking in `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323efdc2-27d4-4d07-a5f1-89e181e08d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_logistic = make_finetuning_pipeline(\n",
    "    linear_model.LogisticRegression(\n",
    "        solver=\"saga\", random_state=config.seed, max_iter=10000, n_jobs=None, fit_intercept=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335144e9-ae4f-44c5-a6e0-dd6eac1a28c6",
   "metadata": {},
   "source": [
    "Define our search space for the hyperparameters:\n",
    "\n",
    "```python\n",
    "param_grid = {model__penalty=[\"l1\", \"l2\"],\n",
    "              model__C=np.logspace(-4, 4, 10)}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd695e89-a26e-4014-898a-c918f53eb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    model__penalty=[\"l1\", \"l2\"],\n",
    "    model__C=np.logspace(-4, 4, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de67f0-d18c-4962-b040-12abfd64ed22",
   "metadata": {},
   "source": [
    "Run our hyperparameter search with cross-validation. For example, our `param_grid` has $2 \\times 10 = 20$ combinations, and our cross validation has 5 folds, then there will be a total of 100 fits.\n",
    "\n",
    "---\n",
    "\n",
    "Below details the pseudo code of what happens under the hood:\n",
    "\n",
    "- Define $G$ as the set of combination of hyperparamters. Define number of splits to be $K$.\n",
    "- For each set of hyperparameter $z \\in Z$:\n",
    "    - for fold $j$ in K:\n",
    "        - Set $F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}$\n",
    "        - Set $F_{\\text{val}} = F_{j}$ as the validation set\n",
    "        - Perform Standard Scaling on $F_{\\text{train}}$ and find the mean and std\n",
    "        - Perform VIF recursively on $F_{\\text{train}}$ and find the selected features\n",
    "        - Transform $F_{\\text{val}}$ using the mean and std found using $F_{\\text{train}}$\n",
    "        - Transform $F_{\\text{val}}$ to have only the selected features from $F_{\\text{train}}$\n",
    "        - Train and fit on $F_{\\text{train}}$ \n",
    "    - Evaluate the fitted parameters on $F_{\\text{val}}$ to obtain $\\mathcal{M}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7edc0e-3830-45c9-8a79-eaf26290d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = model_selection.GridSearchCV(pipeline_logistic, param_grid=param_grid, cv=5, refit=True, verbose=3, scoring = \"roc_auc\")\n",
    "_ = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21239e-1404-44c8-9812-3e30d433df56",
   "metadata": {},
   "source": [
    "We can save our results in a dataframe, we will also look at the top performing hyperparameter by querying the below:\n",
    "\n",
    "```python\n",
    "grid_cv_df = pd.DataFrame(grid.cv_results_)\n",
    "grid_cv_df.loc[grid_cv_df['rank_test_score']==1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7a9ed-5d15-45c8-969c-1bdd33b125b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv_df = pd.DataFrame(grid.cv_results_)\n",
    "best_cv = grid_cv_df.loc[grid_cv_df['rank_test_score']==1]\n",
    "display(best_cv)\n",
    "\n",
    "best_hyperparams = grid.best_params_\n",
    "print(f\"Best Hyperparameters found is {best_hyperparams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde751c-61f0-4c77-a510-e14a3f2f8d57",
   "metadata": {},
   "source": [
    "Our best performing set of hyperparameters `{'model__C': 0.3593813663804626, 'model__penalty': 'l2'}` gives rise to a mean cross validation score of $0.988739$, which is higher than the model with default hyperparameter scoring, $0.987136$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e7164-5449-426b-a3fb-8699255183cb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"warning\">\n",
    "    <b>Room for Improvement:</b> Apart from the other methods to search for the optimal hyperparameters, we can also include preprocessing step as a tunable hyperparameter. More specifically, in our <code>ReduceVIF()</code> step, we hard coded two manual criterion in which the algorithm will stop; if the threshold reaches 10, or if the number of features removed hit 20; we can include them in the search space.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a626cc-c72e-4666-8fd8-5120f3554e6d",
   "metadata": {},
   "source": [
    "## Retrain on the whole training set\n",
    "\n",
    "A common practice after the hyperparameter tuning phase is to retrain the model on the whole dataset $X_{\\text{train}}$ where we will get the estimator's coefficients obtained from the retraining. This is actually already done as the scikit-learn's `GridSearchCV` has a parameter `refit`; if we select it to be true, then after the model selection process is done (i.e. getting the best hyperparameters after cross validation with grid search), the grid search object will retrain on the whole $X_{\\text{train}}$ with the best hyperparameters internally, and return us back an object in which we can call `predict` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5486fa-c992-4e24-b9e0-e62f25c718c6",
   "metadata": {},
   "source": [
    "### Retrain using optimal hyperparameters\n",
    "\n",
    "However, to be extra careful, we can retrain manually using the best hyperparameters and check if scikit-learn is true to its documentation. We will just reconstruct the pipeline using the grid's best hyper parameters. We will then test if the retrained model's coefficients coincide with the grid's best estimator's coefficients. If there difference is 0, this means they are trained under the same circumstances and we can be sure that the refit parameter is behaving true to its words.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "grid_best_hyperparams = grid.best_params_\n",
    "print(grid_best_hyperparams) ->\n",
    "{'model__C': 0.3593813663804626,\n",
    " 'model__penalty': 'l2'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df6f72-e15f-4f28-83e2-e1ad08dbf1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_pipeline = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"standardize\", preprocessing.StandardScaler()),\n",
    "        ('remove_multicollinearity', ReduceVIF(thresh=10)),\n",
    "\n",
    "        (\n",
    "            \"model\",\n",
    "            linear_model.LogisticRegression(\n",
    "                C=0.3593813663804626, max_iter=10000, random_state=1992, solver=\"saga\", penalty=\"l1\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "_ = retrain_pipeline.fit(X_train, y_train)\n",
    "coef_diff = retrain_pipeline['model'].coef_ - grid.best_estimator_['model'].coef_\n",
    "\n",
    "print(\"...\")\n",
    "assert np.all(coef_diff == 0) == True\n",
    "print(\"Retraining Assertion Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd0048-1980-4447-8811-5af9f31a1204",
   "metadata": {},
   "source": [
    "## Interpretation of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb3a31-bf1f-4205-b549-fa2d284563ac",
   "metadata": {},
   "source": [
    "### Interpretation of Coefficients\n",
    "\n",
    "As shown in the figure below, all else being equal, for every square unit increase in mean cell area, the odds of the tumor being malignant increases by a factor of $e^{1.43} = 4.19$. The variation (standard error) of the characteristics of cells also are deemed important by the model, for example, area se played an important role in determining whether a cell is malignant; intuitively, if some cells are noticably larger than the rest, then it is also a good indicator of malignancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b9f75-dd7e-46e6-b5d5-94ddb898981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_by_vif_index = grid.best_estimator_['remove_multicollinearity'].column_indices_kept_ \n",
    "selected_feature_names = np.asarray(predictor_cols)[selected_features_by_vif_index]\n",
    "\n",
    "selected_features_coefficients = grid.best_estimator_['model'].coef_.flatten()\n",
    "\n",
    "# assertion\n",
    "#assert grid.best_estimator_['remove_multicollinearity'].feature_names_ == retrain_pipeline['remove_multicollinearity'].feature_names_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "# .abs()\n",
    "_ = pd.Series(selected_features_coefficients, index=selected_feature_names).sort_values().plot(ax=ax, kind='barh')\n",
    "fig.savefig(config.feature_importance, format=\"png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580baa4c-7151-4fa1-910f-5c9b65acb7b5",
   "metadata": {},
   "source": [
    "### Interpretation of Metric Scores on Train Set\n",
    "\n",
    "We are also interested in choosing an optimal threshold for the model such that it gives the lowest recall, or False Negatives. We note that the default threshold when calling `predict()` from a model is $0.5$. In this section, we will explore one way to get the best tradeoff we can when choosing a high recall, while maintaining a reasonable score for precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b2423-b16f-4b38-b2db-1e51073a8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_train_test_set(\n",
    "    estimator: Callable, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.DataFrame, np.ndarray]\n",
    ") -> Dict[str, Union[float, np.ndarray]]:\n",
    "    \"\"\"This function takes in X and y and returns a dictionary of scores.\n",
    "\n",
    "    Args:\n",
    "        estimator (Callable): [description]\n",
    "        X (Union[pd.DataFrame, np.ndarray]): [description]\n",
    "        y (Union[pd.DataFrame, np.ndarray]): [description]\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Union[float, np.ndarray]]: [description]\n",
    "    \"\"\"\n",
    "\n",
    "    test_results = {}\n",
    "\n",
    "    y_pred = estimator.predict(X)\n",
    "    # This is the probability array of class 1 (malignant)\n",
    "    y_prob = estimator.predict_proba(X)[:, 1]\n",
    "\n",
    "    test_brier = metrics.brier_score_loss(y, y_prob)\n",
    "    test_roc = metrics.roc_auc_score(y, y_prob)\n",
    "\n",
    "    test_results[\"brier\"] = test_brier\n",
    "    test_results[\"roc\"] = test_roc\n",
    "    test_results[\"y\"] = np.asarray(y).flatten()\n",
    "    test_results[\"y_pred\"] = y_pred.flatten()\n",
    "    test_results[\"y_prob\"] = y_prob.flatten()\n",
    "\n",
    "    return test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242c5b1-2769-41b4-9941-bc612ff86c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    \"\"\"\n",
    "    Modified from:\n",
    "    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.89 \n",
    "    and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(config.precision_recall_threshold_plot, format=\"png\", dpi=300)\n",
    "    \n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    \"\"\"\n",
    "    The ROC curve, modified from \n",
    "    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91\n",
    "    and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('ROC Curve')\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.005, 1, 0, 1.005])\n",
    "    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(config.roc_plot, format=\"png\", dpi=300)\n",
    "    \n",
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    Will only work for binary classification problems.\n",
    "    and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc1622-d310-4701-92eb-3bb42c659934",
   "metadata": {},
   "source": [
    "The plots below show the tradeoffs between precision and recall, recall and false positive rate. The confusion matrix on the train set tells us that there is still more false negatives than false positives. We can choose a particular threshold in order to minimize false negatives, at some expense of false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c446c2-3c93-430b-b581-77effe6b94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = evaluate_train_test_set(grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68efcf73-11bb-4cad-9266-7eb3065f8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# CM\n",
    "cm_train = metrics.confusion_matrix(train_results['y'], train_results['y_pred'])\n",
    "\n",
    "#### scores\n",
    "auc = metrics.roc_auc_score(train_results['y'], train_results['y_prob'])\n",
    "\n",
    "#### annotations\n",
    "labels = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"]\n",
    "counts = [\"{0:0.0f}\".format(value) for value in cm_train.flatten()]\n",
    "percentages = [\"{0:.2%}\".format(value) for value in cm_train.flatten() / np.sum(cm_train)]\n",
    "\n",
    "#### final annotations\n",
    "label = (\n",
    "    np.array([f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(labels, counts, percentages)])\n",
    ").reshape(2, 2)\n",
    "\n",
    "# heatmap\n",
    "sns.heatmap(\n",
    "    data=cm_train,\n",
    "    vmin=0,\n",
    "    vmax=330,\n",
    "    cmap=[\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"],\n",
    "    linewidth=2,\n",
    "    linecolor=\"white\",\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    annot=label,\n",
    "    fmt=\"\",\n",
    "    cbar=False,\n",
    "    annot_kws={\"size\": 10, \"color\": \"black\", \"weight\": \"bold\", \"alpha\": 0.8},\n",
    "    alpha=1,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ax.scatter(1, 1, s=3500, c=\"white\")\n",
    "ax.text(\n",
    "    0.72,\n",
    "    1.0,\n",
    "    \"AUC: {}\".format(round(auc, 3)),\n",
    "    {\"size\": 10, \"color\": \"black\", \"weight\": \"bold\"},\n",
    ")\n",
    "\n",
    "## ticks and labels\n",
    "ax.set_xticklabels(\"\")\n",
    "ax.set_yticklabels(\"\")\n",
    "\n",
    "\n",
    "## titles and text\n",
    "fig.text(0, 1.05, \"Train Set Confusion Matrix\", {\"size\": 22, \"weight\": \"bold\"}, alpha=1)\n",
    "fig.text(\n",
    "    0,\n",
    "    1,\n",
    "    \"\"\"Training Set Confusion Matrix.\"\"\",\n",
    "    {\"size\": 12, \"weight\": \"normal\"},\n",
    "    alpha=0.98,\n",
    ")\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=2.5, w_pad=2.5, h_pad=2.5)\n",
    "fig.savefig(config.final_train_confusion_matrix, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575c915-5e4c-4297-a6de-e28d28feb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the precision recall curve\n",
    "precision, recall, pr_thresholds = metrics.precision_recall_curve(train_results['y'], train_results['y_prob'])\n",
    "fpr, tpr, roc_thresholds = metrics.roc_curve(train_results['y'], train_results['y_prob'], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa77187-c91d-4ddf-a29f-c956c6b456d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same p, r, thresholds that were previously calculated\n",
    "plot_precision_recall_vs_threshold(precision, recall, pr_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a42e3-b58f-4f22-9dde-9792fcc4d366",
   "metadata": {},
   "source": [
    "Based on the tradeoff plot above, a good threshold can be set at $t = 0.35$, let us see how it performs with this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c2a4c6-7462-4185-ba8c-e1eefe95dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_adj = adjusted_classes(train_results[\"y_prob\"], t=0.35)\n",
    "\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        metrics.confusion_matrix(train_results[\"y\"], y_pred_adj),\n",
    "        columns=[\"pred_neg\", \"pred_pos\"],\n",
    "        index=[\"neg\", \"pos\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893fe9c6-b9dc-4b32-b9f5-c99f3f0e5d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_true=train_results[\"y\"], y_pred=y_pred_adj))\n",
    "train_brier = train_results['brier']\n",
    "print(f\"train brier: {train_brier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24a144-e729-4078-af7a-8f96adab6037",
   "metadata": {},
   "source": [
    "The False Negatives reduced from 15 to 9, at the expense of increase False Positives from 6 to 14. We should take comfort that less patients are falsely classified as benign when in fact they need treatment. This is a tradeoff that we have to choose. The ROC curve below also paints a similar story, in order for you to have high recall, one must accept that there will more False Positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b4449-b47e-41cd-a0ce-90f5a0b67054",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr, tpr, 'recall_optimized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d7767-f4fe-4f8d-8b35-fe69014a2be1",
   "metadata": {},
   "source": [
    "# Evaluation on Test Set\n",
    "\n",
    "Ultimately, we are interested in finding the estimate of the generalization error of the model, and thus it is time for us to evaluate our model's performance on the \"unseen\" test set $X_{\\text{test}}$ to get a good gauge on how well the model generalizes to unseen data. Take note that now everything has settled, we will use the exact hyperparameters to predict on test set, with the pre-defined threshold of 0.35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f62ee-7c07-4f10-9254-88b52ac312e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluate_train_test_set(grid, X_test, y_test)\n",
    "y_test_pred_adj = adjusted_classes(test_results['y_prob'], t=0.35)\n",
    "\n",
    "print(pd.DataFrame(metrics.confusion_matrix(test_results['y'], y_test_pred_adj),\n",
    "                   columns=['pred_neg', 'pred_pos'], \n",
    "                   index=['neg', 'pos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8e478-a2c2-4fd5-9242-da0cf209a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_roc = test_results['roc']\n",
    "test_brier = test_results['brier']\n",
    "print(test_roc)\n",
    "print(test_brier)\n",
    "print(metrics.classification_report(y_true=test_results[\"y\"], y_pred=y_test_pred_adj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5107c2-087f-481f-b157-b3376161b0c9",
   "metadata": {},
   "source": [
    "Using the same threshold we used on training set, we see that the False Negative is quite low. The overall ROC score is 0.9828, and the corresponding Brier Score is 0.04136, both seem reasonably well performing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814172be-8e08-4b9a-9f4a-03f1c388d9af",
   "metadata": {},
   "source": [
    "# Benefit Structure\n",
    "\n",
    "Refer to health insurance project!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd057235-501a-4ed4-be3a-f1664ef3ead9",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e82274-a0a8-49dd-a6fb-70bd013fefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        grid.best_estimator_['model'], X_train.values, y_train.values, X_test.values, y_test.values, \n",
    "        loss='0-1_loss',\n",
    "        random_seed=123)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1ca9b-6883-4c7a-8c83-b967c71607e5",
   "metadata": {},
   "source": [
    "We use the `mlxtend` library to estimate the Bias-Variance Tradeoff in our Logistic Regression model. The core idea behind this function is to use bagging and repeatedly sample from our training set so as to simulate that we are actually drawing samples from the \"true\" population over a distribution $\\mathcal{P}$. \n",
    "\n",
    "---\n",
    "\n",
    "As expected, Logistic Regression being a linear model, its simplicity contributes to its high bias and low variance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
