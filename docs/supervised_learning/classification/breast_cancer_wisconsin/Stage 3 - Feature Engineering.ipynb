{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe19181f-d930-43fb-a298-794bb2261b99",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "<a id = '1.0'></a>\n",
    "<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal;background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" >Quick Navigation</h1>\n",
    "\n",
    "    \n",
    "* [Dependencies and Configuration](#1)\n",
    "* [Stage 3: Feature Engineering/Feature Selection](#2)\n",
    "    * [Multicollinearity and Feature Selection](#31)\n",
    "        * [Target Distribution](#31)\n",
    "        * [Using Statsmodels Variance Inflation Factor](#31)\n",
    "        * [Oh Dear, we have a Multicollinearity Problem](#31)\n",
    "    * [Save the Data](#31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b74a6-768a-4074-9fd1-b9a6808a97b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dependencies and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4e21348-94cc-444a-9026-436b9550ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import (base, decomposition, linear_model, manifold, metrics,\n",
    "                     preprocessing)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "016e67c9-594a-44c0-99d9-6829672c9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class global_config:\n",
    "    \n",
    "    # File Path\n",
    "    raw_data = \"../data/raw/data.csv\"\n",
    "    processed_data_stage_1 = \"../data/processed/data_stage_1.csv\"\n",
    "    processed_data_stage_2 = \"../data/processed/data_stage_2.csv\"\n",
    "    processed_data_stage_3 = \"../data/processed/data_stage_3.csv\"\n",
    "\n",
    "    # Data Information\n",
    "    target = [\"diagnosis\"]\n",
    "    unwanted_cols = [\"id\", \"Unnamed: 32\"]\n",
    "\n",
    "    # Plotting\n",
    "    colors = [\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"]\n",
    "    cmap_reversed = plt.cm.get_cmap('mako_r')\n",
    "    \n",
    "    # Seed Number\n",
    "    seed = 1992\n",
    "\n",
    "    # Cross Validation\n",
    "    num_folds = 5\n",
    "    cv_schema = \"StratifiedKFold\"\n",
    "    split_size = {\"train_size\": 0.9, \"test_size\": 0.1}\n",
    "\n",
    "\n",
    "def set_seeds(seed: int = 1234) -> None:\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ecaa7fe-c355-4380-a603-356d1ba12a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = global_config\n",
    "\n",
    "# set seeding for reproducibility\n",
    "_ = set_seeds(seed = config.seed)\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(config.processed_data_stage_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9dc733-229b-4909-babc-006eab62c1d6",
   "metadata": {},
   "source": [
    "# Stage 3: Feature Engineering/Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc33543-0b55-49a0-932c-9e72f39e8a03",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Disclaimer!</b> We are fully aware that oftentimes practitioner may accidentally cause data leakage during preprocessing, for example, a subtle yet often mistake is to standardize the whole dataset prior to splitting, or performing feature selection prior to modelling using the information of our response/target variable. However, we can still screen predictors for multicollinearity during EDA phase and have a good intuition on which predictors are highly correlated - subsequently, we will incorporate a feature selection technique in our modelling pipeline. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a6c4f-c54a-4223-806b-aac3be4ebb50",
   "metadata": {},
   "source": [
    "## Multicollinearity and Feature Selection\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <b>Motivation:</b> We need feature selection in certain problems for the following reasons:\n",
    "    <li> Well, one would definitely have heard of the dreaded curse of dimensionality in the journey of learning Machine Learning where having too many predictor/features can lead to overfitting; on the other hand, too many dimensions can cause distance between observations to appear equidistance from one another. observations become harder to cluster â€” believe it or not, too many dimensions causes every observation in your dataset to appear equidistant from all the others, thereby clogging the model's ability to cluster data points (imagine the horror if you use KNN on 1000 dimensions, all the points will be almost the same distance from each other, poor KNN).\n",
    "    <li> In case you have access to Google's GPU clusters, you likely want to train your model faster. Reducing the number predictors can aid this process.\n",
    "    <li> Reducing uninformative features may aid in model's performance, the idea is to remove unnecessary noise from the dataset.\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Multi-Collinearity:</b> Looking back at our dataset, it is clear to me that there are quite a number of features that are correlated with each other, causing multi-collinearity. Multi-Collinearity is an issue in the history of Linear Models, quoting the statement from <a href=\"https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r\"> Is there an intuitive explanation why multicollinearity is a problem in linear regression?</a>\n",
    "    \n",
    "> Consider the simplest case where Y is regressed against X and Z and where X and Z are highly positively correlated. Then the effect of X on Y is hard to distinguish from the effect of Z on Y because any increase in X tends to be associated with an increase in Z.\n",
    "\n",
    "We also note that multi-collinearity is not that big of a problem for non-parametric models such as Decision Tree or Random Forests, however, I will attempt to show that it is still best to avoid in this problem setting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b4ba6-3922-4caf-96ff-31dc50c0b94f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <b> Alert! Alert! Alert! </b> There are many methods to perform feature selection. Scikit-Learn offers some of the following:\n",
    "    <li> Univariate feature selection.\n",
    "    <li> Recursive feature elimination.\n",
    "    <li> Backward Elimination of features using Hypothesis Testing.  \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "    <b> EMERGENCY! </b> We need to be careful when selecting features before cross-validation. It is therefore, recommended to include feature selection in cross-validation to avoid any \"bias\" introduced before model selection phase! I decided to use the good old Variance Inflation Factor (VIF) as a way to reduce multicollinearity. Unfortunately, there is no out-of-the-box function to integrate into the <code>Pipeline</code> of scikit-learn. Thus, I heavily modified an existing code in order achieve what I want below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e26d4-79d6-4354-b21c-b268e21b2a81",
   "metadata": {},
   "source": [
    "A classical way to check for multicollinearity amongst predictors is to calculate the Variable Inflation Factor (VIF). It is simply done by regressing each predictor $\\mathrm{x}_i$ against all other predictors $\\mathrm{x}_j, j \\neq i$. In other words, the VIF for a predictor variable $i$ is given by:\n",
    "\n",
    "$$\\text{VIF}_i = \\dfrac{1}{1 - R^{2}_{i}}$$\n",
    "\n",
    "where $R^{2}_{i}$ is, by definition, the proportion of the variation in the \"dependent variable\" $\\mathrm{x}_i$ that is predictable from the indepedent predictors $\\mathrm{x}_j, j \\neq i$. Consequently, the higher the $R^2_i$ of a predictor, the higher the VIF, and this indicates there is linear dependence among predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7fea1-5183-4f07-816f-77545f88386e",
   "metadata": {},
   "source": [
    "### Using Statsmodels Variance Inflation Factor\n",
    "\n",
    "Note that we need to perform scaling first before fitting our `ReduceVIF` to get the exact same result as the previous version. In this version, I manually added a hard threshold for the number of features remaining to be 15. This hard coded number can be turned into a parameter (hyperparameter) in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e7a9206-e167-4c54-b89e-29042adc540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceVIF(base.BaseEstimator, base.TransformerMixin):\n",
    "    \"\"\"The base of the class structure is not implemented by me, however, I heavily modified the class such that it can\n",
    "    take in numpy arrays and correctly implemented the fit and transform method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, thresh=10):\n",
    "        self.thresh = thresh\n",
    "        self.feature_names_ = None\n",
    "        self.predictor_cols = [\n",
    "            \"radius_mean\",\n",
    "            \"texture_mean\",\n",
    "            \"perimeter_mean\",\n",
    "            \"area_mean\",\n",
    "            \"smoothness_mean\",\n",
    "            \"compactness_mean\",\n",
    "            \"concavity_mean\",\n",
    "            \"concave points_mean\",\n",
    "            \"symmetry_mean\",\n",
    "            \"fractal_dimension_mean\",\n",
    "            \"radius_se\",\n",
    "            \"texture_se\",\n",
    "            \"perimeter_se\",\n",
    "            \"area_se\",\n",
    "            \"smoothness_se\",\n",
    "            \"compactness_se\",\n",
    "            \"concavity_se\",\n",
    "            \"concave points_se\",\n",
    "            \"symmetry_se\",\n",
    "            \"fractal_dimension_se\",\n",
    "            \"radius_worst\",\n",
    "            \"texture_worst\",\n",
    "            \"perimeter_worst\",\n",
    "            \"area_worst\",\n",
    "            \"smoothness_worst\",\n",
    "            \"compactness_worst\",\n",
    "            \"concavity_worst\",\n",
    "            \"concave points_worst\",\n",
    "            \"symmetry_worst\",\n",
    "            \"fractal_dimension_worst\",\n",
    "        ]\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.predictor_cols = [\n",
    "            \"radius_mean\",\n",
    "            \"texture_mean\",\n",
    "            \"perimeter_mean\",\n",
    "            \"area_mean\",\n",
    "            \"smoothness_mean\",\n",
    "            \"compactness_mean\",\n",
    "            \"concavity_mean\",\n",
    "            \"concave points_mean\",\n",
    "            \"symmetry_mean\",\n",
    "            \"fractal_dimension_mean\",\n",
    "            \"radius_se\",\n",
    "            \"texture_se\",\n",
    "            \"perimeter_se\",\n",
    "            \"area_se\",\n",
    "            \"smoothness_se\",\n",
    "            \"compactness_se\",\n",
    "            \"concavity_se\",\n",
    "            \"concave points_se\",\n",
    "            \"symmetry_se\",\n",
    "            \"fractal_dimension_se\",\n",
    "            \"radius_worst\",\n",
    "            \"texture_worst\",\n",
    "            \"perimeter_worst\",\n",
    "            \"area_worst\",\n",
    "            \"smoothness_worst\",\n",
    "            \"compactness_worst\",\n",
    "            \"concavity_worst\",\n",
    "            \"concave points_worst\",\n",
    "            \"symmetry_worst\",\n",
    "            \"fractal_dimension_worst\",\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"ReduceVIF fit\")\n",
    "        tmp, self.predictor_cols = ReduceVIF.calculate_vif(X, self.predictor_cols, self.thresh)\n",
    "        self.feature_names_ = self.predictor_cols  # save as an attribute to call later\n",
    "        # If you notice this code is wrong here, we will return a sequential index no matter what.\n",
    "        col_index = [self.predictor_cols.index(col_name) for col_name in self.predictor_cols]\n",
    "        self.col_index = col_index\n",
    "        self.reset()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print(\"ReduceVIF transform\")\n",
    "        # columns = X.columns.tolist()\n",
    "        # print(X.shape)\n",
    "        print(self.col_index)\n",
    "        return X[:, self.col_index]\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_vif(X: Union[np.ndarray, pd.DataFrame], columns: List[str], thresh: float = 10.0):\n",
    "        \"\"\"Implements a VIF function that recursively eliminates features.\n",
    "\n",
    "        Args:\n",
    "            X (Union[np.ndarray, pd.DataFrame]): [description]\n",
    "            columns (List[str]): [description]\n",
    "            thresh (float, optional): [description]. Defaults to 10.0.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "\n",
    "        dropped = True\n",
    "        count = 0\n",
    "        while dropped and count <= 15:\n",
    "            column_index = X.shape[1]\n",
    "            predictor_cols = np.arange(X.shape[1])\n",
    "            dropped = False\n",
    "            vif = []\n",
    "            for var in range(column_index):\n",
    "                # print(predictor_cols.shape)\n",
    "                vif.append(variance_inflation_factor(X[:, predictor_cols], var))\n",
    "\n",
    "            max_vif = max(vif)\n",
    "            if max_vif > thresh:\n",
    "                maxloc = vif.index(max_vif)\n",
    "                print(f\"Dropping {maxloc} with vif={max_vif}\")\n",
    "                # X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
    "                X = np.delete(X, maxloc, axis=1)\n",
    "                columns.pop(maxloc)\n",
    "                dropped = True\n",
    "                count += 1\n",
    "        return X, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c397adc9-c655-486a-b7c0-373d5594286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReduceVIF fit\n",
      "Dropping 0 with vif=3806.1152963979675\n",
      "Dropping 19 with vif=616.3508614719424\n",
      "Dropping 1 with vif=325.64131198187516\n",
      "Dropping 19 with vif=123.25781086343038\n",
      "Dropping 4 with vif=64.65479584770004\n",
      "Dropping 7 with vif=35.61751844352034\n",
      "Dropping 19 with vif=33.96063880508537\n",
      "Dropping 20 with vif=30.596655364833975\n",
      "Dropping 1 with vif=25.387829695531387\n",
      "Dropping 2 with vif=18.843208489973282\n",
      "Dropping 14 with vif=17.232376192128665\n",
      "Dropping 7 with vif=16.333806476471736\n",
      "Dropping 15 with vif=15.510661467365699\n",
      "ReduceVIF transform\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "predictor_cols = df.columns[1:]\n",
    "transformer = ReduceVIF()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X = scaler.fit_transform(df[predictor_cols])\n",
    "# Only use 10 columns for speed in this example\n",
    "X = transformer.fit_transform(X)\n",
    "\n",
    "vif_df = pd.DataFrame({'Predictors': transformer.feature_names_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18c9ae9d-f5d3-4bc0-a887-5f9a8ebcdfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Features: ['texture_mean', 'smoothness_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'texture_se', 'perimeter_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'area_worst', 'smoothness_worst', 'symmetry_worst', 'fractal_dimension_worst']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>texture_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smoothness_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>concave points_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>symmetry_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fractal_dimension_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>texture_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>perimeter_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>smoothness_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>compactness_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>concavity_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>concave points_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>symmetry_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fractal_dimension_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>area_worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>smoothness_worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>symmetry_worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fractal_dimension_worst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predictors\n",
       "0              texture_mean\n",
       "1           smoothness_mean\n",
       "2       concave points_mean\n",
       "3             symmetry_mean\n",
       "4    fractal_dimension_mean\n",
       "5                texture_se\n",
       "6              perimeter_se\n",
       "7             smoothness_se\n",
       "8            compactness_se\n",
       "9              concavity_se\n",
       "10        concave points_se\n",
       "11              symmetry_se\n",
       "12     fractal_dimension_se\n",
       "13               area_worst\n",
       "14         smoothness_worst\n",
       "15           symmetry_worst\n",
       "16  fractal_dimension_worst"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Remaining Features: {transformer.feature_names_}\")\n",
    "display(vif_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c542ad-4ed5-4d57-aac1-af5d7265cd05",
   "metadata": {},
   "source": [
    "### Improved Version of ReduceVIF class\n",
    "\n",
    "This is an improved version of the previous `ReduceVIF` class. Notice in order to write it \"our way\", we took the `variance_inflation_factor` from the `statsmodels` library and redefine it here to suit our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97d87d34-a4e4-47a9-b797-6a5e20ae2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import base\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "def variance_inflation_factor(exog, idx_kept, vif_idx):\n",
    "    \"\"\"Compute VIF for one feature.\n",
    "    \n",
    "    Args:\n",
    "        exog (np.ndarray): Observations\n",
    "        idx_kept (List[int]): Indices of features to consider\n",
    "        vif_idx (int): Index of feature for which to compute VIF\n",
    "    \n",
    "    Returns:\n",
    "        float: VIF for the selected feature\n",
    "    \"\"\"\n",
    "    exog = np.asarray(exog)\n",
    "    \n",
    "    x_i = exog[:, vif_idx]\n",
    "    mask = [col for col in idx_kept if col != vif_idx]\n",
    "    x_noti = exog[:, mask]\n",
    "    \n",
    "    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n",
    "    vif = 1. / (1. - r_squared_i)\n",
    "    \n",
    "    return vif\n",
    "\n",
    "class ReduceVIF(base.BaseEstimator, base.TransformerMixin):\n",
    "    \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class;\n",
    "    I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, thresh=10, max_drop=20):\n",
    "        self.thresh = thresh\n",
    "        self.max_drop = max_drop\n",
    "        self.column_indices_kept_ = []\n",
    "        self.feature_names_kept_ = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the state of predictor columns after each fold.\"\"\"\n",
    "\n",
    "        self.column_indices_kept_ = []\n",
    "        self.feature_names_kept_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names\n",
    "\n",
    "        Args:\n",
    "            X ([type]): [description]\n",
    "            y ([type], optional): [description]. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.column_indices_kept_, self.feature_names_kept_ = self.calculate_vif(X)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Transforms the Validation Set according to the selected feature names.\n",
    "\n",
    "        Args:\n",
    "            X ([type]): [description]\n",
    "            y ([type], optional): [description]. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "\n",
    "        return X[:, self.column_indices_kept_]\n",
    "\n",
    "    def calculate_vif(self, X: Union[np.ndarray, pd.DataFrame]):\n",
    "        \"\"\"Implements a VIF function that recursively eliminates features.\n",
    "\n",
    "        Args:\n",
    "            X (Union[np.ndarray, pd.DataFrame]): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        feature_names = None\n",
    "        column_indices_kept = list(range(X.shape[1]))\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            feature_names = X.columns\n",
    "\n",
    "        dropped = True\n",
    "        count = 0\n",
    "        \n",
    "        while dropped and count <= self.max_drop:\n",
    "            dropped = False\n",
    "            \n",
    "            max_vif, max_vif_col = None, None\n",
    "            \n",
    "            for col in column_indices_kept:\n",
    "                \n",
    "                vif = variance_inflation_factor(X, column_indices_kept, col)\n",
    "                \n",
    "                if max_vif is None or vif > max_vif:\n",
    "                    max_vif = vif\n",
    "                    max_vif_col = col\n",
    "            \n",
    "            if max_vif > self.thresh:\n",
    "                print(f\"Droppingggggg {max_vif_col} with vif={max_vif}\")\n",
    "                column_indices_kept.remove(max_vif_col)\n",
    "                \n",
    "                if feature_names is not None:\n",
    "                    feature_names.pop(max_vif_col)\n",
    "                    \n",
    "                dropped = True\n",
    "                count += 1\n",
    "                \n",
    "        return column_indices_kept, feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a6e78-c72f-4dfe-a17d-cd9ed0a00193",
   "metadata": {},
   "source": [
    "We do a sanity check if this coincides with the previous defined class, and the results are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7eb8eae-5f4a-445b-aca5-ea34edde4ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droppingggggg 0 with vif=3806.1152963979675\n",
      "Droppingggggg 20 with vif=616.3508614719424\n",
      "Droppingggggg 2 with vif=325.64131198187516\n",
      "Droppingggggg 22 with vif=123.25781086343038\n",
      "Droppingggggg 6 with vif=64.65479584770004\n",
      "Droppingggggg 10 with vif=35.61751844352034\n",
      "Droppingggggg 25 with vif=33.96063880508537\n",
      "Droppingggggg 27 with vif=30.596655364833975\n",
      "Droppingggggg 3 with vif=25.387829695531387\n",
      "Droppingggggg 5 with vif=18.843208489973282\n",
      "Droppingggggg 21 with vif=17.232376192128665\n",
      "Droppingggggg 13 with vif=16.333806476471736\n",
      "Droppingggggg 26 with vif=15.510661467365699\n",
      "Remaining Features: [1, 4, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19, 23, 24, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "predictor_cols = df.columns[1:]\n",
    "transformer = ReduceVIF()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X = scaler.fit_transform(df[predictor_cols])\n",
    "# Only use 10 columns for speed in this example\n",
    "X = transformer.fit_transform(X)\n",
    "\n",
    "print(f\"Remaining Features: {transformer.column_indices_kept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568996cb-733f-4391-b7be-dd25b01acafd",
   "metadata": {},
   "source": [
    "We have the remaining indices, and therefore simply use numpy to subset the column indices to get back the original column names that are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2634f944-4d36-4a27-b621-999b2787eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df = pd.DataFrame({'Predictors': predictor_cols[transformer.column_indices_kept_]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72adeed8-b69f-4371-b736-422f74a8790b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>texture_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smoothness_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>concave points_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>symmetry_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fractal_dimension_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>texture_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>perimeter_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>smoothness_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>compactness_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>concavity_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>concave points_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>symmetry_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fractal_dimension_se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>area_worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>smoothness_worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>symmetry_worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fractal_dimension_worst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predictors\n",
       "0              texture_mean\n",
       "1           smoothness_mean\n",
       "2       concave points_mean\n",
       "3             symmetry_mean\n",
       "4    fractal_dimension_mean\n",
       "5                texture_se\n",
       "6              perimeter_se\n",
       "7             smoothness_se\n",
       "8            compactness_se\n",
       "9              concavity_se\n",
       "10        concave points_se\n",
       "11              symmetry_se\n",
       "12     fractal_dimension_se\n",
       "13               area_worst\n",
       "14         smoothness_worst\n",
       "15           symmetry_worst\n",
       "16  fractal_dimension_worst"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(vif_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44d6e3-bd6a-4a55-8780-b10dab085e1f",
   "metadata": {},
   "source": [
    "### Oh Dear, we have a Multicollinearity Problem\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Using VIF in Modelling Pipeline:</b> At this step, we are just showing how we can remove multicollinear features using VIF; but we will not remove them at this point in time. We will incorporate this feature selection technique in our Cross-Validation pipeline in order to avoid data leakage.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a45b17-5801-4a53-baaf-e4d10a346e8d",
   "metadata": {},
   "source": [
    "# Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3509e9b9-d143-468a-bd2a-55666481245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(config.processed_data_stage_3, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
