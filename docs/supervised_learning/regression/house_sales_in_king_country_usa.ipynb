{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "145df933-1c09-483d-b26a-239a29af9be1",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "<a id = '1.0'></a>\n",
    "<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal;background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" >Quick Navigation</h1>\n",
    "\n",
    "    \n",
    "* [Dependencies and Configuration](#1)\n",
    "* [Stage 4: Modelling](#2)\n",
    "    * [How EDA helped us?](#31)\n",
    "    * [Modelling](#31)\n",
    "        * [Spot Checking Algorithms](#31)\n",
    "            * [Make Basic Pipeline (Say No to Data Leakage!)](#31)\n",
    "            * [Define Metrics](#31)\n",
    "            * [Comparison of Cross-Validated Models](#31)\n",
    "            * [Out-of-Fold Confusion Matrix](#31)\n",
    "            * [Hypothesis Testing Across Models](#31)\n",
    "        * [Model Selection: Hyperparameter Tuning with GridSearchCV](#31)\n",
    "        * [Retrain on the whole training set](#31)\n",
    "            * [Retrain using Optimal Hyperparameters](#31)\n",
    "        * [Interpretation of Results](#31)\n",
    "            * [Interpretation of Coefficients](#31)\n",
    "            * [Interpretation of Metric Scores on Train Set](#31)\n",
    "    * [Evaluation on Test Set](#31)\n",
    "    * [Bias-Variance Tradeoff](#31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2fd28b-c218-495e-9b60-0c37d5060ba2",
   "metadata": {},
   "source": [
    "# Dependencies and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d078e09b-5833-4510-b8a2-644087d79510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from functools import wraps\n",
    "from time import time\n",
    "from typing import Callable, Dict, List, Union, Optional, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import mlxtend\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv, bias_variance_decomp\n",
    "from scipy import stats\n",
    "from sklearn import (base, decomposition, dummy, ensemble, feature_selection,\n",
    "                     linear_model, metrics, model_selection, neighbors,\n",
    "                     pipeline, preprocessing, svm, tree)\n",
    "\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "#from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9358921c-dcfa-4b81-8359-d2f1aa3ec8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class config:\n",
    "    raw_data: str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/regression/%20house-sales-in-king-country-usa/data/raw/kc_house_data.csv\"\n",
    "    train_size: float = 0.8\n",
    "    seed: int = 1992\n",
    "    num_folds: int = 5\n",
    "    cv_schema: str = \"KFold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f2d69f-a76a-47aa-a3a8-e26c5b96591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed: int = 1234) -> None:\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "def init_logger(log_file: str = \"info.log\"):\n",
    "    \"\"\"\n",
    "    Initialize logger.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\", datefmt= \"%Y-%m-%d,%H:%M:%S\"))\n",
    "    file_handler = logging.FileHandler(filename=log_file)\n",
    "    file_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\",  datefmt= \"%Y-%m-%d,%H:%M:%S\"))\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d10cc4-01dd-4c2e-b419-3389c9dcd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeding for reproducibility\n",
    "_ = set_seeds(seed = config.seed)\n",
    "\n",
    "# set logger\n",
    "logger = init_logger()\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(config.raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b36a2a-d514-408c-bc37-5029a896eb10",
   "metadata": {},
   "source": [
    "# How EDA helped us?\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <b>Insights derived from EDA:</b> \n",
    "    <li> To fill in.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0f8f0-ce09-4c85-b4dc-f11315ec84f4",
   "metadata": {},
   "source": [
    "# Cross-Validation Strategy\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Generalization:</b>     \n",
    "    <blockquote cite=\"https://www.huxley.net/bnw/four.html\">\n",
    "        <p>Ultimately, we are interested in the Generalization Error made by the model, that is, how well the model perform on <b>unseen data</b> that is not taken from our sample set $\\mathcal{D}$. In general, we use <b>validation set</b> for <b>Model Selection</b> and the <b>test set</b> for <b>an estimate of generalization error</b> on new data.\n",
    "            <br> <b>- Refactored from Elements of Statistical Learning, Chapter 7.2</b></p>\n",
    "    </blockquote>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <b>Step 1: Train-Test-Split:</b> Since this dataset is relatively small, we will not use the <b>train-validation-test</b> split and only split into train and test in a ratio of 9:1, whereby the split is stratified on our target, using <code>stratify=y</code> parameter in <code>train_test_split()</code> to ensure that our target has equal representation in both train and test. We note that this is a relatively small dataset and in practice, we need a large sample size to get a reliable/stable split, it is also recommended to retrain the whole dataset (without the \"unseen\" test set) after we have done the model selection process (eg. finding best hyperparameters). \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <b>Step 2: Resampling Stategy:</b> Note that we will be performing <code>StratifiedKFold</code> as our resampling strategy. After our split in Step 1, we have a training set $X_{\\text{train}}$, we will then perform our resampling strategy on this $X_{\\text{train}}$. We will choose our choice of $K = 5$. The choice of $K$ is somewhat arbitrary, and is derived <a href=\"https://stats.stackexchange.com/questions/61783/bias-and-variance-in-leave-one-out-vs-k-fold-cross-validation\">empirically</a>. \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "To recap, we have the following:\n",
    "\n",
    "- **Training Set ($X_{\\text{train}}$)**: This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis $h \\in \\mathcal{H}$.\n",
    "- **Validation Set ($X_{\\text{val}}$)**: This is split from our $X_{\\text{train}}$ during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis $g \\in \\mathcal{H}$).\n",
    "- **Test Set ($X_{\\text{test}}$)**: This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model $g$, we will use $g$ to predict on the test set to get an estimate of the generalization error (also called out-of-sample error).\n",
    "\n",
    "---\n",
    "\n",
    "<figure>\n",
    "<img src='https://scikit-learn.org/stable/_images/grid_search_workflow.png' width=\"500\"/>\n",
    "<figcaption align = \"center\"><b>Courtesy of scikit-learn on a typical Cross-Validation workflow.</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b06385-2201-43e6-8dd8-2088280db7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10,12:09:44 - \n",
      "The predictor columns are \n",
      "['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']\n"
     ]
    }
   ],
   "source": [
    "predictor_cols = df.columns[3:].tolist()\n",
    "target_col = [\"price\"]\n",
    "logger.info(f\"\\nThe predictor columns are \\n{predictor_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e764449-0505-4c79-85f0-60d71d4bf6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[predictor_cols].copy()\n",
    "y = df[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d165c0-5668-4127-a186-93f2c2676ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10,12:09:44 - \n",
      "Shape of train: (17290, 18)\n",
      "Shape of test: (4323, 18)\n"
     ]
    }
   ],
   "source": [
    "# Split train - test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, train_size=config.train_size, shuffle=True, random_state=config.seed\n",
    ")\n",
    "\n",
    "logger.info(f\"\\nShape of train: {X_train.shape}\\nShape of test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a62783b-1def-4567-8f20-d6a81f987f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folds(\n",
    "    df: pd.DataFrame,\n",
    "    num_folds: int,\n",
    "    cv_schema: str,\n",
    "    seed: int,\n",
    "    predictor_col: List,\n",
    "    target_col: List,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Split the given dataframe into training folds.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): [description]\n",
    "        num_folds (int): [description]\n",
    "        cv_schema (str): [description]\n",
    "        seed (int): [description]\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: [description]\n",
    "    \"\"\"\n",
    "\n",
    "    if cv_schema == \"KFold\":\n",
    "        df_folds = df.copy()\n",
    "        kf = model_selection.KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(\n",
    "            kf.split(X=df_folds[predictor_col], y=df_folds[target_col])\n",
    "        ):\n",
    "            df_folds.loc[val_idx, \"fold\"] = int(fold + 1)\n",
    "\n",
    "        df_folds[\"fold\"] = df_folds[\"fold\"].astype(int)\n",
    "\n",
    "    elif cv_schema == \"StratifiedKFold\":\n",
    "        df_folds = df.copy()\n",
    "        skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(\n",
    "            skf.split(X=df_folds[predictor_col], y=df_folds[target_col])\n",
    "        ):\n",
    "            df_folds.loc[val_idx, \"fold\"] = int(fold + 1)\n",
    "\n",
    "        df_folds[\"fold\"] = df_folds[\"fold\"].astype(int)\n",
    "        print(df_folds.groupby([\"fold\", \"diagnosis\"]).size())\n",
    "\n",
    "    return df_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "035e7e10-5ce3-4209-8e61-664c72cfdc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_train = pd.concat([X_train, y_train], axis = 1).reset_index(drop=True)\n",
    "df_folds = make_folds(X_y_train, num_folds=config.num_folds, cv_schema=config.cv_schema, seed=config.seed, predictor_col= predictor_cols, target_col = target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cefba9-401a-4603-b174-cd5e29188b38",
   "metadata": {},
   "source": [
    "Looks good! All our five folds are now in `df_fold`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ae5dd-b5c5-427c-8cb9-0f660c771d1e",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabfea4-8d8f-4843-a3d7-8cda24406906",
   "metadata": {},
   "source": [
    "## Spot Checking Algorithms\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <b>Terminology Alert!</b> This method is advocated by <a href=\"https://machinelearningmastery.com/\">Jason Brownlee PhD</a> and this serves as the first stage of my modelling process. We will rapidly test (spot check) different classifier algorithms, from <code>DummyClassifier</code>, to <code>LinearModel</code> to more sophisticated ensemble trees like <code>RandomForest</code>. \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "I also note to the readers that we need to think of a few things when choosing the \"optimal\" machine learning algorithm:\n",
    "\n",
    "- [No Lunch Free Theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) intuitively says that no single optimization algorithm can work best in all situations. Therefore, spot checking can help us form a basis of which algorithm might work better in this particular scenario.\n",
    "- [Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor) often appears in many Machine Learning textbook, and the narrative is that a simpler model more often times generalizes better than a complex model. This is not unfamiliar when we think of the bias-variance tradeoff, and that is why there is always a tradeoff that we must make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6c675a-d924-4aa8-9096-efe42854e514",
   "metadata": {},
   "source": [
    "### Make Basic Pipeline (Say No to Data Leakage!)\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Say No to Data Leakage:</b> This has been emphasized throughout and we must be careful as we should never touch the test set when fitting the model.\n",
    "    <li> This means that preprocessing steps such as <code>StandardScaling()</code> should only be fitted on the training data, and then apply the same transformation (mean and std) on the test data. In other words, do not apply scaling on the whole dataset before splitting. \n",
    "    <li> However, it is also equally important to take note <b>not to contaminate</b> our validation set, which is often overlooked, resulting in over optimistic results from model selection phase, but perform badly on unseen test set. As a result, when we use a 5 fold cross validation, we should be careful during fitting that the preprocessing steps are only applied on the training folds, and not on all 5 folds.\n",
    "    <li> The same idea is also applied to our <code>ReduceVIF()</code> preprocessing step. We should also include this in our pipeline and not select the features outside the cross-validation loop.</li>\n",
    "</div>   \n",
    "    \n",
    "Scikit Learn's `Pipeline` object will prevent us from data leakage, as the steps in a pipeline is already pre-defined. There is also a lot of flexibility in this object, as you can even write custom functions in your pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aecef8e-238d-4f43-81a5-7f3baafec7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_inflation_factor(exog, idx_kept, vif_idx):\n",
    "    \"\"\"Compute VIF for one feature.\n",
    "    \n",
    "    Args:\n",
    "        exog (np.ndarray): Observations\n",
    "        idx_kept (List[int]): Indices of features to consider\n",
    "        vif_idx (int): Index of feature for which to compute VIF\n",
    "    \n",
    "    Returns:\n",
    "        float: VIF for the selected feature\n",
    "    \"\"\"\n",
    "    exog = np.asarray(exog)\n",
    "    \n",
    "    x_i = exog[:, vif_idx]\n",
    "    mask = [col for col in idx_kept if col != vif_idx]\n",
    "    x_noti = exog[:, mask]\n",
    "    \n",
    "    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n",
    "    vif = 1. / (1. - r_squared_i)\n",
    "    \n",
    "    return vif\n",
    "\n",
    "class ReduceVIF(base.BaseEstimator, base.TransformerMixin):\n",
    "    \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class;\n",
    "    I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, thresh=10, max_drop=20):\n",
    "        self.thresh = thresh\n",
    "        self.max_drop = max_drop\n",
    "        self.column_indices_kept_ = []\n",
    "        self.feature_names_kept_ = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the state of predictor columns after each fold.\"\"\"\n",
    "\n",
    "        self.column_indices_kept_ = []\n",
    "        self.feature_names_kept_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names\n",
    "\n",
    "        Args:\n",
    "            X ([type]): [description]\n",
    "            y ([type], optional): [description]. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.column_indices_kept_, self.feature_names_kept_ = self.calculate_vif(X)     \n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Transforms the Validation Set according to the selected feature names.\n",
    "\n",
    "        Args:\n",
    "            X ([type]): [description]\n",
    "            y ([type], optional): [description]. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "\n",
    "        return X[:, self.column_indices_kept_]\n",
    "\n",
    "    def calculate_vif(self, X: Union[np.ndarray, pd.DataFrame]):\n",
    "        \"\"\"Implements a VIF function that recursively eliminates features.\n",
    "\n",
    "        Args:\n",
    "            X (Union[np.ndarray, pd.DataFrame]): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        feature_names = None\n",
    "        column_indices_kept = list(range(X.shape[1]))\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            feature_names = X.columns\n",
    "\n",
    "        dropped = True\n",
    "        count = 0\n",
    "        \n",
    "        while dropped and count <= self.max_drop:\n",
    "            dropped = False\n",
    "            \n",
    "            max_vif, max_vif_col = None, None\n",
    "            \n",
    "            for col in column_indices_kept:\n",
    "                \n",
    "                vif = variance_inflation_factor(X, column_indices_kept, col)\n",
    "                \n",
    "                if max_vif is None or vif > max_vif:\n",
    "                    max_vif = vif\n",
    "                    max_vif_col = col\n",
    "            \n",
    "            if max_vif > self.thresh:\n",
    "                print(f\"Dropping {max_vif_col} with vif={max_vif}\")\n",
    "                column_indices_kept.remove(max_vif_col)\n",
    "                \n",
    "                if feature_names is not None:\n",
    "                    feature_names.pop(max_vif_col)\n",
    "                    \n",
    "                dropped = True\n",
    "                count += 1\n",
    "                \n",
    "        return column_indices_kept, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "186a6a2b-8e75-4e1d-adf7-4d55ea8d8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature preparation pipeline for a model\n",
    "def make_pipeline(model):\n",
    "    \"\"\"Make a Pipeline for Training.\n",
    "\n",
    "    Args:\n",
    "        model ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    \n",
    "    steps = list()\n",
    "    # standardization\n",
    "    steps.append(('standardize', preprocessing.StandardScaler()))\n",
    "    # reduce VIF\n",
    "    # steps.append((\"remove_multicollinearity\", ReduceVIF(thresh=10)))\n",
    "    # the model\n",
    "    steps.append(('model', model))\n",
    "    # create pipeline\n",
    "    _pipeline = pipeline.Pipeline(steps=steps)\n",
    "    return _pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82edb44c-1c71-4c0e-88e6-ab08860e16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [\n",
    "    # baseline model\n",
    "    dummy.DummyRegressor(strategy=\"mean\"),\n",
    "    # linear model\n",
    "    linear_model.LinearRegression(fit_intercept=True),\n",
    "    linear_model.Ridge(random_state=config.seed, alpha=1, fit_intercept=True),\n",
    "    linear_model.Lasso(\n",
    "        random_state=config.seed,\n",
    "        alpha=1,\n",
    "        fit_intercept=True,\n",
    "    ),\n",
    "    linear_model.ElasticNet(\n",
    "        random_state=config.seed,\n",
    "        alpha=1,\n",
    "        l1_ratio=0.5,\n",
    "        fit_intercept=True,\n",
    "    ),\n",
    "    # tree\n",
    "    tree.DecisionTreeRegressor(\n",
    "        random_state=config.seed, criterion=\"squared_error\"\n",
    "    ),\n",
    "    # ensemble\n",
    "    #  ensemble.RandomForestClassifier(random_state=config.seed),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0926fda-d405-4a4b-a18e-11aff82bc093",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [make_pipeline(model) for model in regressors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d92ec-1d00-4af3-819b-2c9cf8e60c60",
   "metadata": {},
   "source": [
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53104e31-fe84-4911-b4b7-af000432c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_result_names = [\n",
    "    \"y_true\",\n",
    "    \"y_pred\",\n",
    "]\n",
    "\n",
    "default_logit_names = [\n",
    "    \"y_true\",\n",
    "    \"y_pred\",\n",
    "]\n",
    "\n",
    "default_score_names = [\n",
    "    \"explained_variance_score\",\n",
    "    \"mean_squared_error\",\n",
    "    \"mean_absolute_error\",\n",
    "    \"root_mean_squared_error\",\n",
    "    \"r2_score\",\n",
    "    \"mean_absolute_percentage_error\",\n",
    "]\n",
    "\n",
    "custom_score_names = [\"adjusted_r2\"]\n",
    "\n",
    "\n",
    "def adjusted_r2(r2: float, n: int, k: int) -> float:\n",
    "    \"\"\"Calculate adjusted R^2.\n",
    "\n",
    "    Args:\n",
    "        r2 (float): r2 score of the model/\n",
    "        n (int): number of samples.\n",
    "        k (int): number of features minus the constant bias term.\n",
    "\n",
    "    Returns:\n",
    "        adjusted_r2_score (float): r2 * (n - 1) / (n - k - 1)\n",
    "    \"\"\"\n",
    "    adjusted_r2_score = r2 * (n - 1) / (n - k - 1)\n",
    "    return adjusted_r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f233f365-1aac-4154-8264-ac81ebe86ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results:\n",
    "    \"\"\"Stores results for model training in columnwise format.\"\"\"\n",
    "    \n",
    "    _result_dict: Dict\n",
    "        \n",
    "    logit_names: List[str]\n",
    "    score_names: List[str]\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        logit_names: List[str] = default_logit_names,\n",
    "        score_names: List[str] = default_score_names,\n",
    "        existing_dict: Optional[Dict] = None,\n",
    "    ):\n",
    "        \"\"\"Construct a new results store.\"\"\"       \n",
    "        self.logit_names = logit_names\n",
    "        self.score_names = score_names\n",
    "        \n",
    "        if existing_dict is not None:\n",
    "            self._result_dict = copy.deepcopy(existing_dict)\n",
    "            return\n",
    "        \n",
    "        dict_keys = [\"identifier\", *logit_names, *score_names]\n",
    "        \n",
    "        self._result_dict = {\n",
    "            key: [] for key in dict_keys\n",
    "        }\n",
    "    \n",
    "    def add(self, identifier: str, results: Dict, in_place=False):\n",
    "        \"\"\"Add a new results row.\"\"\"        \n",
    "        if not in_place:\n",
    "            return Results(\n",
    "                self.logit_names,\n",
    "                self.score_names,\n",
    "                self._result_dict\n",
    "            ).add(identifier, results, in_place=True)\n",
    "        \n",
    "        self._result_dict[\"identifier\"].append(identifier)\n",
    "        \n",
    "        for result_name in set([*results.keys(), *self.logit_names, *self.score_names]):\n",
    "            \n",
    "            result_value = results.get(result_name, np.nan)\n",
    "            \n",
    "            self._result_dict[result_name].append(result_value)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_result(self, result_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get a map of identifiers to result values for a result.\"\"\"\n",
    "        return {\n",
    "            identifier: result_value for\n",
    "            identifier, result_value in\n",
    "            zip(self._result_dict[\"identifier\"], self._result_dict[result_name])\n",
    "        }\n",
    "    \n",
    "    def get_result_values(self, result_name: str) -> List[Any]:\n",
    "        \"\"\"Get a list of values for a result.\"\"\"\n",
    "        return self._result_dict[result_name]\n",
    "    \n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Get a Data Frame containing the results.\"\"\"\n",
    "        return pd.DataFrame.from_dict(self._result_dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Get a dictionary containing the results.\n",
    "        \n",
    "        Returns:\n",
    "             Dict[str, List[Any]]: Dictionary of result columns \n",
    "        \"\"\"\n",
    "        return self._result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4e061c2-4004-47db-bb6c-8d1554547e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits: Dict[str, np.ndarray]) -> Dict[str, Any]:\n",
    "    \"\"\"Compute metrics from logits.\"\"\"\n",
    "\n",
    "    y_true, y_pred = logits[\"y_true\"], logits[\"y_pred\"]\n",
    "\n",
    "    default_score_names = [\n",
    "        \"explained_variance_score\",\n",
    "        \"mean_squared_error\",\n",
    "        \"mean_absolute_error\",\n",
    "        \"root_mean_squared_error\",\n",
    "        \"r2_score\",\n",
    "        \"mean_absolute_percentage_error\",\n",
    "    ]\n",
    "\n",
    "    default_metrics_dict: Dict[str, float] = {}\n",
    "    custom_metrics_dict: Dict[str, float] = {}\n",
    "\n",
    "    for metric_name in default_score_names:\n",
    "        if hasattr(metrics._regression, metric_name):\n",
    "            # TODO: get metric score with default parameters, consider adding kwargs if you want to configure parameters\n",
    "            metric_score = getattr(metrics._regression, metric_name)(y_true, y_pred)\n",
    "        else:\n",
    "            # logger.info(f\"{metrics._regression} has no such attribute {metric_name}!\")\n",
    "            # add custom metrics here\n",
    "            rmse = metrics._regression.mean_squared_error(y_true, y_pred, squared=False)\n",
    "            custom_metrics_dict[\"root_mean_squared_error\"] = rmse\n",
    "            \n",
    "        if metric_name not in default_metrics_dict:\n",
    "            default_metrics_dict[metric_name] = metric_score\n",
    "        \n",
    "        metrics_dict = {**default_metrics_dict, **custom_metrics_dict}\n",
    "            \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7eaa6809-435c-4f37-ad41-fee9102c1477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_score(score_values) -> Union[float, np.ndarray]:\n",
    "    \"\"\"Compute the mean score.\"\"\"\n",
    "    \n",
    "    score_values = np.array(score_values)\n",
    "    \n",
    "    shape = score_values.shape\n",
    "    \n",
    "    if len(shape) == 1:\n",
    "        return score_values.mean()\n",
    "    \n",
    "    return score_values.mean(axis=0)\n",
    "\n",
    "def mean_cv_results(model_results: Results) -> Dict:\n",
    "    \"\"\"Add mean cross-validation results.\n",
    "    \n",
    "    This method computes the mean value for all\n",
    "    score types in the model_results, including\n",
    "    for scores (e.g., confusion matrix) where\n",
    "    the mean value may contain decimal places.\n",
    "    \"\"\"\n",
    "    cv_logits = {\n",
    "        y_result: np.concatenate(model_results.get_result_values(y_result))\n",
    "        for y_result in\n",
    "        model_results.logit_names\n",
    "    }\n",
    "    \n",
    "    cv_scores = {\n",
    "        score: mean_score(\n",
    "            model_results.get_result_values(score)\n",
    "        )\n",
    "        for score in model_results.score_names\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        **cv_logits,\n",
    "        **cv_scores,\n",
    "    }\n",
    "\n",
    "def oof_cv_results(model_results: Results) -> Dict:\n",
    "    \"\"\"Add OOF cross-validation results.\"\"\"\n",
    "    \n",
    "    cv_logits = {\n",
    "        y_result: np.concatenate(\n",
    "            model_results.get_result_values(y_result)\n",
    "        )\n",
    "        for y_result in\n",
    "        model_results.logit_names\n",
    "    }\n",
    "    \n",
    "    cv_scores = compute_metrics(cv_logits)\n",
    "    \n",
    "    return {\n",
    "        **cv_logits,\n",
    "        **cv_scores,\n",
    "    }\n",
    "\n",
    "def add_cv_results(model_results: Results):\n",
    "    \"\"\"Add cross-validation results.\n",
    "    \n",
    "    This method returns a copy of the given model results\n",
    "    with summary columns for mean and CV cross-validation.\n",
    "    \"\"\"\n",
    "    mean_cv = mean_cv_results(model_results)\n",
    "    oof_cv = oof_cv_results(model_results)\n",
    "    \n",
    "    return (\n",
    "        model_results\n",
    "        .add(\"mean_cv\", mean_cv)\n",
    "        .add(\"oof_cv\", oof_cv)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f25cb461-dca0-43af-963a-59ae478a4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_fold(\n",
    "    df_folds: pd.DataFrame,\n",
    "    models: List[Callable],\n",
    "    num_folds: int,\n",
    "    predictor_col: List,\n",
    "    target_col: List,\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results.\n",
    "\n",
    "    Args:\n",
    "        df_folds (pd.DataFrame): [description]\n",
    "        model (Callable): [description]\n",
    "        num_folds (int): [description]\n",
    "        predictor_col (List): [description]\n",
    "        target_col (List): [description]\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List]: [description]\n",
    "    \"\"\"\n",
    "  \n",
    "    y_true = df_folds[target_col].values.flatten()\n",
    "\n",
    "    # test_pred_arr: np.ndarray = np.zeros(len(X_test))\n",
    "\n",
    "    model_dict = {}\n",
    "\n",
    "    for model in models:\n",
    "        model_results = Results()\n",
    "\n",
    "        if isinstance(model, pipeline.Pipeline):\n",
    "            model_name = model[\"model\"].__class__.__name__\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "\n",
    "        # out-of-fold validation predictions\n",
    "        oof_pred_arr: np.ndarray = np.zeros(len(df_folds))\n",
    "      \n",
    "        for fold in range(1, num_folds + 1):\n",
    "\n",
    "            train_df = df_folds[df_folds[\"fold\"] != fold].reset_index(drop=True)\n",
    "            val_df = df_folds[df_folds[\"fold\"] == fold].reset_index(drop=True)\n",
    "            val_idx = df_folds[df_folds[\"fold\"] == fold].index.values\n",
    "            X_train, y_train = train_df[predictor_col].values, train_df[target_col].values\n",
    "            X_val, y_val = val_df[predictor_col].values, val_df[target_col].values\n",
    "    \n",
    "            model.fit(X_train, y_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "\n",
    "            \n",
    "            logits = {\n",
    "                \"y_true\": y_val,\n",
    "                \"y_pred\": y_val_pred,\n",
    "            }\n",
    "            \n",
    "            metrics = compute_metrics(logits)\n",
    "            \n",
    "            model_results.add(f\"fold {fold}\", {\n",
    "                **logits,\n",
    "                **metrics\n",
    "            }, in_place=True)\n",
    "            \n",
    "           \n",
    "        if model_name not in model_dict:\n",
    "            model_dict[model_name] = model_results\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f84b8e7-457d-434b-9111-7cc387f63636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.676e+13, tolerance: 1.868e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "2021-11-10,12:09:46 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+13, tolerance: 1.862e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "2021-11-10,12:09:48 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.603e+13, tolerance: 1.760e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "2021-11-10,12:09:49 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+13, tolerance: 1.897e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "2021-11-10,12:09:50 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e+13, tolerance: 1.913e+11\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:53 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:09:53 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n"
     ]
    }
   ],
   "source": [
    "model_dict = train_on_fold(\n",
    "    df_folds,\n",
    "    models = regressors,\n",
    "    num_folds=5,\n",
    "    predictor_col=predictor_cols,\n",
    "    target_col = target_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e2c52c1-5388-4dce-87eb-53af38365ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n",
      "2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error!\n"
     ]
    }
   ],
   "source": [
    "model_dict_with_summary = {\n",
    "    model: add_cv_results(model_results)\n",
    "    for model, model_results in model_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f492982e-3f34-431e-beb9-11bc01fab817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>fold 5</th>\n",
       "      <th>mean_cv</th>\n",
       "      <th>oof_cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">DummyRegressor</th>\n",
       "      <th>identifier</th>\n",
       "      <td>fold 1</td>\n",
       "      <td>fold 2</td>\n",
       "      <td>fold 3</td>\n",
       "      <td>fold 4</td>\n",
       "      <td>fold 5</td>\n",
       "      <td>mean_cv</td>\n",
       "      <td>oof_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_true</th>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "      <td>[[850000.0], [653000.0], [532000.0], [385000.0...</td>\n",
       "      <td>[[1274950.0], [392137.0], [850000.0], [520000....</td>\n",
       "      <td>[[450000.0], [495000.0], [395000.0], [280000.0...</td>\n",
       "      <td>[[754999.0], [588500.0], [525000.0], [525000.0...</td>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_pred</th>\n",
       "      <td>[541073.7741469058, 541073.7741469058, 541073....</td>\n",
       "      <td>[539415.1803788317, 539415.1803788317, 539415....</td>\n",
       "      <td>[536694.9069548872, 536694.9069548872, 536694....</td>\n",
       "      <td>[539857.5267495662, 539857.5267495662, 539857....</td>\n",
       "      <td>[540990.9316078658, 540990.9316078658, 540990....</td>\n",
       "      <td>[541073.7741469058, 541073.7741469058, 541073....</td>\n",
       "      <td>[541073.7741469058, 541073.7741469058, 541073....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explained_variance_score</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_squared_error</th>\n",
       "      <td>132247512137.06163</td>\n",
       "      <td>133951959536.310318</td>\n",
       "      <td>163296091978.429077</td>\n",
       "      <td>123686085780.066254</td>\n",
       "      <td>119223815039.600311</td>\n",
       "      <td>134481092894.293533</td>\n",
       "      <td>134481092894.293488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <td>229404.508989</td>\n",
       "      <td>232306.56185</td>\n",
       "      <td>241944.838637</td>\n",
       "      <td>232308.320526</td>\n",
       "      <td>230976.128917</td>\n",
       "      <td>233388.071784</td>\n",
       "      <td>233388.071784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <td>363658.510332</td>\n",
       "      <td>365994.480199</td>\n",
       "      <td>404099.111578</td>\n",
       "      <td>351690.326538</td>\n",
       "      <td>345288.017515</td>\n",
       "      <td>366146.089233</td>\n",
       "      <td>366716.63842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_score</th>\n",
       "      <td>-0.000407</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.0013</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000402</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>-0.000169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_percentage_error</th>\n",
       "      <td>0.542257</td>\n",
       "      <td>0.530016</td>\n",
       "      <td>0.531438</td>\n",
       "      <td>0.534771</td>\n",
       "      <td>0.540719</td>\n",
       "      <td>0.53584</td>\n",
       "      <td>0.53584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">LinearRegression</th>\n",
       "      <th>identifier</th>\n",
       "      <td>fold 1</td>\n",
       "      <td>fold 2</td>\n",
       "      <td>fold 3</td>\n",
       "      <td>fold 4</td>\n",
       "      <td>fold 5</td>\n",
       "      <td>mean_cv</td>\n",
       "      <td>oof_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_true</th>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "      <td>[[850000.0], [653000.0], [532000.0], [385000.0...</td>\n",
       "      <td>[[1274950.0], [392137.0], [850000.0], [520000....</td>\n",
       "      <td>[[450000.0], [495000.0], [395000.0], [280000.0...</td>\n",
       "      <td>[[754999.0], [588500.0], [525000.0], [525000.0...</td>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_pred</th>\n",
       "      <td>[[667876.5428875468], [614410.8399372244], [73...</td>\n",
       "      <td>[[694869.3745167988], [618367.5827104518], [46...</td>\n",
       "      <td>[[1023189.3043934056], [304317.4518826463], [9...</td>\n",
       "      <td>[[477332.04737439007], [492396.46923110134], [...</td>\n",
       "      <td>[[853190.5779001702], [574292.3295129627], [77...</td>\n",
       "      <td>[[667876.5428875468], [614410.8399372244], [73...</td>\n",
       "      <td>[[667876.5428875468], [614410.8399372244], [73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explained_variance_score</th>\n",
       "      <td>0.680095</td>\n",
       "      <td>0.705119</td>\n",
       "      <td>0.685252</td>\n",
       "      <td>0.693963</td>\n",
       "      <td>0.721335</td>\n",
       "      <td>0.697153</td>\n",
       "      <td>0.696083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_squared_error</th>\n",
       "      <td>42290033057.306816</td>\n",
       "      <td>39515957195.400787</td>\n",
       "      <td>51430470051.222527</td>\n",
       "      <td>37873842125.074295</td>\n",
       "      <td>33210290838.463482</td>\n",
       "      <td>40864118653.493584</td>\n",
       "      <td>40864118653.493584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <td>127943.982537</td>\n",
       "      <td>124266.336776</td>\n",
       "      <td>130373.471382</td>\n",
       "      <td>126075.417537</td>\n",
       "      <td>122514.978064</td>\n",
       "      <td>126234.837259</td>\n",
       "      <td>126234.837259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <td>205645.406118</td>\n",
       "      <td>198786.209772</td>\n",
       "      <td>226782.869836</td>\n",
       "      <td>194612.029754</td>\n",
       "      <td>182236.908552</td>\n",
       "      <td>201612.684806</td>\n",
       "      <td>202148.753777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_score</th>\n",
       "      <td>0.68009</td>\n",
       "      <td>0.704997</td>\n",
       "      <td>0.684638</td>\n",
       "      <td>0.693787</td>\n",
       "      <td>0.721334</td>\n",
       "      <td>0.696969</td>\n",
       "      <td>0.696083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_percentage_error</th>\n",
       "      <td>0.265815</td>\n",
       "      <td>0.257898</td>\n",
       "      <td>0.250641</td>\n",
       "      <td>0.251933</td>\n",
       "      <td>0.255471</td>\n",
       "      <td>0.256352</td>\n",
       "      <td>0.256352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">Ridge</th>\n",
       "      <th>identifier</th>\n",
       "      <td>fold 1</td>\n",
       "      <td>fold 2</td>\n",
       "      <td>fold 3</td>\n",
       "      <td>fold 4</td>\n",
       "      <td>fold 5</td>\n",
       "      <td>mean_cv</td>\n",
       "      <td>oof_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_true</th>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "      <td>[[850000.0], [653000.0], [532000.0], [385000.0...</td>\n",
       "      <td>[[1274950.0], [392137.0], [850000.0], [520000....</td>\n",
       "      <td>[[450000.0], [495000.0], [395000.0], [280000.0...</td>\n",
       "      <td>[[754999.0], [588500.0], [525000.0], [525000.0...</td>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_pred</th>\n",
       "      <td>[[667923.954879363], [614423.8176756387], [731...</td>\n",
       "      <td>[[694094.2870407818], [617981.9933915635], [46...</td>\n",
       "      <td>[[1023626.0773182933], [304085.70358412666], [...</td>\n",
       "      <td>[[477416.1959023318], [492474.1026423527], [30...</td>\n",
       "      <td>[[853485.3791880516], [574203.5276395974], [77...</td>\n",
       "      <td>[[667923.954879363], [614423.8176756387], [731...</td>\n",
       "      <td>[[667923.954879363], [614423.8176756387], [731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explained_variance_score</th>\n",
       "      <td>0.680109</td>\n",
       "      <td>0.70512</td>\n",
       "      <td>0.685333</td>\n",
       "      <td>0.693972</td>\n",
       "      <td>0.721308</td>\n",
       "      <td>0.697168</td>\n",
       "      <td>0.696102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_squared_error</th>\n",
       "      <td>42288266590.795799</td>\n",
       "      <td>39515834657.582001</td>\n",
       "      <td>51417532361.73008</td>\n",
       "      <td>37872817115.509499</td>\n",
       "      <td>33213482338.320721</td>\n",
       "      <td>40861586612.787613</td>\n",
       "      <td>40861586612.787621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <td>127934.770004</td>\n",
       "      <td>124362.619797</td>\n",
       "      <td>130426.951817</td>\n",
       "      <td>126064.678687</td>\n",
       "      <td>122549.911589</td>\n",
       "      <td>126267.786379</td>\n",
       "      <td>126267.786379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <td>205641.11114</td>\n",
       "      <td>198785.901556</td>\n",
       "      <td>226754.343645</td>\n",
       "      <td>194609.396267</td>\n",
       "      <td>182245.6648</td>\n",
       "      <td>201607.283482</td>\n",
       "      <td>202142.490864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_score</th>\n",
       "      <td>0.680104</td>\n",
       "      <td>0.704998</td>\n",
       "      <td>0.684718</td>\n",
       "      <td>0.693795</td>\n",
       "      <td>0.721307</td>\n",
       "      <td>0.696984</td>\n",
       "      <td>0.696102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_percentage_error</th>\n",
       "      <td>0.265796</td>\n",
       "      <td>0.258152</td>\n",
       "      <td>0.250808</td>\n",
       "      <td>0.251901</td>\n",
       "      <td>0.255562</td>\n",
       "      <td>0.256444</td>\n",
       "      <td>0.256444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">Lasso</th>\n",
       "      <th>identifier</th>\n",
       "      <td>fold 1</td>\n",
       "      <td>fold 2</td>\n",
       "      <td>fold 3</td>\n",
       "      <td>fold 4</td>\n",
       "      <td>fold 5</td>\n",
       "      <td>mean_cv</td>\n",
       "      <td>oof_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_true</th>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "      <td>[[850000.0], [653000.0], [532000.0], [385000.0...</td>\n",
       "      <td>[[1274950.0], [392137.0], [850000.0], [520000....</td>\n",
       "      <td>[[450000.0], [495000.0], [395000.0], [280000.0...</td>\n",
       "      <td>[[754999.0], [588500.0], [525000.0], [525000.0...</td>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_pred</th>\n",
       "      <td>[667918.069528075, 614415.1560338883, 731173.9...</td>\n",
       "      <td>[694081.9059207196, 617992.7838152755, 463279....</td>\n",
       "      <td>[1023674.2634054194, 304066.4170321575, 955604...</td>\n",
       "      <td>[477415.1327845714, 492463.7396309793, 305057....</td>\n",
       "      <td>[853505.6953654164, 574205.7699892861, 771097....</td>\n",
       "      <td>[667918.069528075, 614415.1560338883, 731173.9...</td>\n",
       "      <td>[667918.069528075, 614415.1560338883, 731173.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explained_variance_score</th>\n",
       "      <td>0.680106</td>\n",
       "      <td>0.70512</td>\n",
       "      <td>0.685338</td>\n",
       "      <td>0.693969</td>\n",
       "      <td>0.721305</td>\n",
       "      <td>0.697168</td>\n",
       "      <td>0.696102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_squared_error</th>\n",
       "      <td>42288598880.705284</td>\n",
       "      <td>39515796822.947304</td>\n",
       "      <td>51416606662.838509</td>\n",
       "      <td>37873203587.611603</td>\n",
       "      <td>33213918899.943615</td>\n",
       "      <td>40861624970.809258</td>\n",
       "      <td>40861624970.809265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <td>127937.742862</td>\n",
       "      <td>124365.678646</td>\n",
       "      <td>130429.153983</td>\n",
       "      <td>126067.441635</td>\n",
       "      <td>122553.005403</td>\n",
       "      <td>126270.604506</td>\n",
       "      <td>126270.604506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <td>205641.919075</td>\n",
       "      <td>198785.806392</td>\n",
       "      <td>226752.302442</td>\n",
       "      <td>194610.389208</td>\n",
       "      <td>182246.862524</td>\n",
       "      <td>201607.455928</td>\n",
       "      <td>202142.585743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_score</th>\n",
       "      <td>0.680101</td>\n",
       "      <td>0.704998</td>\n",
       "      <td>0.684723</td>\n",
       "      <td>0.693792</td>\n",
       "      <td>0.721303</td>\n",
       "      <td>0.696984</td>\n",
       "      <td>0.696102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_percentage_error</th>\n",
       "      <td>0.265806</td>\n",
       "      <td>0.258161</td>\n",
       "      <td>0.250816</td>\n",
       "      <td>0.25191</td>\n",
       "      <td>0.255571</td>\n",
       "      <td>0.256453</td>\n",
       "      <td>0.256453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">ElasticNet</th>\n",
       "      <th>identifier</th>\n",
       "      <td>fold 1</td>\n",
       "      <td>fold 2</td>\n",
       "      <td>fold 3</td>\n",
       "      <td>fold 4</td>\n",
       "      <td>fold 5</td>\n",
       "      <td>mean_cv</td>\n",
       "      <td>oof_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_true</th>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "      <td>[[850000.0], [653000.0], [532000.0], [385000.0...</td>\n",
       "      <td>[[1274950.0], [392137.0], [850000.0], [520000....</td>\n",
       "      <td>[[450000.0], [495000.0], [395000.0], [280000.0...</td>\n",
       "      <td>[[754999.0], [588500.0], [525000.0], [525000.0...</td>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_pred</th>\n",
       "      <td>[656773.6005291657, 614707.2479590292, 745914....</td>\n",
       "      <td>[685212.4217947914, 599436.3647276227, 483018....</td>\n",
       "      <td>[907890.7497500202, 369129.65691215475, 862824...</td>\n",
       "      <td>[469736.2740988735, 539209.0261494196, 379320....</td>\n",
       "      <td>[804965.135403886, 571735.3944930851, 737716.4...</td>\n",
       "      <td>[656773.6005291657, 614707.2479590292, 745914....</td>\n",
       "      <td>[656773.6005291657, 614707.2479590292, 745914....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explained_variance_score</th>\n",
       "      <td>0.65603</td>\n",
       "      <td>0.673131</td>\n",
       "      <td>0.638955</td>\n",
       "      <td>0.670126</td>\n",
       "      <td>0.695926</td>\n",
       "      <td>0.666834</td>\n",
       "      <td>0.664827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_squared_error</th>\n",
       "      <td>45475490348.208717</td>\n",
       "      <td>43800728719.55619</td>\n",
       "      <td>59005250118.409103</td>\n",
       "      <td>40812502573.376862</td>\n",
       "      <td>36239917129.190384</td>\n",
       "      <td>45066777777.748253</td>\n",
       "      <td>45066777777.748253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <td>124419.180393</td>\n",
       "      <td>122248.905619</td>\n",
       "      <td>131347.081356</td>\n",
       "      <td>124722.490988</td>\n",
       "      <td>120613.606068</td>\n",
       "      <td>124670.252885</td>\n",
       "      <td>124670.252885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <td>213249.830828</td>\n",
       "      <td>209286.236336</td>\n",
       "      <td>242909.962987</td>\n",
       "      <td>202021.044877</td>\n",
       "      <td>190367.846889</td>\n",
       "      <td>211566.984383</td>\n",
       "      <td>212289.372739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_score</th>\n",
       "      <td>0.655993</td>\n",
       "      <td>0.673009</td>\n",
       "      <td>0.638191</td>\n",
       "      <td>0.670027</td>\n",
       "      <td>0.695912</td>\n",
       "      <td>0.666627</td>\n",
       "      <td>0.664827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_percentage_error</th>\n",
       "      <td>0.247878</td>\n",
       "      <td>0.245323</td>\n",
       "      <td>0.243383</td>\n",
       "      <td>0.244499</td>\n",
       "      <td>0.243172</td>\n",
       "      <td>0.244851</td>\n",
       "      <td>0.244851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">DecisionTreeRegressor</th>\n",
       "      <th>identifier</th>\n",
       "      <td>fold 1</td>\n",
       "      <td>fold 2</td>\n",
       "      <td>fold 3</td>\n",
       "      <td>fold 4</td>\n",
       "      <td>fold 5</td>\n",
       "      <td>mean_cv</td>\n",
       "      <td>oof_cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_true</th>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "      <td>[[850000.0], [653000.0], [532000.0], [385000.0...</td>\n",
       "      <td>[[1274950.0], [392137.0], [850000.0], [520000....</td>\n",
       "      <td>[[450000.0], [495000.0], [395000.0], [280000.0...</td>\n",
       "      <td>[[754999.0], [588500.0], [525000.0], [525000.0...</td>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_pred</th>\n",
       "      <td>[915000.0, 300000.0, 316000.0, 643000.0, 28200...</td>\n",
       "      <td>[535000.0, 630000.0, 410000.0, 335000.0, 72000...</td>\n",
       "      <td>[1605000.0, 479000.0, 850000.0, 614000.0, 6505...</td>\n",
       "      <td>[449000.0, 555000.0, 440000.0, 247500.0, 35000...</td>\n",
       "      <td>[715000.0, 545000.0, 597000.0, 530000.0, 50000...</td>\n",
       "      <td>[915000.0, 300000.0, 316000.0, 643000.0, 28200...</td>\n",
       "      <td>[915000.0, 300000.0, 316000.0, 643000.0, 28200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explained_variance_score</th>\n",
       "      <td>0.752389</td>\n",
       "      <td>0.756145</td>\n",
       "      <td>0.770122</td>\n",
       "      <td>0.743783</td>\n",
       "      <td>0.767617</td>\n",
       "      <td>0.758011</td>\n",
       "      <td>0.758577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_squared_error</th>\n",
       "      <td>32751471474.735687</td>\n",
       "      <td>32664895113.423656</td>\n",
       "      <td>37510896623.824608</td>\n",
       "      <td>31693198492.168594</td>\n",
       "      <td>27707113797.312031</td>\n",
       "      <td>32465515100.292915</td>\n",
       "      <td>32465515100.292915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <td>101035.96819</td>\n",
       "      <td>99810.482071</td>\n",
       "      <td>105829.744939</td>\n",
       "      <td>101019.912377</td>\n",
       "      <td>96233.899075</td>\n",
       "      <td>100786.00133</td>\n",
       "      <td>100786.00133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <td>180973.676193</td>\n",
       "      <td>180734.321902</td>\n",
       "      <td>193677.300229</td>\n",
       "      <td>178025.836586</td>\n",
       "      <td>166454.539732</td>\n",
       "      <td>179973.134928</td>\n",
       "      <td>180181.894485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_score</th>\n",
       "      <td>0.752246</td>\n",
       "      <td>0.756143</td>\n",
       "      <td>0.769991</td>\n",
       "      <td>0.743758</td>\n",
       "      <td>0.767511</td>\n",
       "      <td>0.75793</td>\n",
       "      <td>0.758546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_percentage_error</th>\n",
       "      <td>0.192566</td>\n",
       "      <td>0.186579</td>\n",
       "      <td>0.189517</td>\n",
       "      <td>0.183729</td>\n",
       "      <td>0.184638</td>\n",
       "      <td>0.187406</td>\n",
       "      <td>0.187406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 fold 1  \\\n",
       "DummyRegressor        identifier                                                                 fold 1   \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...   \n",
       "                      y_pred                          [541073.7741469058, 541073.7741469058, 541073....   \n",
       "                      explained_variance_score                                                     -0.0   \n",
       "                      mean_squared_error                                             132247512137.06163   \n",
       "                      mean_absolute_error                                                 229404.508989   \n",
       "                      root_mean_squared_error                                             363658.510332   \n",
       "                      r2_score                                                                -0.000407   \n",
       "                      mean_absolute_percentage_error                                           0.542257   \n",
       "LinearRegression      identifier                                                                 fold 1   \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...   \n",
       "                      y_pred                          [[667876.5428875468], [614410.8399372244], [73...   \n",
       "                      explained_variance_score                                                 0.680095   \n",
       "                      mean_squared_error                                             42290033057.306816   \n",
       "                      mean_absolute_error                                                 127943.982537   \n",
       "                      root_mean_squared_error                                             205645.406118   \n",
       "                      r2_score                                                                  0.68009   \n",
       "                      mean_absolute_percentage_error                                           0.265815   \n",
       "Ridge                 identifier                                                                 fold 1   \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...   \n",
       "                      y_pred                          [[667923.954879363], [614423.8176756387], [731...   \n",
       "                      explained_variance_score                                                 0.680109   \n",
       "                      mean_squared_error                                             42288266590.795799   \n",
       "                      mean_absolute_error                                                 127934.770004   \n",
       "                      root_mean_squared_error                                              205641.11114   \n",
       "                      r2_score                                                                 0.680104   \n",
       "                      mean_absolute_percentage_error                                           0.265796   \n",
       "Lasso                 identifier                                                                 fold 1   \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...   \n",
       "                      y_pred                          [667918.069528075, 614415.1560338883, 731173.9...   \n",
       "                      explained_variance_score                                                 0.680106   \n",
       "                      mean_squared_error                                             42288598880.705284   \n",
       "                      mean_absolute_error                                                 127937.742862   \n",
       "                      root_mean_squared_error                                             205641.919075   \n",
       "                      r2_score                                                                 0.680101   \n",
       "                      mean_absolute_percentage_error                                           0.265806   \n",
       "ElasticNet            identifier                                                                 fold 1   \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...   \n",
       "                      y_pred                          [656773.6005291657, 614707.2479590292, 745914....   \n",
       "                      explained_variance_score                                                  0.65603   \n",
       "                      mean_squared_error                                             45475490348.208717   \n",
       "                      mean_absolute_error                                                 124419.180393   \n",
       "                      root_mean_squared_error                                             213249.830828   \n",
       "                      r2_score                                                                 0.655993   \n",
       "                      mean_absolute_percentage_error                                           0.247878   \n",
       "DecisionTreeRegressor identifier                                                                 fold 1   \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...   \n",
       "                      y_pred                          [915000.0, 300000.0, 316000.0, 643000.0, 28200...   \n",
       "                      explained_variance_score                                                 0.752389   \n",
       "                      mean_squared_error                                             32751471474.735687   \n",
       "                      mean_absolute_error                                                  101035.96819   \n",
       "                      root_mean_squared_error                                             180973.676193   \n",
       "                      r2_score                                                                 0.752246   \n",
       "                      mean_absolute_percentage_error                                           0.192566   \n",
       "\n",
       "                                                                                                 fold 2  \\\n",
       "DummyRegressor        identifier                                                                 fold 2   \n",
       "                      y_true                          [[850000.0], [653000.0], [532000.0], [385000.0...   \n",
       "                      y_pred                          [539415.1803788317, 539415.1803788317, 539415....   \n",
       "                      explained_variance_score                                                      0.0   \n",
       "                      mean_squared_error                                            133951959536.310318   \n",
       "                      mean_absolute_error                                                  232306.56185   \n",
       "                      root_mean_squared_error                                             365994.480199   \n",
       "                      r2_score                                                                -0.000007   \n",
       "                      mean_absolute_percentage_error                                           0.530016   \n",
       "LinearRegression      identifier                                                                 fold 2   \n",
       "                      y_true                          [[850000.0], [653000.0], [532000.0], [385000.0...   \n",
       "                      y_pred                          [[694869.3745167988], [618367.5827104518], [46...   \n",
       "                      explained_variance_score                                                 0.705119   \n",
       "                      mean_squared_error                                             39515957195.400787   \n",
       "                      mean_absolute_error                                                 124266.336776   \n",
       "                      root_mean_squared_error                                             198786.209772   \n",
       "                      r2_score                                                                 0.704997   \n",
       "                      mean_absolute_percentage_error                                           0.257898   \n",
       "Ridge                 identifier                                                                 fold 2   \n",
       "                      y_true                          [[850000.0], [653000.0], [532000.0], [385000.0...   \n",
       "                      y_pred                          [[694094.2870407818], [617981.9933915635], [46...   \n",
       "                      explained_variance_score                                                  0.70512   \n",
       "                      mean_squared_error                                             39515834657.582001   \n",
       "                      mean_absolute_error                                                 124362.619797   \n",
       "                      root_mean_squared_error                                             198785.901556   \n",
       "                      r2_score                                                                 0.704998   \n",
       "                      mean_absolute_percentage_error                                           0.258152   \n",
       "Lasso                 identifier                                                                 fold 2   \n",
       "                      y_true                          [[850000.0], [653000.0], [532000.0], [385000.0...   \n",
       "                      y_pred                          [694081.9059207196, 617992.7838152755, 463279....   \n",
       "                      explained_variance_score                                                  0.70512   \n",
       "                      mean_squared_error                                             39515796822.947304   \n",
       "                      mean_absolute_error                                                 124365.678646   \n",
       "                      root_mean_squared_error                                             198785.806392   \n",
       "                      r2_score                                                                 0.704998   \n",
       "                      mean_absolute_percentage_error                                           0.258161   \n",
       "ElasticNet            identifier                                                                 fold 2   \n",
       "                      y_true                          [[850000.0], [653000.0], [532000.0], [385000.0...   \n",
       "                      y_pred                          [685212.4217947914, 599436.3647276227, 483018....   \n",
       "                      explained_variance_score                                                 0.673131   \n",
       "                      mean_squared_error                                              43800728719.55619   \n",
       "                      mean_absolute_error                                                 122248.905619   \n",
       "                      root_mean_squared_error                                             209286.236336   \n",
       "                      r2_score                                                                 0.673009   \n",
       "                      mean_absolute_percentage_error                                           0.245323   \n",
       "DecisionTreeRegressor identifier                                                                 fold 2   \n",
       "                      y_true                          [[850000.0], [653000.0], [532000.0], [385000.0...   \n",
       "                      y_pred                          [535000.0, 630000.0, 410000.0, 335000.0, 72000...   \n",
       "                      explained_variance_score                                                 0.756145   \n",
       "                      mean_squared_error                                             32664895113.423656   \n",
       "                      mean_absolute_error                                                  99810.482071   \n",
       "                      root_mean_squared_error                                             180734.321902   \n",
       "                      r2_score                                                                 0.756143   \n",
       "                      mean_absolute_percentage_error                                           0.186579   \n",
       "\n",
       "                                                                                                 fold 3  \\\n",
       "DummyRegressor        identifier                                                                 fold 3   \n",
       "                      y_true                          [[1274950.0], [392137.0], [850000.0], [520000....   \n",
       "                      y_pred                          [536694.9069548872, 536694.9069548872, 536694....   \n",
       "                      explained_variance_score                                                      0.0   \n",
       "                      mean_squared_error                                            163296091978.429077   \n",
       "                      mean_absolute_error                                                 241944.838637   \n",
       "                      root_mean_squared_error                                             404099.111578   \n",
       "                      r2_score                                                                  -0.0013   \n",
       "                      mean_absolute_percentage_error                                           0.531438   \n",
       "LinearRegression      identifier                                                                 fold 3   \n",
       "                      y_true                          [[1274950.0], [392137.0], [850000.0], [520000....   \n",
       "                      y_pred                          [[1023189.3043934056], [304317.4518826463], [9...   \n",
       "                      explained_variance_score                                                 0.685252   \n",
       "                      mean_squared_error                                             51430470051.222527   \n",
       "                      mean_absolute_error                                                 130373.471382   \n",
       "                      root_mean_squared_error                                             226782.869836   \n",
       "                      r2_score                                                                 0.684638   \n",
       "                      mean_absolute_percentage_error                                           0.250641   \n",
       "Ridge                 identifier                                                                 fold 3   \n",
       "                      y_true                          [[1274950.0], [392137.0], [850000.0], [520000....   \n",
       "                      y_pred                          [[1023626.0773182933], [304085.70358412666], [...   \n",
       "                      explained_variance_score                                                 0.685333   \n",
       "                      mean_squared_error                                              51417532361.73008   \n",
       "                      mean_absolute_error                                                 130426.951817   \n",
       "                      root_mean_squared_error                                             226754.343645   \n",
       "                      r2_score                                                                 0.684718   \n",
       "                      mean_absolute_percentage_error                                           0.250808   \n",
       "Lasso                 identifier                                                                 fold 3   \n",
       "                      y_true                          [[1274950.0], [392137.0], [850000.0], [520000....   \n",
       "                      y_pred                          [1023674.2634054194, 304066.4170321575, 955604...   \n",
       "                      explained_variance_score                                                 0.685338   \n",
       "                      mean_squared_error                                             51416606662.838509   \n",
       "                      mean_absolute_error                                                 130429.153983   \n",
       "                      root_mean_squared_error                                             226752.302442   \n",
       "                      r2_score                                                                 0.684723   \n",
       "                      mean_absolute_percentage_error                                           0.250816   \n",
       "ElasticNet            identifier                                                                 fold 3   \n",
       "                      y_true                          [[1274950.0], [392137.0], [850000.0], [520000....   \n",
       "                      y_pred                          [907890.7497500202, 369129.65691215475, 862824...   \n",
       "                      explained_variance_score                                                 0.638955   \n",
       "                      mean_squared_error                                             59005250118.409103   \n",
       "                      mean_absolute_error                                                 131347.081356   \n",
       "                      root_mean_squared_error                                             242909.962987   \n",
       "                      r2_score                                                                 0.638191   \n",
       "                      mean_absolute_percentage_error                                           0.243383   \n",
       "DecisionTreeRegressor identifier                                                                 fold 3   \n",
       "                      y_true                          [[1274950.0], [392137.0], [850000.0], [520000....   \n",
       "                      y_pred                          [1605000.0, 479000.0, 850000.0, 614000.0, 6505...   \n",
       "                      explained_variance_score                                                 0.770122   \n",
       "                      mean_squared_error                                             37510896623.824608   \n",
       "                      mean_absolute_error                                                 105829.744939   \n",
       "                      root_mean_squared_error                                             193677.300229   \n",
       "                      r2_score                                                                 0.769991   \n",
       "                      mean_absolute_percentage_error                                           0.189517   \n",
       "\n",
       "                                                                                                 fold 4  \\\n",
       "DummyRegressor        identifier                                                                 fold 4   \n",
       "                      y_true                          [[450000.0], [495000.0], [395000.0], [280000.0...   \n",
       "                      y_pred                          [539857.5267495662, 539857.5267495662, 539857....   \n",
       "                      explained_variance_score                                                      0.0   \n",
       "                      mean_squared_error                                            123686085780.066254   \n",
       "                      mean_absolute_error                                                 232308.320526   \n",
       "                      root_mean_squared_error                                             351690.326538   \n",
       "                      r2_score                                                                -0.000013   \n",
       "                      mean_absolute_percentage_error                                           0.534771   \n",
       "LinearRegression      identifier                                                                 fold 4   \n",
       "                      y_true                          [[450000.0], [495000.0], [395000.0], [280000.0...   \n",
       "                      y_pred                          [[477332.04737439007], [492396.46923110134], [...   \n",
       "                      explained_variance_score                                                 0.693963   \n",
       "                      mean_squared_error                                             37873842125.074295   \n",
       "                      mean_absolute_error                                                 126075.417537   \n",
       "                      root_mean_squared_error                                             194612.029754   \n",
       "                      r2_score                                                                 0.693787   \n",
       "                      mean_absolute_percentage_error                                           0.251933   \n",
       "Ridge                 identifier                                                                 fold 4   \n",
       "                      y_true                          [[450000.0], [495000.0], [395000.0], [280000.0...   \n",
       "                      y_pred                          [[477416.1959023318], [492474.1026423527], [30...   \n",
       "                      explained_variance_score                                                 0.693972   \n",
       "                      mean_squared_error                                             37872817115.509499   \n",
       "                      mean_absolute_error                                                 126064.678687   \n",
       "                      root_mean_squared_error                                             194609.396267   \n",
       "                      r2_score                                                                 0.693795   \n",
       "                      mean_absolute_percentage_error                                           0.251901   \n",
       "Lasso                 identifier                                                                 fold 4   \n",
       "                      y_true                          [[450000.0], [495000.0], [395000.0], [280000.0...   \n",
       "                      y_pred                          [477415.1327845714, 492463.7396309793, 305057....   \n",
       "                      explained_variance_score                                                 0.693969   \n",
       "                      mean_squared_error                                             37873203587.611603   \n",
       "                      mean_absolute_error                                                 126067.441635   \n",
       "                      root_mean_squared_error                                             194610.389208   \n",
       "                      r2_score                                                                 0.693792   \n",
       "                      mean_absolute_percentage_error                                            0.25191   \n",
       "ElasticNet            identifier                                                                 fold 4   \n",
       "                      y_true                          [[450000.0], [495000.0], [395000.0], [280000.0...   \n",
       "                      y_pred                          [469736.2740988735, 539209.0261494196, 379320....   \n",
       "                      explained_variance_score                                                 0.670126   \n",
       "                      mean_squared_error                                             40812502573.376862   \n",
       "                      mean_absolute_error                                                 124722.490988   \n",
       "                      root_mean_squared_error                                             202021.044877   \n",
       "                      r2_score                                                                 0.670027   \n",
       "                      mean_absolute_percentage_error                                           0.244499   \n",
       "DecisionTreeRegressor identifier                                                                 fold 4   \n",
       "                      y_true                          [[450000.0], [495000.0], [395000.0], [280000.0...   \n",
       "                      y_pred                          [449000.0, 555000.0, 440000.0, 247500.0, 35000...   \n",
       "                      explained_variance_score                                                 0.743783   \n",
       "                      mean_squared_error                                             31693198492.168594   \n",
       "                      mean_absolute_error                                                 101019.912377   \n",
       "                      root_mean_squared_error                                             178025.836586   \n",
       "                      r2_score                                                                 0.743758   \n",
       "                      mean_absolute_percentage_error                                           0.183729   \n",
       "\n",
       "                                                                                                 fold 5  \\\n",
       "DummyRegressor        identifier                                                                 fold 5   \n",
       "                      y_true                          [[754999.0], [588500.0], [525000.0], [525000.0...   \n",
       "                      y_pred                          [540990.9316078658, 540990.9316078658, 540990....   \n",
       "                      explained_variance_score                                                     -0.0   \n",
       "                      mean_squared_error                                            119223815039.600311   \n",
       "                      mean_absolute_error                                                 230976.128917   \n",
       "                      root_mean_squared_error                                             345288.017515   \n",
       "                      r2_score                                                                -0.000402   \n",
       "                      mean_absolute_percentage_error                                           0.540719   \n",
       "LinearRegression      identifier                                                                 fold 5   \n",
       "                      y_true                          [[754999.0], [588500.0], [525000.0], [525000.0...   \n",
       "                      y_pred                          [[853190.5779001702], [574292.3295129627], [77...   \n",
       "                      explained_variance_score                                                 0.721335   \n",
       "                      mean_squared_error                                             33210290838.463482   \n",
       "                      mean_absolute_error                                                 122514.978064   \n",
       "                      root_mean_squared_error                                             182236.908552   \n",
       "                      r2_score                                                                 0.721334   \n",
       "                      mean_absolute_percentage_error                                           0.255471   \n",
       "Ridge                 identifier                                                                 fold 5   \n",
       "                      y_true                          [[754999.0], [588500.0], [525000.0], [525000.0...   \n",
       "                      y_pred                          [[853485.3791880516], [574203.5276395974], [77...   \n",
       "                      explained_variance_score                                                 0.721308   \n",
       "                      mean_squared_error                                             33213482338.320721   \n",
       "                      mean_absolute_error                                                 122549.911589   \n",
       "                      root_mean_squared_error                                               182245.6648   \n",
       "                      r2_score                                                                 0.721307   \n",
       "                      mean_absolute_percentage_error                                           0.255562   \n",
       "Lasso                 identifier                                                                 fold 5   \n",
       "                      y_true                          [[754999.0], [588500.0], [525000.0], [525000.0...   \n",
       "                      y_pred                          [853505.6953654164, 574205.7699892861, 771097....   \n",
       "                      explained_variance_score                                                 0.721305   \n",
       "                      mean_squared_error                                             33213918899.943615   \n",
       "                      mean_absolute_error                                                 122553.005403   \n",
       "                      root_mean_squared_error                                             182246.862524   \n",
       "                      r2_score                                                                 0.721303   \n",
       "                      mean_absolute_percentage_error                                           0.255571   \n",
       "ElasticNet            identifier                                                                 fold 5   \n",
       "                      y_true                          [[754999.0], [588500.0], [525000.0], [525000.0...   \n",
       "                      y_pred                          [804965.135403886, 571735.3944930851, 737716.4...   \n",
       "                      explained_variance_score                                                 0.695926   \n",
       "                      mean_squared_error                                             36239917129.190384   \n",
       "                      mean_absolute_error                                                 120613.606068   \n",
       "                      root_mean_squared_error                                             190367.846889   \n",
       "                      r2_score                                                                 0.695912   \n",
       "                      mean_absolute_percentage_error                                           0.243172   \n",
       "DecisionTreeRegressor identifier                                                                 fold 5   \n",
       "                      y_true                          [[754999.0], [588500.0], [525000.0], [525000.0...   \n",
       "                      y_pred                          [715000.0, 545000.0, 597000.0, 530000.0, 50000...   \n",
       "                      explained_variance_score                                                 0.767617   \n",
       "                      mean_squared_error                                             27707113797.312031   \n",
       "                      mean_absolute_error                                                  96233.899075   \n",
       "                      root_mean_squared_error                                             166454.539732   \n",
       "                      r2_score                                                                 0.767511   \n",
       "                      mean_absolute_percentage_error                                           0.184638   \n",
       "\n",
       "                                                                                                mean_cv  \\\n",
       "DummyRegressor        identifier                                                                mean_cv   \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...   \n",
       "                      y_pred                          [541073.7741469058, 541073.7741469058, 541073....   \n",
       "                      explained_variance_score                                                     -0.0   \n",
       "                      mean_squared_error                                            134481092894.293533   \n",
       "                      mean_absolute_error                                                 233388.071784   \n",
       "                      root_mean_squared_error                                             366146.089233   \n",
       "                      r2_score                                                                -0.000426   \n",
       "                      mean_absolute_percentage_error                                            0.53584   \n",
       "LinearRegression      identifier                                                                mean_cv   \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...   \n",
       "                      y_pred                          [[667876.5428875468], [614410.8399372244], [73...   \n",
       "                      explained_variance_score                                                 0.697153   \n",
       "                      mean_squared_error                                             40864118653.493584   \n",
       "                      mean_absolute_error                                                 126234.837259   \n",
       "                      root_mean_squared_error                                             201612.684806   \n",
       "                      r2_score                                                                 0.696969   \n",
       "                      mean_absolute_percentage_error                                           0.256352   \n",
       "Ridge                 identifier                                                                mean_cv   \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...   \n",
       "                      y_pred                          [[667923.954879363], [614423.8176756387], [731...   \n",
       "                      explained_variance_score                                                 0.697168   \n",
       "                      mean_squared_error                                             40861586612.787613   \n",
       "                      mean_absolute_error                                                 126267.786379   \n",
       "                      root_mean_squared_error                                             201607.283482   \n",
       "                      r2_score                                                                 0.696984   \n",
       "                      mean_absolute_percentage_error                                           0.256444   \n",
       "Lasso                 identifier                                                                mean_cv   \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...   \n",
       "                      y_pred                          [667918.069528075, 614415.1560338883, 731173.9...   \n",
       "                      explained_variance_score                                                 0.697168   \n",
       "                      mean_squared_error                                             40861624970.809258   \n",
       "                      mean_absolute_error                                                 126270.604506   \n",
       "                      root_mean_squared_error                                             201607.455928   \n",
       "                      r2_score                                                                 0.696984   \n",
       "                      mean_absolute_percentage_error                                           0.256453   \n",
       "ElasticNet            identifier                                                                mean_cv   \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...   \n",
       "                      y_pred                          [656773.6005291657, 614707.2479590292, 745914....   \n",
       "                      explained_variance_score                                                 0.666834   \n",
       "                      mean_squared_error                                             45066777777.748253   \n",
       "                      mean_absolute_error                                                 124670.252885   \n",
       "                      root_mean_squared_error                                             211566.984383   \n",
       "                      r2_score                                                                 0.666627   \n",
       "                      mean_absolute_percentage_error                                           0.244851   \n",
       "DecisionTreeRegressor identifier                                                                mean_cv   \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...   \n",
       "                      y_pred                          [915000.0, 300000.0, 316000.0, 643000.0, 28200...   \n",
       "                      explained_variance_score                                                 0.758011   \n",
       "                      mean_squared_error                                             32465515100.292915   \n",
       "                      mean_absolute_error                                                  100786.00133   \n",
       "                      root_mean_squared_error                                             179973.134928   \n",
       "                      r2_score                                                                  0.75793   \n",
       "                      mean_absolute_percentage_error                                           0.187406   \n",
       "\n",
       "                                                                                                 oof_cv  \n",
       "DummyRegressor        identifier                                                                 oof_cv  \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...  \n",
       "                      y_pred                          [541073.7741469058, 541073.7741469058, 541073....  \n",
       "                      explained_variance_score                                                -0.000169  \n",
       "                      mean_squared_error                                            134481092894.293488  \n",
       "                      mean_absolute_error                                                 233388.071784  \n",
       "                      root_mean_squared_error                                              366716.63842  \n",
       "                      r2_score                                                                -0.000169  \n",
       "                      mean_absolute_percentage_error                                            0.53584  \n",
       "LinearRegression      identifier                                                                 oof_cv  \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...  \n",
       "                      y_pred                          [[667876.5428875468], [614410.8399372244], [73...  \n",
       "                      explained_variance_score                                                 0.696083  \n",
       "                      mean_squared_error                                             40864118653.493584  \n",
       "                      mean_absolute_error                                                 126234.837259  \n",
       "                      root_mean_squared_error                                             202148.753777  \n",
       "                      r2_score                                                                 0.696083  \n",
       "                      mean_absolute_percentage_error                                           0.256352  \n",
       "Ridge                 identifier                                                                 oof_cv  \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...  \n",
       "                      y_pred                          [[667923.954879363], [614423.8176756387], [731...  \n",
       "                      explained_variance_score                                                 0.696102  \n",
       "                      mean_squared_error                                             40861586612.787621  \n",
       "                      mean_absolute_error                                                 126267.786379  \n",
       "                      root_mean_squared_error                                             202142.490864  \n",
       "                      r2_score                                                                 0.696102  \n",
       "                      mean_absolute_percentage_error                                           0.256444  \n",
       "Lasso                 identifier                                                                 oof_cv  \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...  \n",
       "                      y_pred                          [667918.069528075, 614415.1560338883, 731173.9...  \n",
       "                      explained_variance_score                                                 0.696102  \n",
       "                      mean_squared_error                                             40861624970.809265  \n",
       "                      mean_absolute_error                                                 126270.604506  \n",
       "                      root_mean_squared_error                                             202142.585743  \n",
       "                      r2_score                                                                 0.696102  \n",
       "                      mean_absolute_percentage_error                                           0.256453  \n",
       "ElasticNet            identifier                                                                 oof_cv  \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...  \n",
       "                      y_pred                          [656773.6005291657, 614707.2479590292, 745914....  \n",
       "                      explained_variance_score                                                 0.664827  \n",
       "                      mean_squared_error                                             45066777777.748253  \n",
       "                      mean_absolute_error                                                 124670.252885  \n",
       "                      root_mean_squared_error                                             212289.372739  \n",
       "                      r2_score                                                                 0.664827  \n",
       "                      mean_absolute_percentage_error                                           0.244851  \n",
       "DecisionTreeRegressor identifier                                                                 oof_cv  \n",
       "                      y_true                          [[945000.0], [352500.0], [560000.0], [500000.0...  \n",
       "                      y_pred                          [915000.0, 300000.0, 316000.0, 643000.0, 28200...  \n",
       "                      explained_variance_score                                                 0.758577  \n",
       "                      mean_squared_error                                             32465515100.292915  \n",
       "                      mean_absolute_error                                                  100786.00133  \n",
       "                      root_mean_squared_error                                             180181.894485  \n",
       "                      r2_score                                                                 0.758546  \n",
       "                      mean_absolute_percentage_error                                           0.187406  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.concat({\n",
    "    name: results.to_dataframe().T\n",
    "    for name, results\n",
    "    in model_dict_with_summary.items()\n",
    "}, axis=0)\n",
    "\n",
    "results_df.columns = ['fold 1', 'fold 2', 'fold 3', 'fold 4', 'fold 5', 'mean_cv', 'oof_cv']\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3989a8-29e2-4938-a3a4-aad63375c0d3",
   "metadata": {},
   "source": [
    "### Comparison of Cross-Validated Models\n",
    "\n",
    "The point of the following comparison is to check how different models are performing across folds. More specifically, if we have 5 folds, we will have a metric score for each fold, subsequently, we can find the standard error of model's performance. We need to be aware of models that have high variance across folds in terms of the metrics performance. This can indicate that the model is highly unstable, and may be a sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e9a2d-6f30-497d-bb97-9a152ecaeb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_metrics(metric_name):\n",
    "    ls = []\n",
    "    for model_name, inner_dict in model_dict.items():\n",
    "        folds = inner_dict[\"identifier\"][:-2]\n",
    "        all_obs = []\n",
    "        for idx, obs in enumerate(inner_dict[metric_name][:-2]):\n",
    "            ls.append((model_name, folds[idx], obs))\n",
    "            all_obs.append(obs)\n",
    "        ls.append((model_name, \"SE\", np.std(all_obs, ddof=1) / len(all_obs) ** 0.5))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    summary_df = pd.DataFrame(ls, columns=[\"model\", \"fold\", metric_name])\n",
    "    # summary_df.to_csv\n",
    "    _ = sns.boxplot(x=\"model\", y=metric_name, data=summary_df[(summary_df['model'] != 'DummyClassifier') & (summary_df['fold'] != 'SE')], ax=ax)\n",
    "    \n",
    "    fig.savefig(config.spot_checking_boxplot, format='png', dpi=300)\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7fc82-808c-4c95-9056-1ac620140835",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = summarize_metrics(\"roc\")\n",
    "display(summary_df.tail(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d07e8-804b-46b7-8943-fb10ffdd8b5c",
   "metadata": {},
   "source": [
    "### Out-of-Fold Confusion Matrix\n",
    "\n",
    "We do have information on the performance of each folds, we now look at the performance of all 5 folds together. Typicall there are two ways to do it, one is to simply take the average of the score of five folds, the other is to take a look at out of folds predictions.\n",
    "\n",
    "---\n",
    "\n",
    "From the confusion matrix of the out of fold performance, Logistic Regression does seem to be a model we can explore on, although slightly lower in terms of overall AUROC score than SVC, it seems to have the quite low False Negatives amongst all. With further hyperparameter tuning and threshold optimization, we can make it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60173744-c169-4cfd-bb25-23152732592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [model for model in model_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1022fde-7449-4f68-9632-03b2f0bc5401",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(10, 10))\n",
    "\n",
    "for axes, algo in zip(ax.ravel(), model_names):\n",
    "\n",
    "    cf_mat = results_df.oof_cv[algo].confusion_matrix\n",
    "\n",
    "    #### scores\n",
    "    auc = results_df.oof_cv[algo].roc\n",
    "\n",
    "    #### annotations\n",
    "    labels = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"]\n",
    "    counts = [\"{0:0.0f}\".format(value) for value in cf_mat.flatten()]\n",
    "    percentages = [\"{0:.2%}\".format(value) for value in cf_mat.flatten() / np.sum(cf_mat)]\n",
    "\n",
    "    #### final annotations\n",
    "    label = (\n",
    "        np.array([f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(labels, counts, percentages)])\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(\n",
    "        data=cf_mat,\n",
    "        vmin=0,\n",
    "        vmax=330,\n",
    "        cmap=[\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"],\n",
    "        linewidth=2,\n",
    "        linecolor=\"white\",\n",
    "        square=True,\n",
    "        ax=axes,\n",
    "        annot=label,\n",
    "        fmt=\"\",\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": 10, \"color\": \"black\", \"weight\": \"bold\", \"alpha\": 0.8},\n",
    "        alpha=1,\n",
    "    )\n",
    "\n",
    "    axes.text(0, -0, \"{}\".format(algo), {\"size\": 12, \"color\": \"black\", \"weight\": \"bold\"})\n",
    "\n",
    "    axes.scatter(1, 1, s=3500, c=\"white\")\n",
    "    axes.text(\n",
    "        0.72,\n",
    "        1.0,\n",
    "        \"AUC: {}\".format(round(auc, 3)),\n",
    "        {\"size\": 10, \"color\": \"black\", \"weight\": \"bold\"},\n",
    "    )\n",
    "\n",
    "    ## ticks and labels\n",
    "    axes.set_xticklabels(\"\")\n",
    "    axes.set_yticklabels(\"\")\n",
    "\n",
    "\n",
    "## titles and text\n",
    "fig.text(0, 1.05, \"Out Of Fold Confusion Matrix\", {\"size\": 22, \"weight\": \"bold\"}, alpha=1)\n",
    "fig.text(\n",
    "    0,\n",
    "    1,\n",
    "    \"\"\"This Visualization show the results of various classifiers and there respective\n",
    "results.\"\"\",\n",
    "    {\"size\": 14, \"weight\": \"normal\"},\n",
    "    alpha=0.98,\n",
    ")\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=2.5, w_pad=2.5, h_pad=2.5)\n",
    "fig.savefig(config.oof_confusion_matrix, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f521fd-054d-4b39-8971-8af2aa1c8dff",
   "metadata": {},
   "source": [
    "### Hypothesis Testing Across Models\n",
    "\n",
    "I am slightly shocked at the performance of plain LogisticRegression, I decide to use an idea from [Hypothesis Testing Across Models](http://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/) to check if the difference is really by chance or not. Note that I will be modifying his code as his code does not split using StratifiedKFold.\n",
    "\n",
    "---\n",
    "\n",
    "The basic idea is to test if two model's difference in scores (in this case roc), is statistically significant or not. However, we note that this method may violate an assumption of Student's t test.\n",
    "\n",
    "---\n",
    "\n",
    "- Null Hypothesis $H_0$: The difference in the performance score of two classifiers is Statistically Significant.\n",
    "- Alternate Hypothesis $H_1$: The difference in the performance score of two classifiers is **not** Statistically Significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25f5a2-9921-4371-a93a-0c5466eef811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_ttest_skfold_cv(\n",
    "    estimator1, estimator2, X, y, cv=10, scoring=None, shuffle=False, random_seed=None\n",
    "):\n",
    "    \"\"\"Modified from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/ttest.py to accomodate StratifiedKFold\"\"\"\n",
    "\n",
    "    if not shuffle:\n",
    "        skf = model_selection.StratifiedKFold(n_splits=cv, shuffle=shuffle)\n",
    "    else:\n",
    "        skf = model_selection.StratifiedKFold(\n",
    "            n_splits=cv, random_state=random_seed, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "    if scoring is None:\n",
    "        if estimator1._estimator_type == \"classifier\":\n",
    "            scoring = \"accuracy\"\n",
    "        elif estimator1._estimator_type == \"regressor\":\n",
    "            scoring = \"r2\"\n",
    "        else:\n",
    "            raise AttributeError(\"Estimator must \" \"be a Classifier or Regressor.\")\n",
    "    if isinstance(scoring, str):\n",
    "        scorer = metrics.get_scorer(scoring)\n",
    "    else:\n",
    "        scorer = scoring\n",
    "\n",
    "    score_diff = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X=X, y=y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        estimator1.fit(X_train, y_train)\n",
    "        estimator2.fit(X_train, y_train)\n",
    "\n",
    "        est1_score = scorer(estimator1, X_test, y_test)\n",
    "        est2_score = scorer(estimator2, X_test, y_test)\n",
    "        score_diff.append(est1_score - est2_score)\n",
    "\n",
    "    avg_diff = np.mean(score_diff)\n",
    "\n",
    "    numerator = avg_diff * np.sqrt(cv)\n",
    "    denominator = np.sqrt(sum([(diff - avg_diff) ** 2 for diff in score_diff]) / (cv - 1))\n",
    "    t_stat = numerator / denominator\n",
    "\n",
    "    pvalue = stats.t.sf(np.abs(t_stat), cv - 1) * 2.0\n",
    "    return float(t_stat), float(pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e1daa-1175-42d3-b398-29e45b066e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if difference between algorithms is real\n",
    "X_tmp = X_y_train[predictor_cols].values\n",
    "y_tmp = X_y_train['diagnosis'].values\n",
    "\n",
    "t, p = paired_ttest_skfold_cv(estimator1=classifiers[1], estimator2=classifiers[-1],shuffle=True,cv=5, X=X_tmp, y=y_tmp, scoring='roc_auc', random_seed=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d7844-d8da-427d-a46f-9d0398e2bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('P-value: %.3f, t-Statistic: %.3f' % (p, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b67b3-2a6f-4aea-a777-5981c0968464",
   "metadata": {},
   "source": [
    "Since P value is quite high, and more the basic threshold of 0.05 or 0.1, we fail to reject the null hypothesis, and say that there is no significant difference between these two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9dcf7-9de6-4508-9f83-d97b7aa724f0",
   "metadata": {},
   "source": [
    "## Model Selection: Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06510a3-7aaf-4b45-982f-ce6703c3e602",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <b>Hyperparameter Tuning:</b>\n",
    "    <li> We have done a quick spot checking on algorithms and realized that <code>LogisticRegression</code> is doing well for this task. For this purpose, I will just perform hyperparameter tuning on this single algorithm. However, in practice and if resources are allowed, I will also tune other models such as <code>RandomForest()</code>, or gradient boosting algorithms such as <code>XGBoost</code>, as I believe they will perform no worse than our Logistic Regression model given the right hyperparameters.\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <b>Grid Search:</b>\n",
    "    <li> We will use an old-fashioned way to search for hyperparameters, which is brute force method. The time complexity of Grid Search is high and if you have many hyperparameters to tune, I recommend trying out <b>Random Grid Search</b> or libraries like <b>Optuna</b> that uses Bayesian Optimization.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9666a82-1983-45ba-904f-fd292785265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_finetuning_pipeline(model):\n",
    "    \"\"\"Make a Pipeline for Training.\n",
    "\n",
    "    Args:\n",
    "        model ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    \n",
    "    steps = list()\n",
    "    # standardization\n",
    "    steps.append(('standardize', preprocessing.StandardScaler()))\n",
    "    # reduce VIF\n",
    "    steps.append(('remove_multicollinearity', ReduceVIF(thresh=10)))\n",
    "    # the model\n",
    "    steps.append(('model', model))\n",
    "    # create pipeline\n",
    "    _pipeline = pipeline.Pipeline(steps=steps)\n",
    "    return _pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4a664-a7fb-4d0b-9778-a5380f6a0dd5",
   "metadata": {},
   "source": [
    "Reconstruct our pipeline but now only taking in `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323efdc2-27d4-4d07-a5f1-89e181e08d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_logistic = make_finetuning_pipeline(\n",
    "    linear_model.LogisticRegression(\n",
    "        solver=\"saga\", random_state=config.seed, max_iter=10000, n_jobs=None, fit_intercept=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335144e9-ae4f-44c5-a6e0-dd6eac1a28c6",
   "metadata": {},
   "source": [
    "Define our search space for the hyperparameters:\n",
    "\n",
    "```python\n",
    "param_grid = {model__penalty=[\"l1\", \"l2\"],\n",
    "              model__C=np.logspace(-4, 4, 10)}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd695e89-a26e-4014-898a-c918f53eb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    model__penalty=[\"l1\", \"l2\"],\n",
    "    model__C=np.logspace(-4, 4, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de67f0-d18c-4962-b040-12abfd64ed22",
   "metadata": {},
   "source": [
    "Run our hyperparameter search with cross-validation. For example, our `param_grid` has $2 \\times 10 = 20$ combinations, and our cross validation has 5 folds, then there will be a total of 100 fits.\n",
    "\n",
    "---\n",
    "\n",
    "Below details the pseudo code of what happens under the hood:\n",
    "\n",
    "- Define $G$ as the set of combination of hyperparamters. Define number of splits to be $K$.\n",
    "- For each set of hyperparameter $z \\in Z$:\n",
    "    - for fold $j$ in K:\n",
    "        - Set $F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}$\n",
    "        - Set $F_{\\text{val}} = F_{j}$ as the validation set\n",
    "        - Perform Standard Scaling on $F_{\\text{train}}$ and find the mean and std\n",
    "        - Perform VIF recursively on $F_{\\text{train}}$ and find the selected features\n",
    "        - Transform $F_{\\text{val}}$ using the mean and std found using $F_{\\text{train}}$\n",
    "        - Transform $F_{\\text{val}}$ to have only the selected features from $F_{\\text{train}}$\n",
    "        - Train and fit on $F_{\\text{train}}$ \n",
    "    - Evaluate the fitted parameters on $F_{\\text{val}}$ to obtain $\\mathcal{M}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7edc0e-3830-45c9-8a79-eaf26290d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = model_selection.GridSearchCV(pipeline_logistic, param_grid=param_grid, cv=5, refit=True, verbose=3, scoring = \"roc_auc\")\n",
    "_ = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21239e-1404-44c8-9812-3e30d433df56",
   "metadata": {},
   "source": [
    "We can save our results in a dataframe, we will also look at the top performing hyperparameter by querying the below:\n",
    "\n",
    "```python\n",
    "grid_cv_df = pd.DataFrame(grid.cv_results_)\n",
    "grid_cv_df.loc[grid_cv_df['rank_test_score']==1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7a9ed-5d15-45c8-969c-1bdd33b125b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv_df = pd.DataFrame(grid.cv_results_)\n",
    "best_cv = grid_cv_df.loc[grid_cv_df['rank_test_score']==1]\n",
    "display(best_cv)\n",
    "\n",
    "best_hyperparams = grid.best_params_\n",
    "print(f\"Best Hyperparameters found is {best_hyperparams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde751c-61f0-4c77-a510-e14a3f2f8d57",
   "metadata": {},
   "source": [
    "Our best performing set of hyperparameters `{'model__C': 0.3593813663804626, 'model__penalty': 'l2'}` gives rise to a mean cross validation score of $0.988739$, which is higher than the model with default hyperparameter scoring, $0.987136$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e7164-5449-426b-a3fb-8699255183cb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"warning\">\n",
    "    <b>Room for Improvement:</b> Apart from the other methods to search for the optimal hyperparameters, we can also include preprocessing step as a tunable hyperparameter. More specifically, in our <code>ReduceVIF()</code> step, we hard coded two manual criterion in which the algorithm will stop; if the threshold reaches 10, or if the number of features removed hit 20; we can include them in the search space.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a626cc-c72e-4666-8fd8-5120f3554e6d",
   "metadata": {},
   "source": [
    "## Retrain on the whole training set\n",
    "\n",
    "A common practice after the hyperparameter tuning phase is to retrain the model on the whole dataset $X_{\\text{train}}$ where we will get the estimator's coefficients obtained from the retraining. This is actually already done as the scikit-learn's `GridSearchCV` has a parameter `refit`; if we select it to be true, then after the model selection process is done (i.e. getting the best hyperparameters after cross validation with grid search), the grid search object will retrain on the whole $X_{\\text{train}}$ with the best hyperparameters internally, and return us back an object in which we can call `predict` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5486fa-c992-4e24-b9e0-e62f25c718c6",
   "metadata": {},
   "source": [
    "### Retrain using optimal hyperparameters\n",
    "\n",
    "However, to be extra careful, we can retrain manually using the best hyperparameters and check if scikit-learn is true to its documentation. We will just reconstruct the pipeline using the grid's best hyper parameters. We will then test if the retrained model's coefficients coincide with the grid's best estimator's coefficients. If there difference is 0, this means they are trained under the same circumstances and we can be sure that the refit parameter is behaving true to its words.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "grid_best_hyperparams = grid.best_params_\n",
    "print(grid_best_hyperparams) ->\n",
    "{'model__C': 0.3593813663804626,\n",
    " 'model__penalty': 'l2'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df6f72-e15f-4f28-83e2-e1ad08dbf1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_pipeline = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"standardize\", preprocessing.StandardScaler()),\n",
    "        ('remove_multicollinearity', ReduceVIF(thresh=10)),\n",
    "\n",
    "        (\n",
    "            \"model\",\n",
    "            linear_model.LogisticRegression(\n",
    "                C=0.3593813663804626, max_iter=10000, random_state=1992, solver=\"saga\", penalty=\"l1\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "_ = retrain_pipeline.fit(X_train, y_train)\n",
    "coef_diff = retrain_pipeline['model'].coef_ - grid.best_estimator_['model'].coef_\n",
    "\n",
    "print(\"...\")\n",
    "assert np.all(coef_diff == 0) == True\n",
    "print(\"Retraining Assertion Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd0048-1980-4447-8811-5af9f31a1204",
   "metadata": {},
   "source": [
    "## Interpretation of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb3a31-bf1f-4205-b549-fa2d284563ac",
   "metadata": {},
   "source": [
    "### Interpretation of Coefficients\n",
    "\n",
    "As shown in the figure below, all else being equal, for every square unit increase in mean cell area, the odds of the tumor being malignant increases by a factor of $e^{1.43} = 4.19$. The variation (standard error) of the characteristics of cells also are deemed important by the model, for example, area se played an important role in determining whether a cell is malignant; intuitively, if some cells are noticably larger than the rest, then it is also a good indicator of malignancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b9f75-dd7e-46e6-b5d5-94ddb898981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_by_vif_index = grid.best_estimator_['remove_multicollinearity'].column_indices_kept_ \n",
    "selected_feature_names = np.asarray(predictor_cols)[selected_features_by_vif_index]\n",
    "\n",
    "selected_features_coefficients = grid.best_estimator_['model'].coef_.flatten()\n",
    "\n",
    "# assertion\n",
    "#assert grid.best_estimator_['remove_multicollinearity'].feature_names_ == retrain_pipeline['remove_multicollinearity'].feature_names_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "# .abs()\n",
    "_ = pd.Series(selected_features_coefficients, index=selected_feature_names).sort_values().plot(ax=ax, kind='barh')\n",
    "fig.savefig(config.feature_importance, format=\"png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580baa4c-7151-4fa1-910f-5c9b65acb7b5",
   "metadata": {},
   "source": [
    "### Interpretation of Metric Scores on Train Set\n",
    "\n",
    "We are also interested in choosing an optimal threshold for the model such that it gives the lowest recall, or False Negatives. We note that the default threshold when calling `predict()` from a model is $0.5$. In this section, we will explore one way to get the best tradeoff we can when choosing a high recall, while maintaining a reasonable score for precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b2423-b16f-4b38-b2db-1e51073a8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_train_test_set(\n",
    "    estimator: Callable, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.DataFrame, np.ndarray]\n",
    ") -> Dict[str, Union[float, np.ndarray]]:\n",
    "    \"\"\"This function takes in X and y and returns a dictionary of scores.\n",
    "\n",
    "    Args:\n",
    "        estimator (Callable): [description]\n",
    "        X (Union[pd.DataFrame, np.ndarray]): [description]\n",
    "        y (Union[pd.DataFrame, np.ndarray]): [description]\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Union[float, np.ndarray]]: [description]\n",
    "    \"\"\"\n",
    "\n",
    "    test_results = {}\n",
    "\n",
    "    y_pred = estimator.predict(X)\n",
    "    # This is the probability array of class 1 (malignant)\n",
    "    y_prob = estimator.predict_proba(X)[:, 1]\n",
    "\n",
    "    test_brier = metrics.brier_score_loss(y, y_prob)\n",
    "    test_roc = metrics.roc_auc_score(y, y_prob)\n",
    "\n",
    "    test_results[\"brier\"] = test_brier\n",
    "    test_results[\"roc\"] = test_roc\n",
    "    test_results[\"y\"] = np.asarray(y).flatten()\n",
    "    test_results[\"y_pred\"] = y_pred.flatten()\n",
    "    test_results[\"y_prob\"] = y_prob.flatten()\n",
    "\n",
    "    return test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242c5b1-2769-41b4-9941-bc612ff86c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    \"\"\"\n",
    "    Modified from:\n",
    "    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.89 \n",
    "    and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(config.precision_recall_threshold_plot, format=\"png\", dpi=300)\n",
    "    \n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    \"\"\"\n",
    "    The ROC curve, modified from \n",
    "    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91\n",
    "    and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('ROC Curve')\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.005, 1, 0, 1.005])\n",
    "    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(config.roc_plot, format=\"png\", dpi=300)\n",
    "    \n",
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    Will only work for binary classification problems.\n",
    "    and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc1622-d310-4701-92eb-3bb42c659934",
   "metadata": {},
   "source": [
    "The plots below show the tradeoffs between precision and recall, recall and false positive rate. The confusion matrix on the train set tells us that there is still more false negatives than false positives. We can choose a particular threshold in order to minimize false negatives, at some expense of false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c446c2-3c93-430b-b581-77effe6b94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = evaluate_train_test_set(grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68efcf73-11bb-4cad-9266-7eb3065f8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# CM\n",
    "cm_train = metrics.confusion_matrix(train_results['y'], train_results['y_pred'])\n",
    "\n",
    "#### scores\n",
    "auc = metrics.roc_auc_score(train_results['y'], train_results['y_prob'])\n",
    "\n",
    "#### annotations\n",
    "labels = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"]\n",
    "counts = [\"{0:0.0f}\".format(value) for value in cm_train.flatten()]\n",
    "percentages = [\"{0:.2%}\".format(value) for value in cm_train.flatten() / np.sum(cm_train)]\n",
    "\n",
    "#### final annotations\n",
    "label = (\n",
    "    np.array([f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(labels, counts, percentages)])\n",
    ").reshape(2, 2)\n",
    "\n",
    "# heatmap\n",
    "sns.heatmap(\n",
    "    data=cm_train,\n",
    "    vmin=0,\n",
    "    vmax=330,\n",
    "    cmap=[\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"],\n",
    "    linewidth=2,\n",
    "    linecolor=\"white\",\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    annot=label,\n",
    "    fmt=\"\",\n",
    "    cbar=False,\n",
    "    annot_kws={\"size\": 10, \"color\": \"black\", \"weight\": \"bold\", \"alpha\": 0.8},\n",
    "    alpha=1,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ax.scatter(1, 1, s=3500, c=\"white\")\n",
    "ax.text(\n",
    "    0.72,\n",
    "    1.0,\n",
    "    \"AUC: {}\".format(round(auc, 3)),\n",
    "    {\"size\": 10, \"color\": \"black\", \"weight\": \"bold\"},\n",
    ")\n",
    "\n",
    "## ticks and labels\n",
    "ax.set_xticklabels(\"\")\n",
    "ax.set_yticklabels(\"\")\n",
    "\n",
    "\n",
    "## titles and text\n",
    "fig.text(0, 1.05, \"Train Set Confusion Matrix\", {\"size\": 22, \"weight\": \"bold\"}, alpha=1)\n",
    "fig.text(\n",
    "    0,\n",
    "    1,\n",
    "    \"\"\"Training Set Confusion Matrix.\"\"\",\n",
    "    {\"size\": 12, \"weight\": \"normal\"},\n",
    "    alpha=0.98,\n",
    ")\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=2.5, w_pad=2.5, h_pad=2.5)\n",
    "fig.savefig(config.final_train_confusion_matrix, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575c915-5e4c-4297-a6de-e28d28feb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the precision recall curve\n",
    "precision, recall, pr_thresholds = metrics.precision_recall_curve(train_results['y'], train_results['y_prob'])\n",
    "fpr, tpr, roc_thresholds = metrics.roc_curve(train_results['y'], train_results['y_prob'], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa77187-c91d-4ddf-a29f-c956c6b456d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same p, r, thresholds that were previously calculated\n",
    "plot_precision_recall_vs_threshold(precision, recall, pr_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a42e3-b58f-4f22-9dde-9792fcc4d366",
   "metadata": {},
   "source": [
    "Based on the tradeoff plot above, a good threshold can be set at $t = 0.35$, let us see how it performs with this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c2a4c6-7462-4185-ba8c-e1eefe95dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_adj = adjusted_classes(train_results[\"y_prob\"], t=0.35)\n",
    "\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        metrics.confusion_matrix(train_results[\"y\"], y_pred_adj),\n",
    "        columns=[\"pred_neg\", \"pred_pos\"],\n",
    "        index=[\"neg\", \"pos\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893fe9c6-b9dc-4b32-b9f5-c99f3f0e5d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_true=train_results[\"y\"], y_pred=y_pred_adj))\n",
    "train_brier = train_results['brier']\n",
    "print(f\"train brier: {train_brier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24a144-e729-4078-af7a-8f96adab6037",
   "metadata": {},
   "source": [
    "The False Negatives reduced from 15 to 9, at the expense of increase False Positives from 6 to 14. We should take comfort that less patients are falsely classified as benign when in fact they need treatment. This is a tradeoff that we have to choose. The ROC curve below also paints a similar story, in order for you to have high recall, one must accept that there will more False Positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b4449-b47e-41cd-a0ce-90f5a0b67054",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr, tpr, 'recall_optimized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d7767-f4fe-4f8d-8b35-fe69014a2be1",
   "metadata": {},
   "source": [
    "# Evaluation on Test Set\n",
    "\n",
    "Ultimately, we are interested in finding the estimate of the generalization error of the model, and thus it is time for us to evaluate our model's performance on the \"unseen\" test set $X_{\\text{test}}$ to get a good gauge on how well the model generalizes to unseen data. Take note that now everything has settled, we will use the exact hyperparameters to predict on test set, with the pre-defined threshold of 0.35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f62ee-7c07-4f10-9254-88b52ac312e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluate_train_test_set(grid, X_test, y_test)\n",
    "y_test_pred_adj = adjusted_classes(test_results['y_prob'], t=0.35)\n",
    "\n",
    "print(pd.DataFrame(metrics.confusion_matrix(test_results['y'], y_test_pred_adj),\n",
    "                   columns=['pred_neg', 'pred_pos'], \n",
    "                   index=['neg', 'pos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8e478-a2c2-4fd5-9242-da0cf209a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_roc = test_results['roc']\n",
    "test_brier = test_results['brier']\n",
    "print(test_roc)\n",
    "print(test_brier)\n",
    "print(metrics.classification_report(y_true=test_results[\"y\"], y_pred=y_test_pred_adj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5107c2-087f-481f-b157-b3376161b0c9",
   "metadata": {},
   "source": [
    "Using the same threshold we used on training set, we see that the False Negative is quite low. The overall ROC score is 0.9828, and the corresponding Brier Score is 0.04136, both seem reasonably well performing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd057235-501a-4ed4-be3a-f1664ef3ead9",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e82274-a0a8-49dd-a6fb-70bd013fefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        grid.best_estimator_['model'], X_train.values, y_train.values, X_test.values, y_test.values, \n",
    "        loss='0-1_loss',\n",
    "        random_seed=123)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1ca9b-6883-4c7a-8c83-b967c71607e5",
   "metadata": {},
   "source": [
    "We use the `mlxtend` library to estimate the Bias-Variance Tradeoff in our Logistic Regression model. The core idea behind this function is to use bagging and repeatedly sample from our training set so as to simulate that we are actually drawing samples from the \"true\" population over a distribution $\\mathcal{P}$. \n",
    "\n",
    "---\n",
    "\n",
    "As expected, Logistic Regression being a linear model, its simplicity contributes to its high bias and low variance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
