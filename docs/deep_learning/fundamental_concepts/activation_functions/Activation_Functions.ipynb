{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPLNlQbLcvng"
   },
   "source": [
    "# Regularization\n",
    "\n",
    "https://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/activation-regularization-for-reducing-generalization-error-in-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzBYUaP3dRMw"
   },
   "source": [
    "> CS231N: In the diagram above, we can see that Neural Networks with more neurons can express more complicated functions. However, this is both a blessing (since we can learn to classify more complicated data) and a curse (since it is easier to overfit the training data). Overfitting occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship. For example, the model with 20 hidden neurons fits all the training data but at the cost of segmenting the space into many disjoint red and green decision regions. The model with 3 hidden neurons only has the representational power to classify the data in broad strokes. It models the data as two blobs and interprets the few red points inside the green cluster as outliers (noise). In practice, this could lead to better generalization on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjH82QM0Pwn8"
   },
   "source": [
    "# Softmax\n",
    "\n",
    "The softmax function takes as input a vector $z$ of $K$ real numbers, and normalizes it into a probability distribution consisting of $K$ probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval $[0, 1]$ and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "One key difference is softmax takes in a vector of inputs, while sigmoid can only take in one real value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcd7ALsQQD28"
   },
   "source": [
    "# Sigmoid\n",
    "\n",
    "The sigmoid function takes as input a real value and output one real value as well. In Binary classification case, with class 0 and 1, we only need one output neuron (positive class neuron), and when applied sigmoid will get a number between 0 and 1, say $p^{+}$, then $p^{-} = 1 - p^{+}$.\n",
    "\n",
    "---\n",
    "\n",
    "However the catch is that sigmoid in Binarcy Classification setting works just like softmax, but not when in multi-label!\n",
    "\n",
    "---\n",
    "\n",
    "One key difference is softmax takes in a vector of inputs, while sigmoid can only take in one real value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIcWaOhS-gCu"
   },
   "source": [
    "# Softmax vs Sigmoid\n",
    "\n",
    "> [Sigmoid vs Softmax](https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier) I've noticed people often get directed to this question when searching whether to use sigmoid vs softmax in neural networks. If you are one of those people building a neural network classifier, here is how to decide whether to apply sigmoid or softmax to the raw output values from your network:\n",
    "\n",
    "- If you have a multi-label classification problem = there is more than one \"right answer\" = the outputs are NOT mutually exclusive, then use a sigmoid function on each raw output independently. The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them. Example: classifying diseases in a chest x-ray image. The image might contain pneumonia, emphysema, and/or cancer, or none of those findings.\n",
    "\n",
    "---\n",
    "\n",
    "- If you have a multi-class classification problem = there is only one \"right answer\" = the outputs are mutually exclusive, then use a softmax function. The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes. Example: classifying images from the MNIST data set of handwritten digits. A single picture of a digit has only one true identity - the picture cannot be a 7 and an 8 at the same time.\n",
    "\n",
    "---\n",
    "\n",
    "[More reading](https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jm3l-75jrNaF"
   },
   "source": [
    "# ReLU\n",
    "\n",
    "## Properties\n",
    "\n",
    "https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec\n",
    "\n",
    "$g(z) = \\max(0,z)$\n",
    "\n",
    "- Differentiable over all points except $z = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfuC5epS30ZW"
   },
   "source": [
    "# Swish\n",
    "\n",
    "https://stats.stackexchange.com/questions/544739/why-does-being-bounded-below-in-swish-reduces-overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVosobZ_48e7"
   },
   "source": [
    "Indeed relu is also bounded below, they didn't claim otherwise. The difference is, that swish allows small negative values for small negative inputs, which according to them, increases expressivity and improve gradient flow. \n",
    "\n",
    "The reason behind improving generalization is that, as in regularization, small, approaching zero, weights improve generalization as the function become more smooth and it reduces the effect of fitting the noise. \n",
    "They claim that by bounding large negative vales in the activation function, the effect is that the network \"forgets\" large negative inputs and thus helping the weights to approach to zero. \n",
    "See the image they added, large negative values, which are common before training are forgotten and after training the negative scale is much smaller.  \n",
    "\n",
    "\n",
    "[![bound effect][1]][1]\n",
    "\n",
    "\n",
    "There is a tradeoff between bounded which improve generaliztion and unbounded that avoids saturation of gradients, and help the network to stay in the linear regime. \n",
    "\n",
    "\n",
    "  [1]: https://i.stack.imgur.com/0b7lK.png"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Activation Functions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
