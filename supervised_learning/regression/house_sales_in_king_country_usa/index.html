
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="canonical" href="https://ghnreigns.github.io/reighns-ml-journey/supervised_learning/regression/house_sales_in_king_country_usa/">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.5">
    
    
      
        <title>House sales in king country usa - Hongnan G. Machine Learning Projects</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.cdeb8541.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.3f5d1f46.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../../../css/extra.css">
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    <script>function __prefix(e){return new URL("../../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#dependencies-and-configuration" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Hongnan G. Machine Learning Projects" class="md-header__button md-logo" aria-label="Hongnan G. Machine Learning Projects" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Hongnan G. Machine Learning Projects
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              House sales in king country usa
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Hongnan G. Machine Learning Projects" class="md-nav__button md-logo" aria-label="Hongnan G. Machine Learning Projects" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Hongnan G. Machine Learning Projects
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Getting Started
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Getting Started" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Getting Started
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting_started/" class="md-nav__link">
        Workflow
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../git.md" class="md-nav__link">
        git
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Misc
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Misc" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Misc
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../misc/gcp/" class="md-nav__link">
        GCP
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Machine Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Machine Learning" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Machine Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_1" type="checkbox" id="__nav_5_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_1">
          Supervised Learning Theory
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Supervised Learning Theory" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_1">
          <span class="md-nav__icon md-icon"></span>
          Supervised Learning Theory
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../about/" class="md-nav__link">
        Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_1_2" type="checkbox" id="__nav_5_1_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_1_2">
          Classification
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Classification" data-md-level="3">
        <label class="md-nav__title" for="__nav_5_1_2">
          <span class="md-nav__icon md-icon"></span>
          Classification
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_1_2_1" type="checkbox" id="__nav_5_1_2_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_1_2_1">
          Projects
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Projects" data-md-level="4">
        <label class="md-nav__title" for="__nav_5_1_2_1">
          <span class="md-nav__icon md-icon"></span>
          Projects
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_1_2_1_1" type="checkbox" id="__nav_5_1_2_1_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_1_2_1_1">
          Breast Cancer Wisconsin
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Breast Cancer Wisconsin" data-md-level="5">
        <label class="md-nav__title" for="__nav_5_1_2_1_1">
          <span class="md-nav__icon md-icon"></span>
          Breast Cancer Wisconsin
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../classification/breast_cancer_wisconsin/Stage%200%20-%20Introduction%20and%20Problem%20Statement/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../classification/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/" class="md-nav__link">
        Preliminary Data Inspection and Cleaning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/" class="md-nav__link">
        EDA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../classification/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/" class="md-nav__link">
        Feature Engineering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../classification/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/" class="md-nav__link">
        Modelling (Metrics)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../classification/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/" class="md-nav__link">
        Modelling (Cross-Validation Schema)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/" class="md-nav__link">
        Modelling (Preprocessing and Spot Checking)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/" class="md-nav__link">
        Modelling (Model Selection)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../classification/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/" class="md-nav__link">
        Modelling (Model Evaluation)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          Deep Learning with PyTorch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Deep Learning with PyTorch" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Deep Learning with PyTorch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_1" type="checkbox" id="__nav_6_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_1">
          Computer Vision
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Computer Vision" data-md-level="2">
        <label class="md-nav__title" for="__nav_6_1">
          <span class="md-nav__icon md-icon"></span>
          Computer Vision
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6_1_1" type="checkbox" id="__nav_6_1_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6_1_1">
          General
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="General" data-md-level="3">
        <label class="md-nav__title" for="__nav_6_1_1">
          <span class="md-nav__icon md-icon"></span>
          General
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../deep_learning/computer_vision/general/image_normalization/Image_Normalization_and_Standardization.md" class="md-nav__link">
        Image Normalization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/" class="md-nav__link">
        Visualizing Convolutional Filters
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../deep_learning/computer_vision/general/freeze_layers/freezing_layers/" class="md-nav__link">
        How to freeze Batch Norm Layers
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <p><a id="top"></a></p>
<p><a id = '1.0'></a></p>
<h1 style = "font-family: garamond; font-size: 40px; font-style: normal;background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold" >Quick Navigation</h1>

<ul>
<li><a href="#1">Dependencies and Configuration</a></li>
<li><a href="#2">Stage 4: Modelling</a><ul>
<li><a href="#31">How EDA helped us?</a></li>
<li><a href="#31">Modelling</a><ul>
<li><a href="#31">Spot Checking Algorithms</a><ul>
<li><a href="#31">Make Basic Pipeline (Say No to Data Leakage!)</a></li>
<li><a href="#31">Define Metrics</a></li>
<li><a href="#31">Comparison of Cross-Validated Models</a></li>
<li><a href="#31">Out-of-Fold Confusion Matrix</a></li>
<li><a href="#31">Hypothesis Testing Across Models</a></li>
</ul>
</li>
<li><a href="#31">Model Selection: Hyperparameter Tuning with GridSearchCV</a></li>
<li><a href="#31">Retrain on the whole training set</a><ul>
<li><a href="#31">Retrain using Optimal Hyperparameters</a></li>
</ul>
</li>
<li><a href="#31">Interpretation of Results</a><ul>
<li><a href="#31">Interpretation of Coefficients</a></li>
<li><a href="#31">Interpretation of Metric Scores on Train Set</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#31">Evaluation on Test Set</a></li>
<li><a href="#31">Bias-Variance Tradeoff</a></li>
</ul>
</li>
</ul>
<h1 id="dependencies-and-configuration">Dependencies and Configuration</h1>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Any</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">mlxtend</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">mlxtend.evaluate</span> <span class="kn">import</span> <span class="n">paired_ttest_5x2cv</span><span class="p">,</span> <span class="n">bias_variance_decomp</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">decomposition</span><span class="p">,</span> <span class="n">dummy</span><span class="p">,</span> <span class="n">ensemble</span><span class="p">,</span> <span class="n">feature_selection</span><span class="p">,</span>
                     <span class="n">linear_model</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">model_selection</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span>
                     <span class="n">pipeline</span><span class="p">,</span> <span class="n">preprocessing</span><span class="p">,</span> <span class="n">svm</span><span class="p">,</span> <span class="n">tree</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">statsmodels.regression.linear_model</span> <span class="kn">import</span> <span class="n">OLS</span>
<span class="c1">#from statsmodels.stats.outliers_influence import variance_inflation_factor</span>

<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">import</span> <span class="nn">logging</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">config</span><span class="p">:</span>
    <span class="n">raw_data</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/regression/</span><span class="si">%20ho</span><span class="s2">use-sales-in-king-country-usa/data/raw/kc_house_data.csv&quot;</span>
    <span class="n">train_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.8</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1992</span>
    <span class="n">num_folds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">cv_schema</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;KFold&quot;</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">set_seeds</span><span class="p">(</span><span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1234</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Set seeds for reproducibility.&quot;&quot;&quot;</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">init_logger</span><span class="p">(</span><span class="n">log_file</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;info.log&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize logger.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">stream_handler</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">StreamHandler</span><span class="p">()</span>
    <span class="n">stream_handler</span><span class="o">.</span><span class="n">setFormatter</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">Formatter</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%(asctime)s</span><span class="s2"> - </span><span class="si">%(message)s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">datefmt</span><span class="o">=</span> <span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">,%H:%M:%S&quot;</span><span class="p">))</span>
    <span class="n">file_handler</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">FileHandler</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">log_file</span><span class="p">)</span>
    <span class="n">file_handler</span><span class="o">.</span><span class="n">setFormatter</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">Formatter</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%(asctime)s</span><span class="s2"> - </span><span class="si">%(message)s</span><span class="s2">&quot;</span><span class="p">,</span>  <span class="n">datefmt</span><span class="o">=</span> <span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">,%H:%M:%S&quot;</span><span class="p">))</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">addHandler</span><span class="p">(</span><span class="n">stream_handler</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">addHandler</span><span class="p">(</span><span class="n">file_handler</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logger</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># set seeding for reproducibility</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">set_seeds</span><span class="p">(</span><span class="n">seed</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># set logger</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">init_logger</span><span class="p">()</span>

<span class="c1"># read data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">raw_data</span><span class="p">)</span>
</code></pre></div>
<h1 id="how-eda-helped-us">How EDA helped us?</h1>
<div class="alert alert-success" role="alert">
    <b>Insights derived from EDA:</b> 
    <li> To fill in.
</div>

<h1 id="cross-validation-strategy">Cross-Validation Strategy</h1>
<div class="alert alert-block alert-danger">
<b>Generalization:</b>     
    <blockquote cite="https://www.huxley.net/bnw/four.html">
        <p>Ultimately, we are interested in the Generalization Error made by the model, that is, how well the model perform on <b>unseen data</b> that is not taken from our sample set $\mathcal{D}$. In general, we use <b>validation set</b> for <b>Model Selection</b> and the <b>test set</b> for <b>an estimate of generalization error</b> on new data.
            <br> <b>- Refactored from Elements of Statistical Learning, Chapter 7.2</b></p>
    </blockquote>
</div>

<hr />
<div class="alert alert-success" role="alert">
    <b>Step 1: Train-Test-Split:</b> Since this dataset is relatively small, we will not use the <b>train-validation-test</b> split and only split into train and test in a ratio of 9:1, whereby the split is stratified on our target, using <code>stratify=y</code> parameter in <code>train_test_split()</code> to ensure that our target has equal representation in both train and test. We note that this is a relatively small dataset and in practice, we need a large sample size to get a reliable/stable split, it is also recommended to retrain the whole dataset (without the "unseen" test set) after we have done the model selection process (eg. finding best hyperparameters). 
</div>

<hr />
<div class="alert alert-success" role="alert">
    <b>Step 2: Resampling Stategy:</b> Note that we will be performing <code>StratifiedKFold</code> as our resampling strategy. After our split in Step 1, we have a training set $X_{\text{train}}$, we will then perform our resampling strategy on this $X_{\text{train}}$. We will choose our choice of $K = 5$. The choice of $K$ is somewhat arbitrary, and is derived <a href="https://stats.stackexchange.com/questions/61783/bias-and-variance-in-leave-one-out-vs-k-fold-cross-validation">empirically</a>. 
</div>

<hr />
<p>To recap, we have the following:</p>
<ul>
<li><strong>Training Set (<span class="arithmatex">\(X_{\text{train}}\)</span>)</strong>: This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis <span class="arithmatex">\(h \in \mathcal{H}\)</span>.</li>
<li><strong>Validation Set (<span class="arithmatex">\(X_{\text{val}}\)</span>)</strong>: This is split from our <span class="arithmatex">\(X_{\text{train}}\)</span> during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis <span class="arithmatex">\(g \in \mathcal{H}\)</span>).</li>
<li><strong>Test Set (<span class="arithmatex">\(X_{\text{test}}\)</span>)</strong>: This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model <span class="arithmatex">\(g\)</span>, we will use <span class="arithmatex">\(g\)</span> to predict on the test set to get an estimate of the generalization error (also called out-of-sample error).</li>
</ul>
<hr />
<figure>
<img src='https://scikit-learn.org/stable/_images/grid_search_workflow.png' width="500"/>
<figcaption align = "center"><b>Courtesy of scikit-learn on a typical Cross-Validation workflow.</b></figcaption>
</figure>

<div class="highlight"><pre><span></span><code><span class="n">predictor_cols</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">target_col</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;price&quot;</span><span class="p">]</span>
<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The predictor columns are </span><span class="se">\n</span><span class="si">{</span><span class="n">predictor_cols</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>2021-11-10,12:09:44 - 
The predictor columns are 
[&#39;bedrooms&#39;, &#39;bathrooms&#39;, &#39;sqft_living&#39;, &#39;sqft_lot&#39;, &#39;floors&#39;, &#39;waterfront&#39;, &#39;view&#39;, &#39;condition&#39;, &#39;grade&#39;, &#39;sqft_above&#39;, &#39;sqft_basement&#39;, &#39;yr_built&#39;, &#39;yr_renovated&#39;, &#39;zipcode&#39;, &#39;lat&#39;, &#39;long&#39;, &#39;sqft_living15&#39;, &#39;sqft_lot15&#39;]
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">predictor_cols</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">target_col</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># Split train - test</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">train_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span>
<span class="p">)</span>

<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Shape of train: </span><span class="si">{</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s2">Shape of test: </span><span class="si">{</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>2021-11-10,12:09:44 - 
Shape of train: (17290, 18)
Shape of test: (4323, 18)
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">make_folds</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
    <span class="n">num_folds</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">cv_schema</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">predictor_col</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
    <span class="n">target_col</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Split the given dataframe into training folds.</span>

<span class="sd">    Args:</span>
<span class="sd">        df (pd.DataFrame): [description]</span>
<span class="sd">        num_folds (int): [description]</span>
<span class="sd">        cv_schema (str): [description]</span>
<span class="sd">        seed (int): [description]</span>

<span class="sd">    Returns:</span>
<span class="sd">        pd.DataFrame: [description]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">cv_schema</span> <span class="o">==</span> <span class="s2">&quot;KFold&quot;</span><span class="p">:</span>
        <span class="n">df_folds</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">kf</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">num_folds</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">fold</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">val_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">df_folds</span><span class="p">[</span><span class="n">predictor_col</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">df_folds</span><span class="p">[</span><span class="n">target_col</span><span class="p">])</span>
        <span class="p">):</span>
            <span class="n">df_folds</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">val_idx</span><span class="p">,</span> <span class="s2">&quot;fold&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">fold</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">df_folds</span><span class="p">[</span><span class="s2">&quot;fold&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_folds</span><span class="p">[</span><span class="s2">&quot;fold&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">cv_schema</span> <span class="o">==</span> <span class="s2">&quot;StratifiedKFold&quot;</span><span class="p">:</span>
        <span class="n">df_folds</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">skf</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">num_folds</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">fold</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">val_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">df_folds</span><span class="p">[</span><span class="n">predictor_col</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">df_folds</span><span class="p">[</span><span class="n">target_col</span><span class="p">])</span>
        <span class="p">):</span>
            <span class="n">df_folds</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">val_idx</span><span class="p">,</span> <span class="s2">&quot;fold&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">fold</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">df_folds</span><span class="p">[</span><span class="s2">&quot;fold&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_folds</span><span class="p">[</span><span class="s2">&quot;fold&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">df_folds</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;fold&quot;</span><span class="p">,</span> <span class="s2">&quot;diagnosis&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">df_folds</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">X_y_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df_folds</span> <span class="o">=</span> <span class="n">make_folds</span><span class="p">(</span><span class="n">X_y_train</span><span class="p">,</span> <span class="n">num_folds</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_folds</span><span class="p">,</span> <span class="n">cv_schema</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">cv_schema</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">predictor_col</span><span class="o">=</span> <span class="n">predictor_cols</span><span class="p">,</span> <span class="n">target_col</span> <span class="o">=</span> <span class="n">target_col</span><span class="p">)</span>
</code></pre></div>
<p>Looks good! All our five folds are now in <code>df_fold</code>!</p>
<h1 id="modelling">Modelling</h1>
<h2 id="spot-checking-algorithms">Spot Checking Algorithms</h2>
<div class="alert alert-success" role="alert">
    <b>Terminology Alert!</b> This method is advocated by <a href="https://machinelearningmastery.com/">Jason Brownlee PhD</a> and this serves as the first stage of my modelling process. We will rapidly test (spot check) different classifier algorithms, from <code>DummyClassifier</code>, to <code>LinearModel</code> to more sophisticated ensemble trees like <code>RandomForest</code>. 
</div>

<hr />
<p>I also note to the readers that we need to think of a few things when choosing the "optimal" machine learning algorithm:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem">No Lunch Free Theorem</a> intuitively says that no single optimization algorithm can work best in all situations. Therefore, spot checking can help us form a basis of which algorithm might work better in this particular scenario.</li>
<li><a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam's Razor</a> often appears in many Machine Learning textbook, and the narrative is that a simpler model more often times generalizes better than a complex model. This is not unfamiliar when we think of the bias-variance tradeoff, and that is why there is always a tradeoff that we must make.</li>
</ul>
<h3 id="make-basic-pipeline-say-no-to-data-leakage">Make Basic Pipeline (Say No to Data Leakage!)</h3>
<div class="alert alert-block alert-danger">
<b>Say No to Data Leakage:</b> This has been emphasized throughout and we must be careful as we should never touch the test set when fitting the model.
    <li> This means that preprocessing steps such as <code>StandardScaling()</code> should only be fitted on the training data, and then apply the same transformation (mean and std) on the test data. In other words, do not apply scaling on the whole dataset before splitting. 
    <li> However, it is also equally important to take note <b>not to contaminate</b> our validation set, which is often overlooked, resulting in over optimistic results from model selection phase, but perform badly on unseen test set. As a result, when we use a 5 fold cross validation, we should be careful during fitting that the preprocessing steps are only applied on the training folds, and not on all 5 folds.
    <li> The same idea is also applied to our <code>ReduceVIF()</code> preprocessing step. We should also include this in our pipeline and not select the features outside the cross-validation loop.</li>
</div>

<p>Scikit Learn's <code>Pipeline</code> object will prevent us from data leakage, as the steps in a pipeline is already pre-defined. There is also a lot of flexibility in this object, as you can even write custom functions in your pipeline!</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">variance_inflation_factor</span><span class="p">(</span><span class="n">exog</span><span class="p">,</span> <span class="n">idx_kept</span><span class="p">,</span> <span class="n">vif_idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute VIF for one feature.</span>

<span class="sd">    Args:</span>
<span class="sd">        exog (np.ndarray): Observations</span>
<span class="sd">        idx_kept (List[int]): Indices of features to consider</span>
<span class="sd">        vif_idx (int): Index of feature for which to compute VIF</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: VIF for the selected feature</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">exog</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">exog</span><span class="p">)</span>

    <span class="n">x_i</span> <span class="o">=</span> <span class="n">exog</span><span class="p">[:,</span> <span class="n">vif_idx</span><span class="p">]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">idx_kept</span> <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="n">vif_idx</span><span class="p">]</span>
    <span class="n">x_noti</span> <span class="o">=</span> <span class="n">exog</span><span class="p">[:,</span> <span class="n">mask</span><span class="p">]</span>

    <span class="n">r_squared_i</span> <span class="o">=</span> <span class="n">OLS</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">x_noti</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">rsquared</span>
    <span class="n">vif</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">r_squared_i</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">vif</span>

<span class="k">class</span> <span class="nc">ReduceVIF</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class;</span>
<span class="sd">    I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_drop</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thresh</span> <span class="o">=</span> <span class="n">thresh</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_drop</span> <span class="o">=</span> <span class="n">max_drop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">column_indices_kept_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_names_kept_</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Resets the state of predictor columns after each fold.&quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">column_indices_kept_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_names_kept_</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names</span>

<span class="sd">        Args:</span>
<span class="sd">            X ([type]): [description]</span>
<span class="sd">            y ([type], optional): [description]. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [type]: [description]</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">column_indices_kept_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_names_kept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_vif</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>     

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Transforms the Validation Set according to the selected feature names.</span>

<span class="sd">        Args:</span>
<span class="sd">            X ([type]): [description]</span>
<span class="sd">            y ([type], optional): [description]. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [type]: [description]</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">column_indices_kept_</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">calculate_vif</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;Implements a VIF function that recursively eliminates features.</span>

<span class="sd">        Args:</span>
<span class="sd">            X (Union[np.ndarray, pd.DataFrame]): [description]</span>

<span class="sd">        Returns:</span>
<span class="sd">            [type]: [description]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">feature_names</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">column_indices_kept</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
            <span class="n">feature_names</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span>

        <span class="n">dropped</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">while</span> <span class="n">dropped</span> <span class="ow">and</span> <span class="n">count</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_drop</span><span class="p">:</span>
            <span class="n">dropped</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="n">max_vif</span><span class="p">,</span> <span class="n">max_vif_col</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

            <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">column_indices_kept</span><span class="p">:</span>

                <span class="n">vif</span> <span class="o">=</span> <span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">column_indices_kept</span><span class="p">,</span> <span class="n">col</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">max_vif</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">vif</span> <span class="o">&gt;</span> <span class="n">max_vif</span><span class="p">:</span>
                    <span class="n">max_vif</span> <span class="o">=</span> <span class="n">vif</span>
                    <span class="n">max_vif_col</span> <span class="o">=</span> <span class="n">col</span>

            <span class="k">if</span> <span class="n">max_vif</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">thresh</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dropping </span><span class="si">{</span><span class="n">max_vif_col</span><span class="si">}</span><span class="s2"> with vif=</span><span class="si">{</span><span class="n">max_vif</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">column_indices_kept</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">max_vif_col</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">feature_names</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">feature_names</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">max_vif_col</span><span class="p">)</span>

                <span class="n">dropped</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">column_indices_kept</span><span class="p">,</span> <span class="n">feature_names</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># create a feature preparation pipeline for a model</span>
<span class="k">def</span> <span class="nf">make_pipeline</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make a Pipeline for Training.</span>

<span class="sd">    Args:</span>
<span class="sd">        model ([type]): [description]</span>

<span class="sd">    Returns:</span>
<span class="sd">        [type]: [description]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">steps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="c1"># standardization</span>
    <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;standardize&#39;</span><span class="p">,</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()))</span>
    <span class="c1"># reduce VIF</span>
    <span class="c1"># steps.append((&quot;remove_multicollinearity&quot;, ReduceVIF(thresh=10)))</span>
    <span class="c1"># the model</span>
    <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">model</span><span class="p">))</span>
    <span class="c1"># create pipeline</span>
    <span class="n">_pipeline</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_pipeline</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">regressors</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># baseline model</span>
    <span class="n">dummy</span><span class="o">.</span><span class="n">DummyRegressor</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">),</span>
    <span class="c1"># linear model</span>
    <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">linear_model</span><span class="o">.</span><span class="n">ElasticNet</span><span class="p">(</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="c1"># tree</span>
    <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeRegressor</span><span class="p">(</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;squared_error&quot;</span>
    <span class="p">),</span>
    <span class="c1"># ensemble</span>
    <span class="c1">#  ensemble.RandomForestClassifier(random_state=config.seed),</span>
<span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">regressors</span> <span class="o">=</span> <span class="p">[</span><span class="n">make_pipeline</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">regressors</span><span class="p">]</span>
</code></pre></div>
<h3 id="define-metrics">Define Metrics</h3>
<div class="highlight"><pre><span></span><code><span class="n">default_result_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;y_true&quot;</span><span class="p">,</span>
    <span class="s2">&quot;y_pred&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">default_logit_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;y_true&quot;</span><span class="p">,</span>
    <span class="s2">&quot;y_pred&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">default_score_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;explained_variance_score&quot;</span><span class="p">,</span>
    <span class="s2">&quot;mean_squared_error&quot;</span><span class="p">,</span>
    <span class="s2">&quot;mean_absolute_error&quot;</span><span class="p">,</span>
    <span class="s2">&quot;root_mean_squared_error&quot;</span><span class="p">,</span>
    <span class="s2">&quot;r2_score&quot;</span><span class="p">,</span>
    <span class="s2">&quot;mean_absolute_percentage_error&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">custom_score_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;adjusted_r2&quot;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">adjusted_r2</span><span class="p">(</span><span class="n">r2</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Calculate adjusted R^2.</span>

<span class="sd">    Args:</span>
<span class="sd">        r2 (float): r2 score of the model/</span>
<span class="sd">        n (int): number of samples.</span>
<span class="sd">        k (int): number of features minus the constant bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        adjusted_r2_score (float): r2 * (n - 1) / (n - k - 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">adjusted_r2_score</span> <span class="o">=</span> <span class="n">r2</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adjusted_r2_score</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Results</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Stores results for model training in columnwise format.&quot;&quot;&quot;</span>

    <span class="n">_result_dict</span><span class="p">:</span> <span class="n">Dict</span>

    <span class="n">logit_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
    <span class="n">score_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">logit_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">default_logit_names</span><span class="p">,</span>
        <span class="n">score_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">default_score_names</span><span class="p">,</span>
        <span class="n">existing_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct a new results store.&quot;&quot;&quot;</span>       
        <span class="bp">self</span><span class="o">.</span><span class="n">logit_names</span> <span class="o">=</span> <span class="n">logit_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">score_names</span> <span class="o">=</span> <span class="n">score_names</span>

        <span class="k">if</span> <span class="n">existing_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_result_dict</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">existing_dict</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="n">dict_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;identifier&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">logit_names</span><span class="p">,</span> <span class="o">*</span><span class="n">score_names</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_result_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">dict_keys</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">identifier</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">results</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">in_place</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Add a new results row.&quot;&quot;&quot;</span>        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">in_place</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Results</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logit_names</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">score_names</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_result_dict</span>
            <span class="p">)</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">identifier</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">in_place</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_result_dict</span><span class="p">[</span><span class="s2">&quot;identifier&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">identifier</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">result_name</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">([</span><span class="o">*</span><span class="n">results</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">logit_names</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">score_names</span><span class="p">]):</span>

            <span class="n">result_value</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">result_name</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_result_dict</span><span class="p">[</span><span class="n">result_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result_value</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">get_result</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Get a map of identifiers to result values for a result.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="n">identifier</span><span class="p">:</span> <span class="n">result_value</span> <span class="k">for</span>
            <span class="n">identifier</span><span class="p">,</span> <span class="n">result_value</span> <span class="ow">in</span>
            <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_result_dict</span><span class="p">[</span><span class="s2">&quot;identifier&quot;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_result_dict</span><span class="p">[</span><span class="n">result_name</span><span class="p">])</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">get_result_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Get a list of values for a result.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_result_dict</span><span class="p">[</span><span class="n">result_name</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">to_dataframe</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Get a Data Frame containing the results.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_result_dict</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Get a dictionary containing the results.</span>

<span class="sd">        Returns:</span>
<span class="sd">             Dict[str, List[Any]]: Dictionary of result columns </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_result_dict</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Compute metrics from logits.&quot;&quot;&quot;</span>

    <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="s2">&quot;y_true&quot;</span><span class="p">],</span> <span class="n">logits</span><span class="p">[</span><span class="s2">&quot;y_pred&quot;</span><span class="p">]</span>

    <span class="n">default_score_names</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;explained_variance_score&quot;</span><span class="p">,</span>
        <span class="s2">&quot;mean_squared_error&quot;</span><span class="p">,</span>
        <span class="s2">&quot;mean_absolute_error&quot;</span><span class="p">,</span>
        <span class="s2">&quot;root_mean_squared_error&quot;</span><span class="p">,</span>
        <span class="s2">&quot;r2_score&quot;</span><span class="p">,</span>
        <span class="s2">&quot;mean_absolute_percentage_error&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="n">default_metrics_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">custom_metrics_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">metric_name</span> <span class="ow">in</span> <span class="n">default_score_names</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">_regression</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">):</span>
            <span class="c1"># TODO: get metric score with default parameters, consider adding kwargs if you want to configure parameters</span>
            <span class="n">metric_score</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">_regression</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">)(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># logger.info(f&quot;{metrics._regression} has no such attribute {metric_name}!&quot;)</span>
            <span class="c1"># add custom metrics here</span>
            <span class="n">rmse</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">_regression</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">custom_metrics_dict</span><span class="p">[</span><span class="s2">&quot;root_mean_squared_error&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rmse</span>

        <span class="k">if</span> <span class="n">metric_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">default_metrics_dict</span><span class="p">:</span>
            <span class="n">default_metrics_dict</span><span class="p">[</span><span class="n">metric_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">metric_score</span>

        <span class="n">metrics_dict</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">default_metrics_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">custom_metrics_dict</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">metrics_dict</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">mean_score</span><span class="p">(</span><span class="n">score_values</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Compute the mean score.&quot;&quot;&quot;</span>

    <span class="n">score_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">score_values</span><span class="p">)</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">score_values</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">score_values</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">score_values</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mean_cv_results</span><span class="p">(</span><span class="n">model_results</span><span class="p">:</span> <span class="n">Results</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Add mean cross-validation results.</span>

<span class="sd">    This method computes the mean value for all</span>
<span class="sd">    score types in the model_results, including</span>
<span class="sd">    for scores (e.g., confusion matrix) where</span>
<span class="sd">    the mean value may contain decimal places.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cv_logits</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">y_result</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">model_results</span><span class="o">.</span><span class="n">get_result_values</span><span class="p">(</span><span class="n">y_result</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">y_result</span> <span class="ow">in</span>
        <span class="n">model_results</span><span class="o">.</span><span class="n">logit_names</span>
    <span class="p">}</span>

    <span class="n">cv_scores</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">score</span><span class="p">:</span> <span class="n">mean_score</span><span class="p">(</span>
            <span class="n">model_results</span><span class="o">.</span><span class="n">get_result_values</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">model_results</span><span class="o">.</span><span class="n">score_names</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">cv_logits</span><span class="p">,</span>
        <span class="o">**</span><span class="n">cv_scores</span><span class="p">,</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">oof_cv_results</span><span class="p">(</span><span class="n">model_results</span><span class="p">:</span> <span class="n">Results</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Add OOF cross-validation results.&quot;&quot;&quot;</span>

    <span class="n">cv_logits</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">y_result</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="n">model_results</span><span class="o">.</span><span class="n">get_result_values</span><span class="p">(</span><span class="n">y_result</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">y_result</span> <span class="ow">in</span>
        <span class="n">model_results</span><span class="o">.</span><span class="n">logit_names</span>
    <span class="p">}</span>

    <span class="n">cv_scores</span> <span class="o">=</span> <span class="n">compute_metrics</span><span class="p">(</span><span class="n">cv_logits</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="o">**</span><span class="n">cv_logits</span><span class="p">,</span>
        <span class="o">**</span><span class="n">cv_scores</span><span class="p">,</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">add_cv_results</span><span class="p">(</span><span class="n">model_results</span><span class="p">:</span> <span class="n">Results</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add cross-validation results.</span>

<span class="sd">    This method returns a copy of the given model results</span>
<span class="sd">    with summary columns for mean and CV cross-validation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mean_cv</span> <span class="o">=</span> <span class="n">mean_cv_results</span><span class="p">(</span><span class="n">model_results</span><span class="p">)</span>
    <span class="n">oof_cv</span> <span class="o">=</span> <span class="n">oof_cv_results</span><span class="p">(</span><span class="n">model_results</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">model_results</span>
        <span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">&quot;mean_cv&quot;</span><span class="p">,</span> <span class="n">mean_cv</span><span class="p">)</span>
        <span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">&quot;oof_cv&quot;</span><span class="p">,</span> <span class="n">oof_cv</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">train_on_fold</span><span class="p">(</span>
    <span class="n">df_folds</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
    <span class="n">models</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
    <span class="n">num_folds</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">predictor_col</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
    <span class="n">target_col</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results.</span>

<span class="sd">    Args:</span>
<span class="sd">        df_folds (pd.DataFrame): [description]</span>
<span class="sd">        model (Callable): [description]</span>
<span class="sd">        num_folds (int): [description]</span>
<span class="sd">        predictor_col (List): [description]</span>
<span class="sd">        target_col (List): [description]</span>


<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, List]: [description]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">y_true</span> <span class="o">=</span> <span class="n">df_folds</span><span class="p">[</span><span class="n">target_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="c1"># test_pred_arr: np.ndarray = np.zeros(len(X_test))</span>

    <span class="n">model_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>
        <span class="n">model_results</span> <span class="o">=</span> <span class="n">Results</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">Pipeline</span><span class="p">):</span>
            <span class="n">model_name</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model_name</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="c1"># out-of-fold validation predictions</span>
        <span class="n">oof_pred_arr</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_folds</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_folds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>

            <span class="n">train_df</span> <span class="o">=</span> <span class="n">df_folds</span><span class="p">[</span><span class="n">df_folds</span><span class="p">[</span><span class="s2">&quot;fold&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="n">fold</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">val_df</span> <span class="o">=</span> <span class="n">df_folds</span><span class="p">[</span><span class="n">df_folds</span><span class="p">[</span><span class="s2">&quot;fold&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">fold</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">val_idx</span> <span class="o">=</span> <span class="n">df_folds</span><span class="p">[</span><span class="n">df_folds</span><span class="p">[</span><span class="s2">&quot;fold&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">fold</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="n">predictor_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train_df</span><span class="p">[</span><span class="n">target_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
            <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">val_df</span><span class="p">[</span><span class="n">predictor_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">val_df</span><span class="p">[</span><span class="n">target_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

            <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
            <span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>


            <span class="n">logits</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;y_true&quot;</span><span class="p">:</span> <span class="n">y_val</span><span class="p">,</span>
                <span class="s2">&quot;y_pred&quot;</span><span class="p">:</span> <span class="n">y_val_pred</span><span class="p">,</span>
            <span class="p">}</span>

            <span class="n">metrics</span> <span class="o">=</span> <span class="n">compute_metrics</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

            <span class="n">model_results</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;fold </span><span class="si">{</span><span class="n">fold</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="p">{</span>
                <span class="o">**</span><span class="n">logits</span><span class="p">,</span>
                <span class="o">**</span><span class="n">metrics</span>
            <span class="p">},</span> <span class="n">in_place</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


        <span class="k">if</span> <span class="n">model_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_dict</span><span class="p">:</span>
            <span class="n">model_dict</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_results</span>

    <span class="k">return</span> <span class="n">model_dict</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">model_dict</span> <span class="o">=</span> <span class="n">train_on_fold</span><span class="p">(</span>
    <span class="n">df_folds</span><span class="p">,</span>
    <span class="n">models</span> <span class="o">=</span> <span class="n">regressors</span><span class="p">,</span>
    <span class="n">num_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">predictor_col</span><span class="o">=</span><span class="n">predictor_cols</span><span class="p">,</span>
    <span class="n">target_col</span> <span class="o">=</span> <span class="n">target_col</span>
<span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>2021-11-10,12:09:44 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:44 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:44 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:44 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:44 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:45 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:45 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:45 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:45 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:45 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:45 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:45 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:45 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:45 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:45 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
C:\Users\reighns\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.676e+13, tolerance: 1.868e+11
  model = cd_fast.enet_coordinate_descent(
2021-11-10,12:09:46 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
C:\Users\reighns\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+13, tolerance: 1.862e+11
  model = cd_fast.enet_coordinate_descent(
2021-11-10,12:09:48 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
C:\Users\reighns\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.603e+13, tolerance: 1.760e+11
  model = cd_fast.enet_coordinate_descent(
2021-11-10,12:09:49 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
C:\Users\reighns\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+13, tolerance: 1.897e+11
  model = cd_fast.enet_coordinate_descent(
2021-11-10,12:09:50 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
C:\Users\reighns\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e+13, tolerance: 1.913e+11
  model = cd_fast.enet_coordinate_descent(
2021-11-10,12:09:52 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:52 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:52 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:52 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:52 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:52 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:52 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:52 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:52 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:53 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:09:53 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">model_dict_with_summary</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">add_cv_results</span><span class="p">(</span><span class="n">model_results</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">model_results</span> <span class="ow">in</span> <span class="n">model_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="p">}</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>2021-11-10,12:19:12 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:19:12 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:19:12 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:19:12 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:19:12 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
2021-11-10,12:19:12 - &lt;module &#39;sklearn.metrics._regression&#39; from &#39;C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_regression.py&#39;&gt; has no such attribute root_mean_squared_error!
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">({</span>
    <span class="n">name</span><span class="p">:</span> <span class="n">results</span><span class="o">.</span><span class="n">to_dataframe</span><span class="p">()</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">results</span>
    <span class="ow">in</span> <span class="n">model_dict_with_summary</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="p">},</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">results_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;fold 1&#39;</span><span class="p">,</span> <span class="s1">&#39;fold 2&#39;</span><span class="p">,</span> <span class="s1">&#39;fold 3&#39;</span><span class="p">,</span> <span class="s1">&#39;fold 4&#39;</span><span class="p">,</span> <span class="s1">&#39;fold 5&#39;</span><span class="p">,</span> <span class="s1">&#39;mean_cv&#39;</span><span class="p">,</span> <span class="s1">&#39;oof_cv&#39;</span><span class="p">]</span>
<span class="n">results_df</span>
</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>fold 1</th>
      <th>fold 2</th>
      <th>fold 3</th>
      <th>fold 4</th>
      <th>fold 5</th>
      <th>mean_cv</th>
      <th>oof_cv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="9" valign="top">DummyRegressor</th>
      <th>identifier</th>
      <td>fold 1</td>
      <td>fold 2</td>
      <td>fold 3</td>
      <td>fold 4</td>
      <td>fold 5</td>
      <td>mean_cv</td>
      <td>oof_cv</td>
    </tr>
    <tr>
      <th>y_true</th>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
      <td>[[850000.0], [653000.0], [532000.0], [385000.0...</td>
      <td>[[1274950.0], [392137.0], [850000.0], [520000....</td>
      <td>[[450000.0], [495000.0], [395000.0], [280000.0...</td>
      <td>[[754999.0], [588500.0], [525000.0], [525000.0...</td>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
    </tr>
    <tr>
      <th>y_pred</th>
      <td>[541073.7741469058, 541073.7741469058, 541073....</td>
      <td>[539415.1803788317, 539415.1803788317, 539415....</td>
      <td>[536694.9069548872, 536694.9069548872, 536694....</td>
      <td>[539857.5267495662, 539857.5267495662, 539857....</td>
      <td>[540990.9316078658, 540990.9316078658, 540990....</td>
      <td>[541073.7741469058, 541073.7741469058, 541073....</td>
      <td>[541073.7741469058, 541073.7741469058, 541073....</td>
    </tr>
    <tr>
      <th>explained_variance_score</th>
      <td>-0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.0</td>
      <td>-0.0</td>
      <td>-0.000169</td>
    </tr>
    <tr>
      <th>mean_squared_error</th>
      <td>132247512137.06163</td>
      <td>133951959536.310318</td>
      <td>163296091978.429077</td>
      <td>123686085780.066254</td>
      <td>119223815039.600311</td>
      <td>134481092894.293533</td>
      <td>134481092894.293488</td>
    </tr>
    <tr>
      <th>mean_absolute_error</th>
      <td>229404.508989</td>
      <td>232306.56185</td>
      <td>241944.838637</td>
      <td>232308.320526</td>
      <td>230976.128917</td>
      <td>233388.071784</td>
      <td>233388.071784</td>
    </tr>
    <tr>
      <th>root_mean_squared_error</th>
      <td>363658.510332</td>
      <td>365994.480199</td>
      <td>404099.111578</td>
      <td>351690.326538</td>
      <td>345288.017515</td>
      <td>366146.089233</td>
      <td>366716.63842</td>
    </tr>
    <tr>
      <th>r2_score</th>
      <td>-0.000407</td>
      <td>-0.000007</td>
      <td>-0.0013</td>
      <td>-0.000013</td>
      <td>-0.000402</td>
      <td>-0.000426</td>
      <td>-0.000169</td>
    </tr>
    <tr>
      <th>mean_absolute_percentage_error</th>
      <td>0.542257</td>
      <td>0.530016</td>
      <td>0.531438</td>
      <td>0.534771</td>
      <td>0.540719</td>
      <td>0.53584</td>
      <td>0.53584</td>
    </tr>
    <tr>
      <th rowspan="9" valign="top">LinearRegression</th>
      <th>identifier</th>
      <td>fold 1</td>
      <td>fold 2</td>
      <td>fold 3</td>
      <td>fold 4</td>
      <td>fold 5</td>
      <td>mean_cv</td>
      <td>oof_cv</td>
    </tr>
    <tr>
      <th>y_true</th>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
      <td>[[850000.0], [653000.0], [532000.0], [385000.0...</td>
      <td>[[1274950.0], [392137.0], [850000.0], [520000....</td>
      <td>[[450000.0], [495000.0], [395000.0], [280000.0...</td>
      <td>[[754999.0], [588500.0], [525000.0], [525000.0...</td>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
    </tr>
    <tr>
      <th>y_pred</th>
      <td>[[667876.5428875468], [614410.8399372244], [73...</td>
      <td>[[694869.3745167988], [618367.5827104518], [46...</td>
      <td>[[1023189.3043934056], [304317.4518826463], [9...</td>
      <td>[[477332.04737439007], [492396.46923110134], [...</td>
      <td>[[853190.5779001702], [574292.3295129627], [77...</td>
      <td>[[667876.5428875468], [614410.8399372244], [73...</td>
      <td>[[667876.5428875468], [614410.8399372244], [73...</td>
    </tr>
    <tr>
      <th>explained_variance_score</th>
      <td>0.680095</td>
      <td>0.705119</td>
      <td>0.685252</td>
      <td>0.693963</td>
      <td>0.721335</td>
      <td>0.697153</td>
      <td>0.696083</td>
    </tr>
    <tr>
      <th>mean_squared_error</th>
      <td>42290033057.306816</td>
      <td>39515957195.400787</td>
      <td>51430470051.222527</td>
      <td>37873842125.074295</td>
      <td>33210290838.463482</td>
      <td>40864118653.493584</td>
      <td>40864118653.493584</td>
    </tr>
    <tr>
      <th>mean_absolute_error</th>
      <td>127943.982537</td>
      <td>124266.336776</td>
      <td>130373.471382</td>
      <td>126075.417537</td>
      <td>122514.978064</td>
      <td>126234.837259</td>
      <td>126234.837259</td>
    </tr>
    <tr>
      <th>root_mean_squared_error</th>
      <td>205645.406118</td>
      <td>198786.209772</td>
      <td>226782.869836</td>
      <td>194612.029754</td>
      <td>182236.908552</td>
      <td>201612.684806</td>
      <td>202148.753777</td>
    </tr>
    <tr>
      <th>r2_score</th>
      <td>0.68009</td>
      <td>0.704997</td>
      <td>0.684638</td>
      <td>0.693787</td>
      <td>0.721334</td>
      <td>0.696969</td>
      <td>0.696083</td>
    </tr>
    <tr>
      <th>mean_absolute_percentage_error</th>
      <td>0.265815</td>
      <td>0.257898</td>
      <td>0.250641</td>
      <td>0.251933</td>
      <td>0.255471</td>
      <td>0.256352</td>
      <td>0.256352</td>
    </tr>
    <tr>
      <th rowspan="9" valign="top">Ridge</th>
      <th>identifier</th>
      <td>fold 1</td>
      <td>fold 2</td>
      <td>fold 3</td>
      <td>fold 4</td>
      <td>fold 5</td>
      <td>mean_cv</td>
      <td>oof_cv</td>
    </tr>
    <tr>
      <th>y_true</th>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
      <td>[[850000.0], [653000.0], [532000.0], [385000.0...</td>
      <td>[[1274950.0], [392137.0], [850000.0], [520000....</td>
      <td>[[450000.0], [495000.0], [395000.0], [280000.0...</td>
      <td>[[754999.0], [588500.0], [525000.0], [525000.0...</td>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
    </tr>
    <tr>
      <th>y_pred</th>
      <td>[[667923.954879363], [614423.8176756387], [731...</td>
      <td>[[694094.2870407818], [617981.9933915635], [46...</td>
      <td>[[1023626.0773182933], [304085.70358412666], [...</td>
      <td>[[477416.1959023318], [492474.1026423527], [30...</td>
      <td>[[853485.3791880516], [574203.5276395974], [77...</td>
      <td>[[667923.954879363], [614423.8176756387], [731...</td>
      <td>[[667923.954879363], [614423.8176756387], [731...</td>
    </tr>
    <tr>
      <th>explained_variance_score</th>
      <td>0.680109</td>
      <td>0.70512</td>
      <td>0.685333</td>
      <td>0.693972</td>
      <td>0.721308</td>
      <td>0.697168</td>
      <td>0.696102</td>
    </tr>
    <tr>
      <th>mean_squared_error</th>
      <td>42288266590.795799</td>
      <td>39515834657.582001</td>
      <td>51417532361.73008</td>
      <td>37872817115.509499</td>
      <td>33213482338.320721</td>
      <td>40861586612.787613</td>
      <td>40861586612.787621</td>
    </tr>
    <tr>
      <th>mean_absolute_error</th>
      <td>127934.770004</td>
      <td>124362.619797</td>
      <td>130426.951817</td>
      <td>126064.678687</td>
      <td>122549.911589</td>
      <td>126267.786379</td>
      <td>126267.786379</td>
    </tr>
    <tr>
      <th>root_mean_squared_error</th>
      <td>205641.11114</td>
      <td>198785.901556</td>
      <td>226754.343645</td>
      <td>194609.396267</td>
      <td>182245.6648</td>
      <td>201607.283482</td>
      <td>202142.490864</td>
    </tr>
    <tr>
      <th>r2_score</th>
      <td>0.680104</td>
      <td>0.704998</td>
      <td>0.684718</td>
      <td>0.693795</td>
      <td>0.721307</td>
      <td>0.696984</td>
      <td>0.696102</td>
    </tr>
    <tr>
      <th>mean_absolute_percentage_error</th>
      <td>0.265796</td>
      <td>0.258152</td>
      <td>0.250808</td>
      <td>0.251901</td>
      <td>0.255562</td>
      <td>0.256444</td>
      <td>0.256444</td>
    </tr>
    <tr>
      <th rowspan="9" valign="top">Lasso</th>
      <th>identifier</th>
      <td>fold 1</td>
      <td>fold 2</td>
      <td>fold 3</td>
      <td>fold 4</td>
      <td>fold 5</td>
      <td>mean_cv</td>
      <td>oof_cv</td>
    </tr>
    <tr>
      <th>y_true</th>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
      <td>[[850000.0], [653000.0], [532000.0], [385000.0...</td>
      <td>[[1274950.0], [392137.0], [850000.0], [520000....</td>
      <td>[[450000.0], [495000.0], [395000.0], [280000.0...</td>
      <td>[[754999.0], [588500.0], [525000.0], [525000.0...</td>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
    </tr>
    <tr>
      <th>y_pred</th>
      <td>[667918.069528075, 614415.1560338883, 731173.9...</td>
      <td>[694081.9059207196, 617992.7838152755, 463279....</td>
      <td>[1023674.2634054194, 304066.4170321575, 955604...</td>
      <td>[477415.1327845714, 492463.7396309793, 305057....</td>
      <td>[853505.6953654164, 574205.7699892861, 771097....</td>
      <td>[667918.069528075, 614415.1560338883, 731173.9...</td>
      <td>[667918.069528075, 614415.1560338883, 731173.9...</td>
    </tr>
    <tr>
      <th>explained_variance_score</th>
      <td>0.680106</td>
      <td>0.70512</td>
      <td>0.685338</td>
      <td>0.693969</td>
      <td>0.721305</td>
      <td>0.697168</td>
      <td>0.696102</td>
    </tr>
    <tr>
      <th>mean_squared_error</th>
      <td>42288598880.705284</td>
      <td>39515796822.947304</td>
      <td>51416606662.838509</td>
      <td>37873203587.611603</td>
      <td>33213918899.943615</td>
      <td>40861624970.809258</td>
      <td>40861624970.809265</td>
    </tr>
    <tr>
      <th>mean_absolute_error</th>
      <td>127937.742862</td>
      <td>124365.678646</td>
      <td>130429.153983</td>
      <td>126067.441635</td>
      <td>122553.005403</td>
      <td>126270.604506</td>
      <td>126270.604506</td>
    </tr>
    <tr>
      <th>root_mean_squared_error</th>
      <td>205641.919075</td>
      <td>198785.806392</td>
      <td>226752.302442</td>
      <td>194610.389208</td>
      <td>182246.862524</td>
      <td>201607.455928</td>
      <td>202142.585743</td>
    </tr>
    <tr>
      <th>r2_score</th>
      <td>0.680101</td>
      <td>0.704998</td>
      <td>0.684723</td>
      <td>0.693792</td>
      <td>0.721303</td>
      <td>0.696984</td>
      <td>0.696102</td>
    </tr>
    <tr>
      <th>mean_absolute_percentage_error</th>
      <td>0.265806</td>
      <td>0.258161</td>
      <td>0.250816</td>
      <td>0.25191</td>
      <td>0.255571</td>
      <td>0.256453</td>
      <td>0.256453</td>
    </tr>
    <tr>
      <th rowspan="9" valign="top">ElasticNet</th>
      <th>identifier</th>
      <td>fold 1</td>
      <td>fold 2</td>
      <td>fold 3</td>
      <td>fold 4</td>
      <td>fold 5</td>
      <td>mean_cv</td>
      <td>oof_cv</td>
    </tr>
    <tr>
      <th>y_true</th>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
      <td>[[850000.0], [653000.0], [532000.0], [385000.0...</td>
      <td>[[1274950.0], [392137.0], [850000.0], [520000....</td>
      <td>[[450000.0], [495000.0], [395000.0], [280000.0...</td>
      <td>[[754999.0], [588500.0], [525000.0], [525000.0...</td>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
    </tr>
    <tr>
      <th>y_pred</th>
      <td>[656773.6005291657, 614707.2479590292, 745914....</td>
      <td>[685212.4217947914, 599436.3647276227, 483018....</td>
      <td>[907890.7497500202, 369129.65691215475, 862824...</td>
      <td>[469736.2740988735, 539209.0261494196, 379320....</td>
      <td>[804965.135403886, 571735.3944930851, 737716.4...</td>
      <td>[656773.6005291657, 614707.2479590292, 745914....</td>
      <td>[656773.6005291657, 614707.2479590292, 745914....</td>
    </tr>
    <tr>
      <th>explained_variance_score</th>
      <td>0.65603</td>
      <td>0.673131</td>
      <td>0.638955</td>
      <td>0.670126</td>
      <td>0.695926</td>
      <td>0.666834</td>
      <td>0.664827</td>
    </tr>
    <tr>
      <th>mean_squared_error</th>
      <td>45475490348.208717</td>
      <td>43800728719.55619</td>
      <td>59005250118.409103</td>
      <td>40812502573.376862</td>
      <td>36239917129.190384</td>
      <td>45066777777.748253</td>
      <td>45066777777.748253</td>
    </tr>
    <tr>
      <th>mean_absolute_error</th>
      <td>124419.180393</td>
      <td>122248.905619</td>
      <td>131347.081356</td>
      <td>124722.490988</td>
      <td>120613.606068</td>
      <td>124670.252885</td>
      <td>124670.252885</td>
    </tr>
    <tr>
      <th>root_mean_squared_error</th>
      <td>213249.830828</td>
      <td>209286.236336</td>
      <td>242909.962987</td>
      <td>202021.044877</td>
      <td>190367.846889</td>
      <td>211566.984383</td>
      <td>212289.372739</td>
    </tr>
    <tr>
      <th>r2_score</th>
      <td>0.655993</td>
      <td>0.673009</td>
      <td>0.638191</td>
      <td>0.670027</td>
      <td>0.695912</td>
      <td>0.666627</td>
      <td>0.664827</td>
    </tr>
    <tr>
      <th>mean_absolute_percentage_error</th>
      <td>0.247878</td>
      <td>0.245323</td>
      <td>0.243383</td>
      <td>0.244499</td>
      <td>0.243172</td>
      <td>0.244851</td>
      <td>0.244851</td>
    </tr>
    <tr>
      <th rowspan="9" valign="top">DecisionTreeRegressor</th>
      <th>identifier</th>
      <td>fold 1</td>
      <td>fold 2</td>
      <td>fold 3</td>
      <td>fold 4</td>
      <td>fold 5</td>
      <td>mean_cv</td>
      <td>oof_cv</td>
    </tr>
    <tr>
      <th>y_true</th>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
      <td>[[850000.0], [653000.0], [532000.0], [385000.0...</td>
      <td>[[1274950.0], [392137.0], [850000.0], [520000....</td>
      <td>[[450000.0], [495000.0], [395000.0], [280000.0...</td>
      <td>[[754999.0], [588500.0], [525000.0], [525000.0...</td>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
      <td>[[945000.0], [352500.0], [560000.0], [500000.0...</td>
    </tr>
    <tr>
      <th>y_pred</th>
      <td>[915000.0, 300000.0, 316000.0, 643000.0, 28200...</td>
      <td>[535000.0, 630000.0, 410000.0, 335000.0, 72000...</td>
      <td>[1605000.0, 479000.0, 850000.0, 614000.0, 6505...</td>
      <td>[449000.0, 555000.0, 440000.0, 247500.0, 35000...</td>
      <td>[715000.0, 545000.0, 597000.0, 530000.0, 50000...</td>
      <td>[915000.0, 300000.0, 316000.0, 643000.0, 28200...</td>
      <td>[915000.0, 300000.0, 316000.0, 643000.0, 28200...</td>
    </tr>
    <tr>
      <th>explained_variance_score</th>
      <td>0.752389</td>
      <td>0.756145</td>
      <td>0.770122</td>
      <td>0.743783</td>
      <td>0.767617</td>
      <td>0.758011</td>
      <td>0.758577</td>
    </tr>
    <tr>
      <th>mean_squared_error</th>
      <td>32751471474.735687</td>
      <td>32664895113.423656</td>
      <td>37510896623.824608</td>
      <td>31693198492.168594</td>
      <td>27707113797.312031</td>
      <td>32465515100.292915</td>
      <td>32465515100.292915</td>
    </tr>
    <tr>
      <th>mean_absolute_error</th>
      <td>101035.96819</td>
      <td>99810.482071</td>
      <td>105829.744939</td>
      <td>101019.912377</td>
      <td>96233.899075</td>
      <td>100786.00133</td>
      <td>100786.00133</td>
    </tr>
    <tr>
      <th>root_mean_squared_error</th>
      <td>180973.676193</td>
      <td>180734.321902</td>
      <td>193677.300229</td>
      <td>178025.836586</td>
      <td>166454.539732</td>
      <td>179973.134928</td>
      <td>180181.894485</td>
    </tr>
    <tr>
      <th>r2_score</th>
      <td>0.752246</td>
      <td>0.756143</td>
      <td>0.769991</td>
      <td>0.743758</td>
      <td>0.767511</td>
      <td>0.75793</td>
      <td>0.758546</td>
    </tr>
    <tr>
      <th>mean_absolute_percentage_error</th>
      <td>0.192566</td>
      <td>0.186579</td>
      <td>0.189517</td>
      <td>0.183729</td>
      <td>0.184638</td>
      <td>0.187406</td>
      <td>0.187406</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="comparison-of-cross-validated-models">Comparison of Cross-Validated Models</h3>
<p>The point of the following comparison is to check how different models are performing across folds. More specifically, if we have 5 folds, we will have a metric score for each fold, subsequently, we can find the standard error of model's performance. We need to be aware of models that have high variance across folds in terms of the metrics performance. This can indicate that the model is highly unstable, and may be a sign of overfitting.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">summarize_metrics</span><span class="p">(</span><span class="n">metric_name</span><span class="p">):</span>
    <span class="n">ls</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">inner_dict</span> <span class="ow">in</span> <span class="n">model_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">folds</span> <span class="o">=</span> <span class="n">inner_dict</span><span class="p">[</span><span class="s2">&quot;identifier&quot;</span><span class="p">][:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">all_obs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">obs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inner_dict</span><span class="p">[</span><span class="n">metric_name</span><span class="p">][:</span><span class="o">-</span><span class="mi">2</span><span class="p">]):</span>
            <span class="n">ls</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">model_name</span><span class="p">,</span> <span class="n">folds</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">obs</span><span class="p">))</span>
            <span class="n">all_obs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
        <span class="n">ls</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">model_name</span><span class="p">,</span> <span class="s2">&quot;SE&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">all_obs</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_obs</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">))</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

    <span class="n">summary_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ls</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="s2">&quot;fold&quot;</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">])</span>
    <span class="c1"># summary_df.to_csv</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">summary_df</span><span class="p">[(</span><span class="n">summary_df</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;DummyClassifier&#39;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">summary_df</span><span class="p">[</span><span class="s1">&#39;fold&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;SE&#39;</span><span class="p">)],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">spot_checking_boxplot</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">summary_df</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">summary_df</span> <span class="o">=</span> <span class="n">summarize_metrics</span><span class="p">(</span><span class="s2">&quot;roc&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">summary_df</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">12</span><span class="p">))</span>
</code></pre></div>
<h3 id="out-of-fold-confusion-matrix">Out-of-Fold Confusion Matrix</h3>
<p>We do have information on the performance of each folds, we now look at the performance of all 5 folds together. Typicall there are two ways to do it, one is to simply take the average of the score of five folds, the other is to take a look at out of folds predictions.</p>
<hr />
<p>From the confusion matrix of the out of fold performance, Logistic Regression does seem to be a model we can explore on, although slightly lower in terms of overall AUROC score than SVC, it seems to have the quite low False Negatives amongst all. With further hyperparameter tuning and threshold optimization, we can make it better.</p>
<div class="highlight"><pre><span></span><code><span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">model_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="k">for</span> <span class="n">axes</span><span class="p">,</span> <span class="n">algo</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">model_names</span><span class="p">):</span>

    <span class="n">cf_mat</span> <span class="o">=</span> <span class="n">results_df</span><span class="o">.</span><span class="n">oof_cv</span><span class="p">[</span><span class="n">algo</span><span class="p">]</span><span class="o">.</span><span class="n">confusion_matrix</span>

    <span class="c1">#### scores</span>
    <span class="n">auc</span> <span class="o">=</span> <span class="n">results_df</span><span class="o">.</span><span class="n">oof_cv</span><span class="p">[</span><span class="n">algo</span><span class="p">]</span><span class="o">.</span><span class="n">roc</span>

    <span class="c1">#### annotations</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;True Neg&quot;</span><span class="p">,</span> <span class="s2">&quot;False Pos&quot;</span><span class="p">,</span> <span class="s2">&quot;False Neg&quot;</span><span class="p">,</span> <span class="s2">&quot;True Pos&quot;</span><span class="p">]</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;</span><span class="si">{0:0.0f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">cf_mat</span><span class="o">.</span><span class="n">flatten</span><span class="p">()]</span>
    <span class="n">percentages</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;</span><span class="si">{0:.2%}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">cf_mat</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cf_mat</span><span class="p">)]</span>

    <span class="c1">#### final annotations</span>
    <span class="n">label</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">v1</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">v2</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">v3</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">,</span> <span class="n">v3</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">percentages</span><span class="p">)])</span>
    <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># heatmap</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">cf_mat</span><span class="p">,</span>
        <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">vmax</span><span class="o">=</span><span class="mi">330</span><span class="p">,</span>
        <span class="n">cmap</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;#fe4a49&quot;</span><span class="p">,</span> <span class="s2">&quot;#2ab7ca&quot;</span><span class="p">,</span> <span class="s2">&quot;#fed766&quot;</span><span class="p">,</span> <span class="s2">&quot;#59981A&quot;</span><span class="p">],</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">linecolor</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">,</span>
        <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span>
        <span class="n">annot</span><span class="o">=</span><span class="n">label</span><span class="p">,</span>
        <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">},</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">axes</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">algo</span><span class="p">),</span> <span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;bold&quot;</span><span class="p">})</span>

    <span class="n">axes</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3500</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
        <span class="mf">0.72</span><span class="p">,</span>
        <span class="mf">1.0</span><span class="p">,</span>
        <span class="s2">&quot;AUC: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">auc</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
        <span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;bold&quot;</span><span class="p">},</span>
    <span class="p">)</span>

    <span class="c1">## ticks and labels</span>
    <span class="n">axes</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>


<span class="c1">## titles and text</span>
<span class="n">fig</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">,</span> <span class="s2">&quot;Out Of Fold Confusion Matrix&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">22</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;bold&quot;</span><span class="p">},</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="mi">0</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">,</span>
    <span class="sd">&quot;&quot;&quot;This Visualization show the results of various classifiers and there respective</span>
<span class="sd">results.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">14</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;normal&quot;</span><span class="p">},</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span>
<span class="p">)</span>


<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">w_pad</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">h_pad</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">oof_confusion_matrix</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div>
<h3 id="hypothesis-testing-across-models">Hypothesis Testing Across Models</h3>
<p>I am slightly shocked at the performance of plain LogisticRegression, I decide to use an idea from <a href="http://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/">Hypothesis Testing Across Models</a> to check if the difference is really by chance or not. Note that I will be modifying his code as his code does not split using StratifiedKFold.</p>
<hr />
<p>The basic idea is to test if two model's difference in scores (in this case roc), is statistically significant or not. However, we note that this method may violate an assumption of Student's t test.</p>
<hr />
<ul>
<li>Null Hypothesis <span class="arithmatex">\(H_0\)</span>: The difference in the performance score of two classifiers is Statistically Significant.</li>
<li>Alternate Hypothesis <span class="arithmatex">\(H_1\)</span>: The difference in the performance score of two classifiers is <strong>not</strong> Statistically Significant.</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">paired_ttest_skfold_cv</span><span class="p">(</span>
    <span class="n">estimator1</span><span class="p">,</span> <span class="n">estimator2</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Modified from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/ttest.py to accomodate StratifiedKFold&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">shuffle</span><span class="p">:</span>
        <span class="n">skf</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">skf</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">StratifiedKFold</span><span class="p">(</span>
            <span class="n">n_splits</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_seed</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">scoring</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">estimator1</span><span class="o">.</span><span class="n">_estimator_type</span> <span class="o">==</span> <span class="s2">&quot;classifier&quot;</span><span class="p">:</span>
            <span class="n">scoring</span> <span class="o">=</span> <span class="s2">&quot;accuracy&quot;</span>
        <span class="k">elif</span> <span class="n">estimator1</span><span class="o">.</span><span class="n">_estimator_type</span> <span class="o">==</span> <span class="s2">&quot;regressor&quot;</span><span class="p">:</span>
            <span class="n">scoring</span> <span class="o">=</span> <span class="s2">&quot;r2&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;Estimator must &quot;</span> <span class="s2">&quot;be a Classifier or Regressor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scoring</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">scorer</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">get_scorer</span><span class="p">(</span><span class="n">scoring</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">scorer</span> <span class="o">=</span> <span class="n">scoring</span>

    <span class="n">score_diff</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">):</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>

        <span class="n">estimator1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">estimator2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="n">est1_score</span> <span class="o">=</span> <span class="n">scorer</span><span class="p">(</span><span class="n">estimator1</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        <span class="n">est2_score</span> <span class="o">=</span> <span class="n">scorer</span><span class="p">(</span><span class="n">estimator2</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        <span class="n">score_diff</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">est1_score</span> <span class="o">-</span> <span class="n">est2_score</span><span class="p">)</span>

    <span class="n">avg_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">score_diff</span><span class="p">)</span>

    <span class="n">numerator</span> <span class="o">=</span> <span class="n">avg_diff</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cv</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">([(</span><span class="n">diff</span> <span class="o">-</span> <span class="n">avg_diff</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">diff</span> <span class="ow">in</span> <span class="n">score_diff</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">cv</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">t_stat</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="n">pvalue</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">t_stat</span><span class="p">),</span> <span class="n">cv</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">2.0</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">t_stat</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">pvalue</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># check if difference between algorithms is real</span>
<span class="n">X_tmp</span> <span class="o">=</span> <span class="n">X_y_train</span><span class="p">[</span><span class="n">predictor_cols</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_tmp</span> <span class="o">=</span> <span class="n">X_y_train</span><span class="p">[</span><span class="s1">&#39;diagnosis&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">t</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">paired_ttest_skfold_cv</span><span class="p">(</span><span class="n">estimator1</span><span class="o">=</span><span class="n">classifiers</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">estimator2</span><span class="o">=</span><span class="n">classifiers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X_tmp</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_tmp</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;P-value: </span><span class="si">%.3f</span><span class="s1">, t-Statistic: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>
</code></pre></div>
<p>Since P value is quite high, and more the basic threshold of 0.05 or 0.1, we fail to reject the null hypothesis, and say that there is no significant difference between these two models.</p>
<h2 id="model-selection-hyperparameter-tuning-with-gridsearchcv">Model Selection: Hyperparameter Tuning with GridSearchCV</h2>
<div class="alert alert-success" role="alert">
    <b>Hyperparameter Tuning:</b>
    <li> We have done a quick spot checking on algorithms and realized that <code>LogisticRegression</code> is doing well for this task. For this purpose, I will just perform hyperparameter tuning on this single algorithm. However, in practice and if resources are allowed, I will also tune other models such as <code>RandomForest()</code>, or gradient boosting algorithms such as <code>XGBoost</code>, as I believe they will perform no worse than our Logistic Regression model given the right hyperparameters.
</div>

<hr />
<div class="alert alert-info" role="alert">
    <b>Grid Search:</b>
    <li> We will use an old-fashioned way to search for hyperparameters, which is brute force method. The time complexity of Grid Search is high and if you have many hyperparameters to tune, I recommend trying out <b>Random Grid Search</b> or libraries like <b>Optuna</b> that uses Bayesian Optimization.
</div>

<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">make_finetuning_pipeline</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make a Pipeline for Training.</span>

<span class="sd">    Args:</span>
<span class="sd">        model ([type]): [description]</span>

<span class="sd">    Returns:</span>
<span class="sd">        [type]: [description]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">steps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="c1"># standardization</span>
    <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;standardize&#39;</span><span class="p">,</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()))</span>
    <span class="c1"># reduce VIF</span>
    <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;remove_multicollinearity&#39;</span><span class="p">,</span> <span class="n">ReduceVIF</span><span class="p">(</span><span class="n">thresh</span><span class="o">=</span><span class="mi">10</span><span class="p">)))</span>
    <span class="c1"># the model</span>
    <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">model</span><span class="p">))</span>
    <span class="c1"># create pipeline</span>
    <span class="n">_pipeline</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_pipeline</span>
</code></pre></div>
<p>Reconstruct our pipeline but now only taking in <code>LogisticRegression</code>.</p>
<div class="highlight"><pre><span></span><code><span class="n">pipeline_logistic</span> <span class="o">=</span> <span class="n">make_finetuning_pipeline</span><span class="p">(</span>
    <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span>
        <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;saga&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
<span class="p">)</span>
</code></pre></div>
<p>Define our search space for the hyperparameters:</p>
<div class="highlight"><pre><span></span><code><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="n">model__penalty</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">],</span>
              <span class="n">model__C</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">)}</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">param_grid</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">model__penalty</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">],</span>
    <span class="n">model__C</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div>
<p>Run our hyperparameter search with cross-validation. For example, our <code>param_grid</code> has <span class="arithmatex">\(2 \times 10 = 20\)</span> combinations, and our cross validation has 5 folds, then there will be a total of 100 fits.</p>
<hr />
<p>Below details the pseudo code of what happens under the hood:</p>
<ul>
<li>Define <span class="arithmatex">\(G\)</span> as the set of combination of hyperparamters. Define number of splits to be <span class="arithmatex">\(K\)</span>.</li>
<li>For each set of hyperparameter <span class="arithmatex">\(z \in Z\)</span>:<ul>
<li>for fold <span class="arithmatex">\(j\)</span> in K:<ul>
<li>Set <span class="arithmatex">\(F_{\text{train}}=\bigcup\limits_{i\neq k}^{K} F_{i}\)</span></li>
<li>Set <span class="arithmatex">\(F_{\text{val}} = F_{j}\)</span> as the validation set</li>
<li>Perform Standard Scaling on <span class="arithmatex">\(F_{\text{train}}\)</span> and find the mean and std</li>
<li>Perform VIF recursively on <span class="arithmatex">\(F_{\text{train}}\)</span> and find the selected features</li>
<li>Transform <span class="arithmatex">\(F_{\text{val}}\)</span> using the mean and std found using <span class="arithmatex">\(F_{\text{train}}\)</span></li>
<li>Transform <span class="arithmatex">\(F_{\text{val}}\)</span> to have only the selected features from <span class="arithmatex">\(F_{\text{train}}\)</span></li>
<li>Train and fit on <span class="arithmatex">\(F_{\text{train}}\)</span> </li>
</ul>
</li>
<li>Evaluate the fitted parameters on <span class="arithmatex">\(F_{\text{val}}\)</span> to obtain <span class="arithmatex">\(\mathcal{M}\)</span></li>
</ul>
</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">grid</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline_logistic</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s2">&quot;roc_auc&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<p>We can save our results in a dataframe, we will also look at the top performing hyperparameter by querying the below:</p>
<div class="highlight"><pre><span></span><code><span class="n">grid_cv_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>
<span class="n">grid_cv_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">grid_cv_df</span><span class="p">[</span><span class="s1">&#39;rank_test_score&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">grid_cv_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>
<span class="n">best_cv</span> <span class="o">=</span> <span class="n">grid_cv_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">grid_cv_df</span><span class="p">[</span><span class="s1">&#39;rank_test_score&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>
<span class="n">display</span><span class="p">(</span><span class="n">best_cv</span><span class="p">)</span>

<span class="n">best_hyperparams</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best Hyperparameters found is </span><span class="si">{</span><span class="n">best_hyperparams</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Our best performing set of hyperparameters <code>{'model__C': 0.3593813663804626, 'model__penalty': 'l2'}</code> gives rise to a mean cross validation score of <span class="arithmatex">\(0.988739\)</span>, which is higher than the model with default hyperparameter scoring, <span class="arithmatex">\(0.987136\)</span>.</p>
<div class="alert alert-success" role="warning">
    <b>Room for Improvement:</b> Apart from the other methods to search for the optimal hyperparameters, we can also include preprocessing step as a tunable hyperparameter. More specifically, in our <code>ReduceVIF()</code> step, we hard coded two manual criterion in which the algorithm will stop; if the threshold reaches 10, or if the number of features removed hit 20; we can include them in the search space.
</div>

<h2 id="retrain-on-the-whole-training-set">Retrain on the whole training set</h2>
<p>A common practice after the hyperparameter tuning phase is to retrain the model on the whole dataset <span class="arithmatex">\(X_{\text{train}}\)</span> where we will get the estimator's coefficients obtained from the retraining. This is actually already done as the scikit-learn's <code>GridSearchCV</code> has a parameter <code>refit</code>; if we select it to be true, then after the model selection process is done (i.e. getting the best hyperparameters after cross validation with grid search), the grid search object will retrain on the whole <span class="arithmatex">\(X_{\text{train}}\)</span> with the best hyperparameters internally, and return us back an object in which we can call <code>predict</code> etc.</p>
<h3 id="retrain-using-optimal-hyperparameters">Retrain using optimal hyperparameters</h3>
<p>However, to be extra careful, we can retrain manually using the best hyperparameters and check if scikit-learn is true to its documentation. We will just reconstruct the pipeline using the grid's best hyper parameters. We will then test if the retrained model's coefficients coincide with the grid's best estimator's coefficients. If there difference is 0, this means they are trained under the same circumstances and we can be sure that the refit parameter is behaving true to its words.</p>
<hr />
<div class="highlight"><pre><span></span><code><span class="n">grid_best_hyperparams</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grid_best_hyperparams</span><span class="p">)</span> <span class="o">-&gt;</span>
<span class="p">{</span><span class="s1">&#39;model__C&#39;</span><span class="p">:</span> <span class="mf">0.3593813663804626</span><span class="p">,</span>
 <span class="s1">&#39;model__penalty&#39;</span><span class="p">:</span> <span class="s1">&#39;l2&#39;</span><span class="p">}</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">retrain_pipeline</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">Pipeline</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;standardize&quot;</span><span class="p">,</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s1">&#39;remove_multicollinearity&#39;</span><span class="p">,</span> <span class="n">ReduceVIF</span><span class="p">(</span><span class="n">thresh</span><span class="o">=</span><span class="mi">10</span><span class="p">)),</span>

        <span class="p">(</span>
            <span class="s2">&quot;model&quot;</span><span class="p">,</span>
            <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span>
                <span class="n">C</span><span class="o">=</span><span class="mf">0.3593813663804626</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1992</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;saga&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l1&quot;</span>
            <span class="p">),</span>
        <span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">retrain_pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">coef_diff</span> <span class="o">=</span> <span class="n">retrain_pipeline</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span> <span class="o">-</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;...&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">coef_diff</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="kc">True</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Retraining Assertion Passed!&quot;</span><span class="p">)</span>
</code></pre></div>
<h2 id="interpretation-of-results">Interpretation of Results</h2>
<h3 id="interpretation-of-coefficients">Interpretation of Coefficients</h3>
<p>As shown in the figure below, all else being equal, for every square unit increase in mean cell area, the odds of the tumor being malignant increases by a factor of <span class="arithmatex">\(e^{1.43} = 4.19\)</span>. The variation (standard error) of the characteristics of cells also are deemed important by the model, for example, area se played an important role in determining whether a cell is malignant; intuitively, if some cells are noticably larger than the rest, then it is also a good indicator of malignancy.</p>
<div class="highlight"><pre><span></span><code><span class="n">selected_features_by_vif_index</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">[</span><span class="s1">&#39;remove_multicollinearity&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">column_indices_kept_</span> 
<span class="n">selected_feature_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">predictor_cols</span><span class="p">)[</span><span class="n">selected_features_by_vif_index</span><span class="p">]</span>

<span class="n">selected_features_coefficients</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># assertion</span>
<span class="c1">#assert grid.best_estimator_[&#39;remove_multicollinearity&#39;].feature_names_ == retrain_pipeline[&#39;remove_multicollinearity&#39;].feature_names_</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="c1"># .abs()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">selected_features_coefficients</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">selected_feature_names</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;barh&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">feature_importance</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;png&quot;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div>
<h3 id="interpretation-of-metric-scores-on-train-set">Interpretation of Metric Scores on Train Set</h3>
<p>We are also interested in choosing an optimal threshold for the model such that it gives the lowest recall, or False Negatives. We note that the default threshold when calling <code>predict()</code> from a model is <span class="arithmatex">\(0.5\)</span>. In this section, we will explore one way to get the best tradeoff we can when choosing a high recall, while maintaining a reasonable score for precision.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">evaluate_train_test_set</span><span class="p">(</span>
    <span class="n">estimator</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">y</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;This function takes in X and y and returns a dictionary of scores.</span>

<span class="sd">    Args:</span>
<span class="sd">        estimator (Callable): [description]</span>
<span class="sd">        X (Union[pd.DataFrame, np.ndarray]): [description]</span>
<span class="sd">        y (Union[pd.DataFrame, np.ndarray]): [description]</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, Union[float, np.ndarray]]: [description]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">test_results</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># This is the probability array of class 1 (malignant)</span>
    <span class="n">y_prob</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

    <span class="n">test_brier</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">)</span>
    <span class="n">test_roc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">)</span>

    <span class="n">test_results</span><span class="p">[</span><span class="s2">&quot;brier&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_brier</span>
    <span class="n">test_results</span><span class="p">[</span><span class="s2">&quot;roc&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_roc</span>
    <span class="n">test_results</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">test_results</span><span class="p">[</span><span class="s2">&quot;y_pred&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">test_results</span><span class="p">[</span><span class="s2">&quot;y_prob&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_prob</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">test_results</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">plot_precision_recall_vs_threshold</span><span class="p">(</span><span class="n">precisions</span><span class="p">,</span> <span class="n">recalls</span><span class="p">,</span> <span class="n">thresholds</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Modified from:</span>
<span class="sd">    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.89 </span>
<span class="sd">    and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Precision and Recall Scores as a function of the decision threshold&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thresholds</span><span class="p">,</span> <span class="n">precisions</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;b--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Precision&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thresholds</span><span class="p">,</span> <span class="n">recalls</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;g-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Recall&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Score&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Decision Threshold&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">precision_recall_threshold_plot</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;png&quot;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_roc_curve</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The ROC curve, modified from </span>
<span class="sd">    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91</span>
<span class="sd">    and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ROC Curve&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mf">0.005</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.005</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;False Positive Rate&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;True Positive Rate (Recall)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">roc_plot</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;png&quot;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">adjusted_classes</span><span class="p">(</span><span class="n">y_scores</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function adjusts class predictions based on the prediction threshold (t).</span>
<span class="sd">    Will only work for binary classification problems.</span>
<span class="sd">    and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">y</span> <span class="o">&gt;=</span> <span class="n">t</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">y_scores</span><span class="p">]</span>
</code></pre></div>
<p>The plots below show the tradeoffs between precision and recall, recall and false positive rate. The confusion matrix on the train set tells us that there is still more false negatives than false positives. We can choose a particular threshold in order to minimize false negatives, at some expense of false positive.</p>
<div class="highlight"><pre><span></span><code><span class="n">train_results</span> <span class="o">=</span> <span class="n">evaluate_train_test_set</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># CM</span>
<span class="n">cm_train</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">train_results</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">train_results</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">])</span>

<span class="c1">#### scores</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">train_results</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">train_results</span><span class="p">[</span><span class="s1">&#39;y_prob&#39;</span><span class="p">])</span>

<span class="c1">#### annotations</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;True Neg&quot;</span><span class="p">,</span> <span class="s2">&quot;False Pos&quot;</span><span class="p">,</span> <span class="s2">&quot;False Neg&quot;</span><span class="p">,</span> <span class="s2">&quot;True Pos&quot;</span><span class="p">]</span>
<span class="n">counts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;</span><span class="si">{0:0.0f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">cm_train</span><span class="o">.</span><span class="n">flatten</span><span class="p">()]</span>
<span class="n">percentages</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;</span><span class="si">{0:.2%}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">cm_train</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">cm_train</span><span class="p">)]</span>

<span class="c1">#### final annotations</span>
<span class="n">label</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">v1</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">v2</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">v3</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">,</span> <span class="n">v3</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">percentages</span><span class="p">)])</span>
<span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># heatmap</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">cm_train</span><span class="p">,</span>
    <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">vmax</span><span class="o">=</span><span class="mi">330</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;#fe4a49&quot;</span><span class="p">,</span> <span class="s2">&quot;#2ab7ca&quot;</span><span class="p">,</span> <span class="s2">&quot;#fed766&quot;</span><span class="p">,</span> <span class="s2">&quot;#59981A&quot;</span><span class="p">],</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">linecolor</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">,</span>
    <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">annot</span><span class="o">=</span><span class="n">label</span><span class="p">,</span>
    <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">},</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>



<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3500</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="mf">0.72</span><span class="p">,</span>
    <span class="mf">1.0</span><span class="p">,</span>
    <span class="s2">&quot;AUC: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">auc</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
    <span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;bold&quot;</span><span class="p">},</span>
<span class="p">)</span>

<span class="c1">## ticks and labels</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>


<span class="c1">## titles and text</span>
<span class="n">fig</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">,</span> <span class="s2">&quot;Train Set Confusion Matrix&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">22</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;bold&quot;</span><span class="p">},</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
    <span class="mi">0</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">,</span>
    <span class="sd">&quot;&quot;&quot;Training Set Confusion Matrix.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="s2">&quot;normal&quot;</span><span class="p">},</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span>
<span class="p">)</span>


<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">w_pad</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">h_pad</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">final_train_confusion_matrix</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># generate the precision recall curve</span>
<span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">pr_thresholds</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_curve</span><span class="p">(</span><span class="n">train_results</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">train_results</span><span class="p">[</span><span class="s1">&#39;y_prob&#39;</span><span class="p">])</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">roc_thresholds</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">train_results</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">train_results</span><span class="p">[</span><span class="s1">&#39;y_prob&#39;</span><span class="p">],</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># use the same p, r, thresholds that were previously calculated</span>
<span class="n">plot_precision_recall_vs_threshold</span><span class="p">(</span><span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">pr_thresholds</span><span class="p">)</span>
</code></pre></div>
<p>Based on the tradeoff plot above, a good threshold can be set at <span class="arithmatex">\(t = 0.35\)</span>, let us see how it performs with this threshold.</p>
<div class="highlight"><pre><span></span><code><span class="n">y_pred_adj</span> <span class="o">=</span> <span class="n">adjusted_classes</span><span class="p">(</span><span class="n">train_results</span><span class="p">[</span><span class="s2">&quot;y_prob&quot;</span><span class="p">],</span> <span class="n">t</span><span class="o">=</span><span class="mf">0.35</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">train_results</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="n">y_pred_adj</span><span class="p">),</span>
        <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;pred_neg&quot;</span><span class="p">,</span> <span class="s2">&quot;pred_pos&quot;</span><span class="p">],</span>
        <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;neg&quot;</span><span class="p">,</span> <span class="s2">&quot;pos&quot;</span><span class="p">],</span>
    <span class="p">)</span>
<span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">train_results</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred_adj</span><span class="p">))</span>
<span class="n">train_brier</span> <span class="o">=</span> <span class="n">train_results</span><span class="p">[</span><span class="s1">&#39;brier&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;train brier: </span><span class="si">{</span><span class="n">train_brier</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<p>The False Negatives reduced from 15 to 9, at the expense of increase False Positives from 6 to 14. We should take comfort that less patients are falsely classified as benign when in fact they need treatment. This is a tradeoff that we have to choose. The ROC curve below also paints a similar story, in order for you to have high recall, one must accept that there will more False Positives.</p>
<div class="highlight"><pre><span></span><code><span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="s1">&#39;recall_optimized&#39;</span><span class="p">)</span>
</code></pre></div>
<h1 id="evaluation-on-test-set">Evaluation on Test Set</h1>
<p>Ultimately, we are interested in finding the estimate of the generalization error of the model, and thus it is time for us to evaluate our model's performance on the "unseen" test set <span class="arithmatex">\(X_{\text{test}}\)</span> to get a good gauge on how well the model generalizes to unseen data. Take note that now everything has settled, we will use the exact hyperparameters to predict on test set, with the pre-defined threshold of 0.35.</p>
<div class="highlight"><pre><span></span><code><span class="n">test_results</span> <span class="o">=</span> <span class="n">evaluate_train_test_set</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">y_test_pred_adj</span> <span class="o">=</span> <span class="n">adjusted_classes</span><span class="p">(</span><span class="n">test_results</span><span class="p">[</span><span class="s1">&#39;y_prob&#39;</span><span class="p">],</span> <span class="n">t</span><span class="o">=</span><span class="mf">0.35</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_results</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">y_test_pred_adj</span><span class="p">),</span>
                   <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pred_neg&#39;</span><span class="p">,</span> <span class="s1">&#39;pred_pos&#39;</span><span class="p">],</span> 
                   <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;neg&#39;</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">]))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">test_roc</span> <span class="o">=</span> <span class="n">test_results</span><span class="p">[</span><span class="s1">&#39;roc&#39;</span><span class="p">]</span>
<span class="n">test_brier</span> <span class="o">=</span> <span class="n">test_results</span><span class="p">[</span><span class="s1">&#39;brier&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_roc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_brier</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">test_results</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_test_pred_adj</span><span class="p">))</span>
</code></pre></div>
<p>Using the same threshold we used on training set, we see that the False Negative is quite low. The overall ROC score is 0.9828, and the corresponding Brier Score is 0.04136, both seem reasonably well performing.</p>
<h1 id="bias-variance-tradeoff">Bias-Variance Tradeoff</h1>
<div class="highlight"><pre><span></span><code><span class="n">avg_expected_loss</span><span class="p">,</span> <span class="n">avg_bias</span><span class="p">,</span> <span class="n">avg_var</span> <span class="o">=</span> <span class="n">bias_variance_decomp</span><span class="p">(</span>
        <span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">],</span> <span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> 
        <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;0-1_loss&#39;</span><span class="p">,</span>
        <span class="n">random_seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average expected loss: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">avg_expected_loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average bias: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">avg_bias</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average variance: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">avg_var</span><span class="p">)</span>
</code></pre></div>
<p>We use the <code>mlxtend</code> library to estimate the Bias-Variance Tradeoff in our Logistic Regression model. The core idea behind this function is to use bagging and repeatedly sample from our training set so as to simulate that we are actually drawing samples from the "true" population over a distribution <span class="arithmatex">\(\mathcal{P}\)</span>. </p>
<hr />
<p>As expected, Logistic Regression being a linear model, its simplicity contributes to its high bias and low variance. </p>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        
          Made with
          <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
            Material for MkDocs
          </a>
        
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../../assets/javascripts/workers/search.8397ff9e.min.js", "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.1e84347e.min.js"></script>
      
        <script src="../../../javascript/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>