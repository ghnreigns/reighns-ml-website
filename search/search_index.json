{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Welcome to Hongnan G.'s website. Here you can follow my learning journey.","title":"Home"},{"location":"#welcome","text":"Welcome to Hongnan G.'s website. Here you can follow my learning journey.","title":"Welcome"},{"location":"about/","text":"I love to document my learning journey.","title":"Regression"},{"location":"getting_started/","text":"Setup Guide by Hongnan Gao Introduction This getting started guide is written to ensure I follow standard coding practices. Important To render Table of Content when compiling mkdocs serve , one must always have the following structure: # Type your article title here ## ... ## ... Notice that the top level header must be #, and subsequent nested headers should be ##. Setting up Environment Assuming VSCode setup, open up terminal/powershell in your respective system and type: code \"path to folder\" to open up VSCode. Virtual Environment In your IDE, you want to set up a virtual environment. # For Ubuntu sudo apt install python3.8 python3.8-venv python3-venv # For Mac pip3 install virtualenv You can activate the VM as follows: # Assuming Windows python -m venv venv_bcw . \\v env_bcw \\S cripts \\a ctivate python -m pip install --upgrade pip setuptools wheel # upgrade pip # Assuming Linux python3 -m venv venv_bcw source venv_bcw/bin/activate python -m pip install --upgrade pip setuptools wheel # upgrade pip # Assuming Mac virtualenv venv_bcw source venv_bcw/bin/activate python -m pip install --upgrade pip setuptools wheel # upgrade pip Setup and requirements Note For small projects, we can have requirements.txt and just run (venv_ml) pip install -r requirements.txt . For larger projects, we can follow the steps below. Create a file named setup.py and requirements.txt concurrently. The latter should have the libraries that one is interested in having for his project while the formal is a setup.py file where it contains the setup object which describes how to set up our package and it's dependencies. The first several lines cover metadata (name, description, etc.) and then we define the requirements. Here we're stating that we require a Python version equal to or above 3.8 and then passing in our required packages to install_requires. Finally, we define extra requirements that different types of users may require. This is a standard practice and more can be understood from madewithml.com. The user can now call the following commands to install the dependencies in their own virtual environment. pip install -e . # installs required packages only python -m pip install -e \".[dev]\" # installs required + dev packages python -m pip install -e \".[test]\" # installs required + test packages python -m pip install -e \".[docs_packages]\" # installs required documentation packages Important Something worth taking note is when you download PyTorch Library, there is a dependency link since we are downloading cuda directly, you may execute as such: pip install -e . -f https://download.pytorch.org/whl/torch_stable.html Command Line Something worth noting is we need to use dash instead of underscore when calling a function in command line. reighns_linear_regression regression-test --solver \"Batch Gradient Descent\" --num-epochs 500 Documentation Type Hints Mkdocs + Docstrings Copy paste the template from Goku in, most of what he use will be in mkdocs.yml file. Remember to create the mkdocs.yml in the root path. Then change accordingly the content inside mkdocs.yml , you can see my template that I have done up. Remember to run python -m pip install -e \".[docs_packages]\" to make sure you do have the packages. Along the way you need to create a few folders, follow the page tree in mkdocs.yml, everything should be created in docs/ folder. As an example, in our reighns-linear-regression folder, we want to show two scenarios: Scenario 1: I merely want a full markdown file to show on the website. In this case, in the \"watch\", we specify a path we want to watch in our docs/ folder. In this case, I created a documentation folder under docs/ so I specify that. Next in the docs/documentation/ folder I create a file called linear_regression.md where I dump all my markdown notes inside. Then in the nav tree in mkdocs.yml , specify nav: - Home: - Introduction: index.md - Getting started: getting_started.md - Detailed Notes: - Notes: documentation/linear_regression.md - Reference: documentation/reference_links.md Note that Home and Getting Started are optional but let us keep it for the sake of completeness. What you need to care is \"Detailed Notes\" and note the path I gave to them - which will point to the folders in docs/documentation/ . Scenario 2: I want a python file with detailed docstrings to appear in my static website. This is slightly more complicated. First if you want a new section of this you can create a section called code_reference , both under the nav above and also in the folder docs/ , meaning docs/code_reference/ must exist. Put it under watch as well. Now in docs/code_reference/ create a file named say linear_regression_from_scratch.md and put src.linear_regression inside, note that you should not have space in between. Misc Problems This is testing 1 footnotes. Failure sss. sss. How to show nbextensions properly https://stackoverflow.com/questions/49647705/jupyter-nbextensions-does-not-appear/50663099 Run Run config.py as this will create folders for you automatically. Setting up GitHub Pages and Mkdocs (Website) Update November 9th, 2021 You can skip most steps if you just fork this repository and follow the format. Welcome to my example website! This website uses MkDocs with the Material theme and an automated deployment workflow for publishing to GitHub Pages . This guide will help you create your own GitHub Pages website with this setup. If you're using Windows, you should run all the commands in this guide on a Windows Subsystem for Linux (WSL) terminal. Initializing your Website Repository First, create a new repository on GitHub. Make sure to skip the initialization step on the website \u2014 you will be initializing the contents of this repository with this template! Note down the Git remote for your new repository, you'll need it later when initializing your local copy of the repository. Next, download the website template and extract it: $ wget https://github.com/jansky/test-website/archive/refs/tags/template.tar.gz $ tar xvf template.tar.gz $ rm template.tar.gz Note If the wget command is not found, you can install it using apt-get: sudo apt-get install wget . This will create a new folder called test-website-template in your current directory with the website template contents. You may wish to rename this folder to something like my-website : $ mv test-website-template my-website Now you can initialize a Git repository with the website contents: $ cd my-website $ git init $ git remote add origin YOUR_GITHUB_REPOSITORY_REMOTE Website Configuration The configuration for your website is stored in mkdocs.yml in the repository root. You only need to change a few settings at the top of the file: 1 2 3 4 5 6 7 site_name : Example Website site_url : https://reighns92.github.io/test-website nav : - Home : index.md - About : about.md - Notebooks : notebooks_list.md ... First, update the site_name and site_url fields to be correct for your website. The URL format for GitHub pages websites is https://USERNAME.github.io/REPOSITORY-NAME As you add content to your website, you can also control the pages that appear on your website's navbar in the nav field. Each nav list element is of the form Link Text : filename.md For navbar links pointing to pages in your site, you should use a file path which is relative to the docs/ folder where all your website content is stored. You may also link to external pages and include sub-links. For more information, you can view the MkDocs nav documentation . GitHub Actions Configuration You also need to update the GitHub Actions deployment workflow with the name and e-mail address to use when the workflow pushes your built website to the gh-pages branch of your repository. In the file .github/workflows/deploy-website.yml , update lines 25 and 26 to reflect your account information: 22 23 24 25 26 27 ... - name : Push Build Website to gh-pages Branch run : | git config --global user.name 'YOUR NAME(Automated)' git config --global user.email 'YOUR-GITHUB-USERNAME@users.noreply.github.com' ... Setting Up Local Development MkDocs makes it easy to develop your website locally and see your changes in real time. To begin, set up and activate Python virtual environment for this project. Then, install the project dependencies: ( venv ) $ pip install -r requirements.txt MkDocs includes a small webserver that allows you to preview your website in your browser as you make changes. Whenever you save one of your source files, the website will be rebuilt and your browser will automatically refresh to show the new changes. You can start this development server using the serve command: ( venv ) $ mkdocs serve INFO - Building documentation... ... INFO - Documentation built in 0 .16 seconds INFO - [ 20 :09:07 ] Serving on http://127.0.0.1:8000/... ... If you copy and paste the URL given by MkDocs into your browser you will see your website preview. Adding Website Content Markdown files added under the docs/ folder will be converted to HTML pages on your website. This website template enables some helpful extensions for rendering of math in LaTeX style, admonitions such as notes and warnings, and code blocks with syntax highlighting. In addition, MkDocs also supports GitHub-flavored Markdown tables. To see examples of syntax for these elements, see the MkDocs website here . Deploying Your Changes When you are ready to deploy your website for the first time, make an initial commit and push to your GitHub remote: $ git add . $ git commit -a -m \"Initial Commit\" $ git push origin master -u Activate Workflow The GitHub Actions deployment workflow included with this template runs whenever you push to the master branch. This workflow will build your website using MkDocs and push the built website files to the gh-pages branch of your repository, which GitHub Pages can use to serve your website. Once you have pushed your changes, go to your repository page on GitHub and confirm that the GitHub Actions workflow has completed successfully (you should see a green checkmark next to the name of the most recent commit). Then, go to your repository settings page, and click on 'Pages'. You will see a section that will let you set the source for your GitHub Pages website. Click the box labelled 'None' and select the gh-pages branch. Leave the selectd folder at '/ (root)' and click 'Save'. Your website is now live! To push additional changes, simply commit and push to the master branch. The GitHub Actions deployment workflow will handle deploying your changes to GitHub Pages. Pandoc (Markdown Converter) More often than not, you will need to convert a jupyter notebook to markdown file for deployment (as markdown supports Admonitions in Mkdocs). Here is a way to do it, it is very convenient as it not only converts your notebook files to markdown, it also stores your output as images in a folder for you. This means any images rendered in notebook by matplotlib etc will now show up in markdown! brew install pandoc git clone https://github.com/jupyter/nbconvert.git cd nbconvert pip install -e . Then to convert, simply do the following: jupyter nbconvert --to markdown mynotebook.ipynb Pandoc (Wikitext to Markdown) From the solution here . We can do the following: pandoc --from mediawiki --to = markdown-definition_lists wiki.txt -o wiki.md where wiki.txt is a text file with wiki markup. Miscellaneous Problems Path Environment Often times, you will encounter a problem with the path environment when working with Windows especially. For example, when you do the following: jupyter nbconvert --to markdown mynotebook.ipynb then 'jupyter' is not recognized as an internal or external command, operable program or batch file. is the error message even though jupyter is installed. Usually, the shell will prompt a message to check PATH . Now go to Advanced System Settings and click on Environment Variables. You will see a list of environment variables. You can add a new environment variable by clicking on the plus sign in the System Variables . Add the path recommended by jupyter to the PATH variable. In my case, it is the obscure C:\\Users\\reighns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Scripts . Mkdocs Full page width Use this custom css code to make page full width. Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9","title":"Workflow"},{"location":"getting_started/#introduction","text":"This getting started guide is written to ensure I follow standard coding practices. Important To render Table of Content when compiling mkdocs serve , one must always have the following structure: # Type your article title here ## ... ## ... Notice that the top level header must be #, and subsequent nested headers should be ##.","title":"Introduction"},{"location":"getting_started/#setting-up-environment","text":"Assuming VSCode setup, open up terminal/powershell in your respective system and type: code \"path to folder\" to open up VSCode.","title":"Setting up Environment"},{"location":"getting_started/#virtual-environment","text":"In your IDE, you want to set up a virtual environment. # For Ubuntu sudo apt install python3.8 python3.8-venv python3-venv # For Mac pip3 install virtualenv You can activate the VM as follows: # Assuming Windows python -m venv venv_bcw . \\v env_bcw \\S cripts \\a ctivate python -m pip install --upgrade pip setuptools wheel # upgrade pip # Assuming Linux python3 -m venv venv_bcw source venv_bcw/bin/activate python -m pip install --upgrade pip setuptools wheel # upgrade pip # Assuming Mac virtualenv venv_bcw source venv_bcw/bin/activate python -m pip install --upgrade pip setuptools wheel # upgrade pip","title":"Virtual Environment"},{"location":"getting_started/#setup-and-requirements","text":"Note For small projects, we can have requirements.txt and just run (venv_ml) pip install -r requirements.txt . For larger projects, we can follow the steps below. Create a file named setup.py and requirements.txt concurrently. The latter should have the libraries that one is interested in having for his project while the formal is a setup.py file where it contains the setup object which describes how to set up our package and it's dependencies. The first several lines cover metadata (name, description, etc.) and then we define the requirements. Here we're stating that we require a Python version equal to or above 3.8 and then passing in our required packages to install_requires. Finally, we define extra requirements that different types of users may require. This is a standard practice and more can be understood from madewithml.com. The user can now call the following commands to install the dependencies in their own virtual environment. pip install -e . # installs required packages only python -m pip install -e \".[dev]\" # installs required + dev packages python -m pip install -e \".[test]\" # installs required + test packages python -m pip install -e \".[docs_packages]\" # installs required documentation packages Important Something worth taking note is when you download PyTorch Library, there is a dependency link since we are downloading cuda directly, you may execute as such: pip install -e . -f https://download.pytorch.org/whl/torch_stable.html","title":"Setup and requirements"},{"location":"getting_started/#command-line","text":"Something worth noting is we need to use dash instead of underscore when calling a function in command line. reighns_linear_regression regression-test --solver \"Batch Gradient Descent\" --num-epochs 500","title":"Command Line"},{"location":"getting_started/#documentation","text":"","title":"Documentation"},{"location":"getting_started/#type-hints","text":"","title":"Type Hints"},{"location":"getting_started/#mkdocs-docstrings","text":"Copy paste the template from Goku in, most of what he use will be in mkdocs.yml file. Remember to create the mkdocs.yml in the root path. Then change accordingly the content inside mkdocs.yml , you can see my template that I have done up. Remember to run python -m pip install -e \".[docs_packages]\" to make sure you do have the packages. Along the way you need to create a few folders, follow the page tree in mkdocs.yml, everything should be created in docs/ folder. As an example, in our reighns-linear-regression folder, we want to show two scenarios: Scenario 1: I merely want a full markdown file to show on the website. In this case, in the \"watch\", we specify a path we want to watch in our docs/ folder. In this case, I created a documentation folder under docs/ so I specify that. Next in the docs/documentation/ folder I create a file called linear_regression.md where I dump all my markdown notes inside. Then in the nav tree in mkdocs.yml , specify nav: - Home: - Introduction: index.md - Getting started: getting_started.md - Detailed Notes: - Notes: documentation/linear_regression.md - Reference: documentation/reference_links.md Note that Home and Getting Started are optional but let us keep it for the sake of completeness. What you need to care is \"Detailed Notes\" and note the path I gave to them - which will point to the folders in docs/documentation/ . Scenario 2: I want a python file with detailed docstrings to appear in my static website. This is slightly more complicated. First if you want a new section of this you can create a section called code_reference , both under the nav above and also in the folder docs/ , meaning docs/code_reference/ must exist. Put it under watch as well. Now in docs/code_reference/ create a file named say linear_regression_from_scratch.md and put src.linear_regression inside, note that you should not have space in between.","title":"Mkdocs + Docstrings"},{"location":"getting_started/#misc-problems","text":"This is testing 1 footnotes. Failure sss. sss. How to show nbextensions properly https://stackoverflow.com/questions/49647705/jupyter-nbextensions-does-not-appear/50663099","title":"Misc Problems"},{"location":"getting_started/#run","text":"Run config.py as this will create folders for you automatically.","title":"Run"},{"location":"getting_started/#setting-up-github-pages-and-mkdocs-website","text":"Update November 9th, 2021 You can skip most steps if you just fork this repository and follow the format. Welcome to my example website! This website uses MkDocs with the Material theme and an automated deployment workflow for publishing to GitHub Pages . This guide will help you create your own GitHub Pages website with this setup. If you're using Windows, you should run all the commands in this guide on a Windows Subsystem for Linux (WSL) terminal.","title":"Setting up GitHub Pages and Mkdocs (Website)"},{"location":"getting_started/#initializing-your-website-repository","text":"First, create a new repository on GitHub. Make sure to skip the initialization step on the website \u2014 you will be initializing the contents of this repository with this template! Note down the Git remote for your new repository, you'll need it later when initializing your local copy of the repository. Next, download the website template and extract it: $ wget https://github.com/jansky/test-website/archive/refs/tags/template.tar.gz $ tar xvf template.tar.gz $ rm template.tar.gz Note If the wget command is not found, you can install it using apt-get: sudo apt-get install wget . This will create a new folder called test-website-template in your current directory with the website template contents. You may wish to rename this folder to something like my-website : $ mv test-website-template my-website Now you can initialize a Git repository with the website contents: $ cd my-website $ git init $ git remote add origin YOUR_GITHUB_REPOSITORY_REMOTE","title":"Initializing your Website Repository"},{"location":"getting_started/#website-configuration","text":"The configuration for your website is stored in mkdocs.yml in the repository root. You only need to change a few settings at the top of the file: 1 2 3 4 5 6 7 site_name : Example Website site_url : https://reighns92.github.io/test-website nav : - Home : index.md - About : about.md - Notebooks : notebooks_list.md ... First, update the site_name and site_url fields to be correct for your website. The URL format for GitHub pages websites is https://USERNAME.github.io/REPOSITORY-NAME As you add content to your website, you can also control the pages that appear on your website's navbar in the nav field. Each nav list element is of the form Link Text : filename.md For navbar links pointing to pages in your site, you should use a file path which is relative to the docs/ folder where all your website content is stored. You may also link to external pages and include sub-links. For more information, you can view the MkDocs nav documentation .","title":"Website Configuration"},{"location":"getting_started/#github-actions-configuration","text":"You also need to update the GitHub Actions deployment workflow with the name and e-mail address to use when the workflow pushes your built website to the gh-pages branch of your repository. In the file .github/workflows/deploy-website.yml , update lines 25 and 26 to reflect your account information: 22 23 24 25 26 27 ... - name : Push Build Website to gh-pages Branch run : | git config --global user.name 'YOUR NAME(Automated)' git config --global user.email 'YOUR-GITHUB-USERNAME@users.noreply.github.com' ...","title":"GitHub Actions Configuration"},{"location":"getting_started/#setting-up-local-development","text":"MkDocs makes it easy to develop your website locally and see your changes in real time. To begin, set up and activate Python virtual environment for this project. Then, install the project dependencies: ( venv ) $ pip install -r requirements.txt MkDocs includes a small webserver that allows you to preview your website in your browser as you make changes. Whenever you save one of your source files, the website will be rebuilt and your browser will automatically refresh to show the new changes. You can start this development server using the serve command: ( venv ) $ mkdocs serve INFO - Building documentation... ... INFO - Documentation built in 0 .16 seconds INFO - [ 20 :09:07 ] Serving on http://127.0.0.1:8000/... ... If you copy and paste the URL given by MkDocs into your browser you will see your website preview.","title":"Setting Up Local Development"},{"location":"getting_started/#adding-website-content","text":"Markdown files added under the docs/ folder will be converted to HTML pages on your website. This website template enables some helpful extensions for rendering of math in LaTeX style, admonitions such as notes and warnings, and code blocks with syntax highlighting. In addition, MkDocs also supports GitHub-flavored Markdown tables. To see examples of syntax for these elements, see the MkDocs website here .","title":"Adding Website Content"},{"location":"getting_started/#deploying-your-changes","text":"When you are ready to deploy your website for the first time, make an initial commit and push to your GitHub remote: $ git add . $ git commit -a -m \"Initial Commit\" $ git push origin master -u","title":"Deploying Your Changes"},{"location":"getting_started/#activate-workflow","text":"The GitHub Actions deployment workflow included with this template runs whenever you push to the master branch. This workflow will build your website using MkDocs and push the built website files to the gh-pages branch of your repository, which GitHub Pages can use to serve your website. Once you have pushed your changes, go to your repository page on GitHub and confirm that the GitHub Actions workflow has completed successfully (you should see a green checkmark next to the name of the most recent commit). Then, go to your repository settings page, and click on 'Pages'. You will see a section that will let you set the source for your GitHub Pages website. Click the box labelled 'None' and select the gh-pages branch. Leave the selectd folder at '/ (root)' and click 'Save'. Your website is now live! To push additional changes, simply commit and push to the master branch. The GitHub Actions deployment workflow will handle deploying your changes to GitHub Pages.","title":"Activate Workflow"},{"location":"getting_started/#pandoc-markdown-converter","text":"More often than not, you will need to convert a jupyter notebook to markdown file for deployment (as markdown supports Admonitions in Mkdocs). Here is a way to do it, it is very convenient as it not only converts your notebook files to markdown, it also stores your output as images in a folder for you. This means any images rendered in notebook by matplotlib etc will now show up in markdown! brew install pandoc git clone https://github.com/jupyter/nbconvert.git cd nbconvert pip install -e . Then to convert, simply do the following: jupyter nbconvert --to markdown mynotebook.ipynb","title":"Pandoc (Markdown Converter)"},{"location":"getting_started/#pandoc-wikitext-to-markdown","text":"From the solution here . We can do the following: pandoc --from mediawiki --to = markdown-definition_lists wiki.txt -o wiki.md where wiki.txt is a text file with wiki markup.","title":"Pandoc (Wikitext to Markdown)"},{"location":"getting_started/#miscellaneous-problems","text":"","title":"Miscellaneous Problems"},{"location":"getting_started/#path-environment","text":"Often times, you will encounter a problem with the path environment when working with Windows especially. For example, when you do the following: jupyter nbconvert --to markdown mynotebook.ipynb then 'jupyter' is not recognized as an internal or external command, operable program or batch file. is the error message even though jupyter is installed. Usually, the shell will prompt a message to check PATH . Now go to Advanced System Settings and click on Environment Variables. You will see a list of environment variables. You can add a new environment variable by clicking on the plus sign in the System Variables . Add the path recommended by jupyter to the PATH variable. In my case, it is the obscure C:\\Users\\reighns\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Scripts .","title":"Path Environment"},{"location":"getting_started/#mkdocs","text":"","title":"Mkdocs"},{"location":"getting_started/#full-page-width","text":"Use this custom css code to make page full width. Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9","title":"Full page width"},{"location":"deep_learning/OOF%20and%20Ensembling/Forward%20Ensemble/Hill%20Climbing%20%28Forward%20Ensembling%29/","text":"Multi-Label Hill Climbing","title":"Hill Climbing (Forward Ensembling)"},{"location":"deep_learning/OOF%20and%20Ensembling/Forward%20Ensemble/Hill%20Climbing%20%28Forward%20Ensembling%29/#multi-label-hill-climbing","text":"","title":"Multi-Label Hill Climbing"},{"location":"deep_learning/OOF%20and%20Ensembling/Forward%20Ensemble/forward-selection-oof-ensemble-0-942-private/","text":"How To Ensemble OOF In this notebook, we learn how to use forward selection to ensemble OOF. First build lots of models using the same KFolds (i.e. use same seed ). Next save all the oof files as oof_XX.csv and submission files as sub_XX.csv where the oof and submission share the same XX number. Then save them in a Kaggle dataset and run the code below. The ensemble begins with the model of highest oof AUC. Next each other model is added one by one to see which additional model increases ensemble AUC the most. The best additional model is kept and the process is repeated until the ensemble AUC doesn't increase. Read OOF Files When i get more time, I will compete this table to describe all 39 models in this notebook. For now here are the ones that get selected: k CV LB read size crop size effNet ext data upsample misc name 1 0.910 0.950 384 384 B6 2018 no oof_100 3 0.916 0.946 384 384 B345 no no oof_108 8 0.935 0.949 768 512 B7 2018 1,1,1,1 oof_113 10 0.920 0.941 512 384 B5 2019 2018 10,0,0,0 oof_117 12 0.935 0.937 768 512 B6 2019 2018 3,3,0,0 oof_120 21 0.933 0.950 1024 512 B6 2018 2,2,2,2 oof_30 26 0.927 0.942 768 384 B4 2018 no oof_385 37 0.936 0.956 512 384 B5 2018 1,1,1,1 oof_67 import pandas as pd , numpy as np , os from sklearn.metrics import roc_auc_score import matplotlib.pyplot as plt PATH = '../input/melanoma-oof-and-sub/' FILES = os . listdir ( PATH ) OOF = np . sort ( [ f for f in FILES if 'oof' in f ] ) OOF_CSV = [ pd . read_csv ( PATH + k ) for k in OOF ] print ( 'We have %i oof files...' % len ( OOF )) print (); print ( OOF ) We have 39 oof files... ['oof_0.csv' 'oof_100.csv' 'oof_105.csv' 'oof_108.csv' 'oof_109.csv' 'oof_11.csv' 'oof_110.csv' 'oof_111.csv' 'oof_113.csv' 'oof_116.csv' 'oof_117.csv' 'oof_12.csv' 'oof_120.csv' 'oof_121.csv' 'oof_13.csv' 'oof_15.csv' 'oof_16.csv' 'oof_2.csv' 'oof_20.csv' 'oof_24.csv' 'oof_28.csv' 'oof_30.csv' 'oof_32.csv' 'oof_33.csv' 'oof_35.csv' 'oof_384.csv' 'oof_385.csv' 'oof_4.csv' 'oof_44.csv' 'oof_54.csv' 'oof_55.csv' 'oof_56.csv' 'oof_57.csv' 'oof_58.csv' 'oof_59.csv' 'oof_6.csv' 'oof_65.csv' 'oof_67.csv' 'oof_77.csv'] x = np . zeros (( len ( OOF_CSV [ 0 ]), len ( OOF ) )) for k in range ( len ( OOF )): x [:, k ] = OOF_CSV [ k ] . pred . values TRUE = OOF_CSV [ 0 ] . target . values all = [] for k in range ( x . shape [ 1 ]): auc = roc_auc_score ( OOF_CSV [ 0 ] . target , x [:, k ]) all . append ( auc ) print ( 'Model %i has OOF AUC = %.4f ' % ( k , auc )) m = [ np . argmax ( all )]; w = [] Model 0 has OOF AUC = 0.9038 Model 1 has OOF AUC = 0.9096 Model 2 has OOF AUC = 0.9116 Model 3 has OOF AUC = 0.9162 Model 4 has OOF AUC = 0.9231 Model 5 has OOF AUC = 0.9206 Model 6 has OOF AUC = 0.9234 Model 7 has OOF AUC = 0.9267 Model 8 has OOF AUC = 0.9353 Model 9 has OOF AUC = 0.9033 Model 10 has OOF AUC = 0.9199 Model 11 has OOF AUC = 0.9178 Model 12 has OOF AUC = 0.9347 Model 13 has OOF AUC = 0.9238 Model 14 has OOF AUC = 0.9182 Model 15 has OOF AUC = 0.9196 Model 16 has OOF AUC = 0.9160 Model 17 has OOF AUC = 0.9267 Model 18 has OOF AUC = 0.9260 Model 19 has OOF AUC = 0.9306 Model 20 has OOF AUC = 0.9310 Model 21 has OOF AUC = 0.9331 Model 22 has OOF AUC = 0.8979 Model 23 has OOF AUC = 0.9148 Model 24 has OOF AUC = 0.9216 Model 25 has OOF AUC = 0.9278 Model 26 has OOF AUC = 0.9267 Model 27 has OOF AUC = 0.9229 Model 28 has OOF AUC = 0.9277 Model 29 has OOF AUC = 0.9328 Model 30 has OOF AUC = 0.9243 Model 31 has OOF AUC = 0.9012 Model 32 has OOF AUC = 0.9129 Model 33 has OOF AUC = 0.9096 Model 34 has OOF AUC = 0.9188 Model 35 has OOF AUC = 0.9182 Model 36 has OOF AUC = 0.9215 Model 37 has OOF AUC = 0.9358 Model 38 has OOF AUC = 0.9325 Build OOF Ensemble. Maximize CV Score old = np . max ( all ); RES = 200 ; PATIENCE = 10 ; TOL = 0.0003 DUPLICATES = False print ( 'Ensemble AUC = %.4f by beginning with model %i ' % ( old , m [ 0 ])) print () for kk in range ( len ( OOF )): # BUILD CURRENT ENSEMBLE md = x [:, m [ 0 ]] for i , k in enumerate ( m [ 1 :]): md = w [ i ] * x [:, k ] + ( 1 - w [ i ]) * md # FIND MODEL TO ADD mx = 0 ; mx_k = 0 ; mx_w = 0 print ( 'Searching for best model to add... ' ) # TRY ADDING EACH MODEL for k in range ( x . shape [ 1 ]): print ( k , ', ' , end = '' ) if not DUPLICATES and ( k in m ): continue # EVALUATE ADDING MODEL K WITH WEIGHTS W bst_j = 0 ; bst = 0 ; ct = 0 for j in range ( RES ): tmp = j / RES * x [:, k ] + ( 1 - j / RES ) * md auc = roc_auc_score ( TRUE , tmp ) if auc > bst : bst = auc bst_j = j / RES else : ct += 1 if ct > PATIENCE : break if bst > mx : mx = bst mx_k = k mx_w = bst_j # STOP IF INCREASE IS LESS THAN TOL inc = mx - old if inc <= TOL : print (); print ( 'No increase. Stopping.' ) break # DISPLAY RESULTS print (); #print(kk,mx,mx_k,mx_w,'%.5f'%inc) print ( 'Ensemble AUC = %.4f after adding model %i with weight %.3f . Increase of %.4f ' % ( mx , mx_k , mx_w , inc )) print () old = mx ; m . append ( mx_k ); w . append ( mx_w ) Ensemble AUC = 0.9358 by beginning with model 37 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9426 after adding model 21 with weight 0.480. Increase of 0.0068 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9461 after adding model 3 with weight 0.565. Increase of 0.0035 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9475 after adding model 12 with weight 0.145. Increase of 0.0014 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9484 after adding model 1 with weight 0.300. Increase of 0.0009 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9490 after adding model 26 with weight 0.065. Increase of 0.0006 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9495 after adding model 8 with weight 0.125. Increase of 0.0005 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9499 after adding model 10 with weight 0.055. Increase of 0.0004 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , No increase. Stopping. print ( 'We are using models' , m ) print ( 'with weights' , w ) print ( 'and achieve ensemble AUC = %.4f ' % old ) We are using models [37, 21, 3, 12, 1, 26, 8, 10] with weights [0.48, 0.565, 0.145, 0.3, 0.065, 0.125, 0.055] and achieve ensemble AUC = 0.9499 md = x [:, m [ 0 ]] for i , k in enumerate ( m [ 1 :]): md = w [ i ] * x [:, k ] + ( 1 - w [ i ]) * md plt . hist ( md , bins = 100 ) plt . title ( 'Ensemble OOF predictions' ) plt . show () df = OOF_CSV [ 0 ] . copy () df . pred = md df . to_csv ( 'ensemble_oof.csv' , index = False ) Load SUB Files SUB = np . sort ( [ f for f in FILES if 'sub' in f ] ) SUB_CSV = [ pd . read_csv ( PATH + k ) for k in SUB ] print ( 'We have %i submission files...' % len ( SUB )) print (); print ( SUB ) We have 39 submission files... ['sub_0.csv' 'sub_100.csv' 'sub_105.csv' 'sub_108.csv' 'sub_109.csv' 'sub_11.csv' 'sub_110.csv' 'sub_111.csv' 'sub_113.csv' 'sub_116.csv' 'sub_117.csv' 'sub_12.csv' 'sub_120.csv' 'sub_121.csv' 'sub_13.csv' 'sub_15.csv' 'sub_16.csv' 'sub_2.csv' 'sub_20.csv' 'sub_24.csv' 'sub_28.csv' 'sub_30.csv' 'sub_32.csv' 'sub_33.csv' 'sub_35.csv' 'sub_384.csv' 'sub_385.csv' 'sub_4.csv' 'sub_44.csv' 'sub_54.csv' 'sub_55.csv' 'sub_56.csv' 'sub_57.csv' 'sub_58.csv' 'sub_59.csv' 'sub_6.csv' 'sub_65.csv' 'sub_67.csv' 'sub_77.csv'] # VERFIY THAT SUBMISSION FILES MATCH OOF FILES a = np . array ( [ int ( x . split ( '_' )[ 1 ] . split ( '.' )[ 0 ]) for x in SUB ] ) b = np . array ( [ int ( x . split ( '_' )[ 1 ] . split ( '.' )[ 0 ]) for x in OOF ] ) if len ( a ) != len ( b ): print ( 'ERROR submission files dont match oof files' ) else : for k in range ( len ( a )): if a [ k ] != b [ k ]: print ( 'ERROR submission files dont match oof files' ) y = np . zeros (( len ( SUB_CSV [ 0 ]), len ( SUB ) )) for k in range ( len ( SUB )): y [:, k ] = SUB_CSV [ k ] . target . values Build SUB Ensemble md2 = y [:, m [ 0 ]] for i , k in enumerate ( m [ 1 :]): md2 = w [ i ] * y [:, k ] + ( 1 - w [ i ]) * md2 plt . hist ( md2 , bins = 100 ) plt . show () df = SUB_CSV [ 0 ] . copy () df . target = md2 df . to_csv ( 'ensemble_sub.csv' , index = False )","title":"How To Ensemble OOF"},{"location":"deep_learning/OOF%20and%20Ensembling/Forward%20Ensemble/forward-selection-oof-ensemble-0-942-private/#how-to-ensemble-oof","text":"In this notebook, we learn how to use forward selection to ensemble OOF. First build lots of models using the same KFolds (i.e. use same seed ). Next save all the oof files as oof_XX.csv and submission files as sub_XX.csv where the oof and submission share the same XX number. Then save them in a Kaggle dataset and run the code below. The ensemble begins with the model of highest oof AUC. Next each other model is added one by one to see which additional model increases ensemble AUC the most. The best additional model is kept and the process is repeated until the ensemble AUC doesn't increase.","title":"How To Ensemble OOF"},{"location":"deep_learning/OOF%20and%20Ensembling/Forward%20Ensemble/forward-selection-oof-ensemble-0-942-private/#read-oof-files","text":"When i get more time, I will compete this table to describe all 39 models in this notebook. For now here are the ones that get selected: k CV LB read size crop size effNet ext data upsample misc name 1 0.910 0.950 384 384 B6 2018 no oof_100 3 0.916 0.946 384 384 B345 no no oof_108 8 0.935 0.949 768 512 B7 2018 1,1,1,1 oof_113 10 0.920 0.941 512 384 B5 2019 2018 10,0,0,0 oof_117 12 0.935 0.937 768 512 B6 2019 2018 3,3,0,0 oof_120 21 0.933 0.950 1024 512 B6 2018 2,2,2,2 oof_30 26 0.927 0.942 768 384 B4 2018 no oof_385 37 0.936 0.956 512 384 B5 2018 1,1,1,1 oof_67 import pandas as pd , numpy as np , os from sklearn.metrics import roc_auc_score import matplotlib.pyplot as plt PATH = '../input/melanoma-oof-and-sub/' FILES = os . listdir ( PATH ) OOF = np . sort ( [ f for f in FILES if 'oof' in f ] ) OOF_CSV = [ pd . read_csv ( PATH + k ) for k in OOF ] print ( 'We have %i oof files...' % len ( OOF )) print (); print ( OOF ) We have 39 oof files... ['oof_0.csv' 'oof_100.csv' 'oof_105.csv' 'oof_108.csv' 'oof_109.csv' 'oof_11.csv' 'oof_110.csv' 'oof_111.csv' 'oof_113.csv' 'oof_116.csv' 'oof_117.csv' 'oof_12.csv' 'oof_120.csv' 'oof_121.csv' 'oof_13.csv' 'oof_15.csv' 'oof_16.csv' 'oof_2.csv' 'oof_20.csv' 'oof_24.csv' 'oof_28.csv' 'oof_30.csv' 'oof_32.csv' 'oof_33.csv' 'oof_35.csv' 'oof_384.csv' 'oof_385.csv' 'oof_4.csv' 'oof_44.csv' 'oof_54.csv' 'oof_55.csv' 'oof_56.csv' 'oof_57.csv' 'oof_58.csv' 'oof_59.csv' 'oof_6.csv' 'oof_65.csv' 'oof_67.csv' 'oof_77.csv'] x = np . zeros (( len ( OOF_CSV [ 0 ]), len ( OOF ) )) for k in range ( len ( OOF )): x [:, k ] = OOF_CSV [ k ] . pred . values TRUE = OOF_CSV [ 0 ] . target . values all = [] for k in range ( x . shape [ 1 ]): auc = roc_auc_score ( OOF_CSV [ 0 ] . target , x [:, k ]) all . append ( auc ) print ( 'Model %i has OOF AUC = %.4f ' % ( k , auc )) m = [ np . argmax ( all )]; w = [] Model 0 has OOF AUC = 0.9038 Model 1 has OOF AUC = 0.9096 Model 2 has OOF AUC = 0.9116 Model 3 has OOF AUC = 0.9162 Model 4 has OOF AUC = 0.9231 Model 5 has OOF AUC = 0.9206 Model 6 has OOF AUC = 0.9234 Model 7 has OOF AUC = 0.9267 Model 8 has OOF AUC = 0.9353 Model 9 has OOF AUC = 0.9033 Model 10 has OOF AUC = 0.9199 Model 11 has OOF AUC = 0.9178 Model 12 has OOF AUC = 0.9347 Model 13 has OOF AUC = 0.9238 Model 14 has OOF AUC = 0.9182 Model 15 has OOF AUC = 0.9196 Model 16 has OOF AUC = 0.9160 Model 17 has OOF AUC = 0.9267 Model 18 has OOF AUC = 0.9260 Model 19 has OOF AUC = 0.9306 Model 20 has OOF AUC = 0.9310 Model 21 has OOF AUC = 0.9331 Model 22 has OOF AUC = 0.8979 Model 23 has OOF AUC = 0.9148 Model 24 has OOF AUC = 0.9216 Model 25 has OOF AUC = 0.9278 Model 26 has OOF AUC = 0.9267 Model 27 has OOF AUC = 0.9229 Model 28 has OOF AUC = 0.9277 Model 29 has OOF AUC = 0.9328 Model 30 has OOF AUC = 0.9243 Model 31 has OOF AUC = 0.9012 Model 32 has OOF AUC = 0.9129 Model 33 has OOF AUC = 0.9096 Model 34 has OOF AUC = 0.9188 Model 35 has OOF AUC = 0.9182 Model 36 has OOF AUC = 0.9215 Model 37 has OOF AUC = 0.9358 Model 38 has OOF AUC = 0.9325","title":"Read OOF Files"},{"location":"deep_learning/OOF%20and%20Ensembling/Forward%20Ensemble/forward-selection-oof-ensemble-0-942-private/#build-oof-ensemble-maximize-cv-score","text":"old = np . max ( all ); RES = 200 ; PATIENCE = 10 ; TOL = 0.0003 DUPLICATES = False print ( 'Ensemble AUC = %.4f by beginning with model %i ' % ( old , m [ 0 ])) print () for kk in range ( len ( OOF )): # BUILD CURRENT ENSEMBLE md = x [:, m [ 0 ]] for i , k in enumerate ( m [ 1 :]): md = w [ i ] * x [:, k ] + ( 1 - w [ i ]) * md # FIND MODEL TO ADD mx = 0 ; mx_k = 0 ; mx_w = 0 print ( 'Searching for best model to add... ' ) # TRY ADDING EACH MODEL for k in range ( x . shape [ 1 ]): print ( k , ', ' , end = '' ) if not DUPLICATES and ( k in m ): continue # EVALUATE ADDING MODEL K WITH WEIGHTS W bst_j = 0 ; bst = 0 ; ct = 0 for j in range ( RES ): tmp = j / RES * x [:, k ] + ( 1 - j / RES ) * md auc = roc_auc_score ( TRUE , tmp ) if auc > bst : bst = auc bst_j = j / RES else : ct += 1 if ct > PATIENCE : break if bst > mx : mx = bst mx_k = k mx_w = bst_j # STOP IF INCREASE IS LESS THAN TOL inc = mx - old if inc <= TOL : print (); print ( 'No increase. Stopping.' ) break # DISPLAY RESULTS print (); #print(kk,mx,mx_k,mx_w,'%.5f'%inc) print ( 'Ensemble AUC = %.4f after adding model %i with weight %.3f . Increase of %.4f ' % ( mx , mx_k , mx_w , inc )) print () old = mx ; m . append ( mx_k ); w . append ( mx_w ) Ensemble AUC = 0.9358 by beginning with model 37 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9426 after adding model 21 with weight 0.480. Increase of 0.0068 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9461 after adding model 3 with weight 0.565. Increase of 0.0035 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9475 after adding model 12 with weight 0.145. Increase of 0.0014 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9484 after adding model 1 with weight 0.300. Increase of 0.0009 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9490 after adding model 26 with weight 0.065. Increase of 0.0006 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9495 after adding model 8 with weight 0.125. Increase of 0.0005 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , Ensemble AUC = 0.9499 after adding model 10 with weight 0.055. Increase of 0.0004 Searching for best model to add... 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , No increase. Stopping. print ( 'We are using models' , m ) print ( 'with weights' , w ) print ( 'and achieve ensemble AUC = %.4f ' % old ) We are using models [37, 21, 3, 12, 1, 26, 8, 10] with weights [0.48, 0.565, 0.145, 0.3, 0.065, 0.125, 0.055] and achieve ensemble AUC = 0.9499 md = x [:, m [ 0 ]] for i , k in enumerate ( m [ 1 :]): md = w [ i ] * x [:, k ] + ( 1 - w [ i ]) * md plt . hist ( md , bins = 100 ) plt . title ( 'Ensemble OOF predictions' ) plt . show () df = OOF_CSV [ 0 ] . copy () df . pred = md df . to_csv ( 'ensemble_oof.csv' , index = False )","title":"Build OOF Ensemble. Maximize CV Score"},{"location":"deep_learning/OOF%20and%20Ensembling/Forward%20Ensemble/forward-selection-oof-ensemble-0-942-private/#load-sub-files","text":"SUB = np . sort ( [ f for f in FILES if 'sub' in f ] ) SUB_CSV = [ pd . read_csv ( PATH + k ) for k in SUB ] print ( 'We have %i submission files...' % len ( SUB )) print (); print ( SUB ) We have 39 submission files... ['sub_0.csv' 'sub_100.csv' 'sub_105.csv' 'sub_108.csv' 'sub_109.csv' 'sub_11.csv' 'sub_110.csv' 'sub_111.csv' 'sub_113.csv' 'sub_116.csv' 'sub_117.csv' 'sub_12.csv' 'sub_120.csv' 'sub_121.csv' 'sub_13.csv' 'sub_15.csv' 'sub_16.csv' 'sub_2.csv' 'sub_20.csv' 'sub_24.csv' 'sub_28.csv' 'sub_30.csv' 'sub_32.csv' 'sub_33.csv' 'sub_35.csv' 'sub_384.csv' 'sub_385.csv' 'sub_4.csv' 'sub_44.csv' 'sub_54.csv' 'sub_55.csv' 'sub_56.csv' 'sub_57.csv' 'sub_58.csv' 'sub_59.csv' 'sub_6.csv' 'sub_65.csv' 'sub_67.csv' 'sub_77.csv'] # VERFIY THAT SUBMISSION FILES MATCH OOF FILES a = np . array ( [ int ( x . split ( '_' )[ 1 ] . split ( '.' )[ 0 ]) for x in SUB ] ) b = np . array ( [ int ( x . split ( '_' )[ 1 ] . split ( '.' )[ 0 ]) for x in OOF ] ) if len ( a ) != len ( b ): print ( 'ERROR submission files dont match oof files' ) else : for k in range ( len ( a )): if a [ k ] != b [ k ]: print ( 'ERROR submission files dont match oof files' ) y = np . zeros (( len ( SUB_CSV [ 0 ]), len ( SUB ) )) for k in range ( len ( SUB )): y [:, k ] = SUB_CSV [ k ] . target . values","title":"Load SUB Files"},{"location":"deep_learning/OOF%20and%20Ensembling/Forward%20Ensemble/forward-selection-oof-ensemble-0-942-private/#build-sub-ensemble","text":"md2 = y [:, m [ 0 ]] for i , k in enumerate ( m [ 1 :]): md2 = w [ i ] * y [:, k ] + ( 1 - w [ i ]) * md2 plt . hist ( md2 , bins = 100 ) plt . show () df = SUB_CSV [ 0 ] . copy () df . target = md2 df . to_csv ( 'ensemble_sub.csv' , index = False )","title":"Build SUB Ensemble"},{"location":"deep_learning/OOF%20and%20Ensembling/General%20Ensembling%20Techniques/cassava-leaf-disease-finding-final-ensembles/","text":"Finding good ensembles to submit in competition We tried different types of ensembles (e.g., means and meta learners). Our best submissions used a stacked mean approach and weights found via an optimization. These submissions scored around 91.3% on the public and also private leaderboard. import pandas as pd import numpy as np from functools import partial import os import random import joblib import json from tqdm import tqdm from PIL import Image from sklearn.metrics import accuracy_score from scipy.optimize import differential_evolution import tensorflow as tf from tensorflow import keras import gc from functools import reduce from itertools import combinations , chain from tqdm import tqdm from sklearn.model_selection import KFold from itertools import chain import warnings warnings . filterwarnings ( \"ignore\" ) Loading Out-of-Fold Predictions for some of our tested models oof_predictions_v3 = joblib . load ( \"../input/cassava-leaf-disease-ensemble-tests/oof_v04 (1).pkl\" ) oof_predictions_v3 . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id label resnext b5 mobilenet vit2020 b4 vit2019 b3 0 1000015157.jpg 0 [0.53539234, 0.08993052, 0.032574702, 0.010111... [0.17068666, 0.28306848, 0.40483505, 0.0044726... [0.7490078, 0.014262087, 0.005231139, 0.000635... [0.42902824, 0.045752272, 0.31388378, 0.005485... [0.29917482, 0.16379356, 0.2504335, 0.06966855... [0.4443596, 0.21404053, 0.24348898, 0.01078206... [0.12975824, 0.41058695, 0.1588993, 0.02674331... 1 1000201771.jpg 3 [3.361342e-07, 5.867391e-07, 5.65951e-05, 0.99... [2.970924e-05, 0.0012548971, 1.7456967e-05, 0.... [0.00012562818, 0.00018958043, 0.0020241379, 0... [1.8922072e-05, 7.141115e-05, 3.6473248e-05, 0... [0.017034736, 0.033697814, 0.028847465, 0.8960... [8.799362e-06, 4.8696966e-05, 1.5933587e-05, 0... [0.020895468, 0.025896251, 0.037321843, 0.8942... 2 100042118.jpg 1 [0.005370396, 0.07950499, 0.017187783, 0.10000... [0.0038973298, 0.12563738, 0.008966217, 0.0198... [0.019499786, 0.06108744, 0.005322082, 0.21714... [0.0003110762, 0.00180416, 0.02432872, 0.00174... [0.054800056, 0.3077832, 0.08947, 0.08236225, ... [0.00047566573, 0.0014882263, 0.007440664, 0.0... [0.020274838, 0.11426823, 0.034628984, 0.07325... Mean and Stacked Mean combinations We used itertools.combinations to check all possible combinations for mean ensembles. In addition to the individual models, we also combined some models in advance. This was an easy way to try out the different combinations including diverse levels. In most cases, the calculated cross-validation scores were quite close to the public leaderboard results. Even after the final submission, the combinations proved to be very stable. # Build some promising stacks for evaluation oof_predictions_v3 . loc [:, \"vits\" ] = oof_predictions_v3 . apply ( lambda x : [ np . mean ( v ) for v in zip ( x [ \"vit2019\" ], x [ \"vit2020\" ])], axis = 1 ) oof_predictions_v3 . loc [:, \"vit_resnext\" ] = oof_predictions_v3 . apply ( lambda x : [ np . mean ( v ) for v in zip ( x [ \"vit2020\" ], x [ \"resnext\" ])], axis = 1 ) columns = oof_predictions_v3 . columns [ 2 :] . tolist () combined = [] for i in range ( len ( columns )): combined . append ( list ( combinations ( columns , i + 1 ))) def evaluate_ensemble ( df , columns ): return df [[ * columns ]] . apply ( lambda x : np . argmax ([ np . sum ( v ) for v in zip ( * [ x [ c ] for c in columns ])]), axis = 1 ) . values results = dict () with tqdm ( total = len ( list ( chain ( * combined )))) as process_bar : for c in list ( chain ( * combined )): process_bar . update ( 1 ) results [ c ] = accuracy_score ( oof_predictions_v3 . label . values , evaluate_ensemble ( oof_predictions_v3 , c )) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 511/511 [19:03<00:00, 2.24s/it] # Get top-50 combinations { k : results [ k ] for k in sorted ( results , key = results . get , reverse = True )[ 0 : 50 ]} {('mobilenet', 'b4', 'vit_resnext'): 0.9103145300743095, ('mobilenet', 'b3', 'vit_resnext'): 0.9092863485535355, ('mobilenet', 'b4', 'b3', 'vit_resnext'): 0.9092863485535355, ('mobilenet', 'b4', 'b3'): 0.909239613029864, ('b5', 'mobilenet', 'b4'): 0.9091928775061925, ('mobilenet', 'vit_resnext'): 0.9090994064588493, ('b5', 'mobilenet', 'vit_resnext'): 0.9089591998878347, ('b5', 'mobilenet', 'b3'): 0.9087722577931485, ('b5', 'mobilenet', 'b4', 'b3'): 0.908632051222134, ('b5', 'mobilenet', 'b4', 'vit_resnext'): 0.9085385801747908, ('b5', 'mobilenet', 'b3', 'vit_resnext'): 0.9085385801747908, ('mobilenet', 'vits'): 0.9082581670327616, ('mobilenet', 'b4', 'vits'): 0.9081646959854185, ('resnext', 'b5', 'mobilenet', 'b4'): 0.9081646959854185, ('resnext', 'mobilenet', 'b4'): 0.9080712249380755, ('b5', 'mobilenet', 'b4', 'b3', 'vit_resnext'): 0.9080244894144038, ('resnext', 'mobilenet', 'b4', 'vits'): 0.9079777538907323, ('resnext', 'mobilenet', 'b4', 'b3'): 0.9079310183670608, ('mobilenet', 'b3', 'vits'): 0.9076973407487031, ('resnext', 'mobilenet', 'b4', 'vit2019'): 0.9076973407487031, ('mobilenet', 'b4', 'b3', 'vits'): 0.90760386970136, ('b5', 'mobilenet', 'b3', 'vits'): 0.9075571341776885, ('b5', 'mobilenet'): 0.907510398654017, ('resnext', 'b5', 'mobilenet'): 0.907510398654017, ('b5', 'mobilenet', 'b4', 'vits'): 0.9074636631303453, ('resnext', 'mobilenet', 'vits'): 0.9073234565593308, ('resnext', 'b5', 'mobilenet', 'b3'): 0.9073234565593308, ('resnext', 'b5', 'mobilenet', 'b4', 'vit2019'): 0.9073234565593308, ('mobilenet', 'b3'): 0.9072299855119876, ('b5', 'mobilenet', 'vits'): 0.9072299855119876, ('mobilenet', 'vit2020', 'b4'): 0.9072299855119876, ('resnext', 'b5', 'mobilenet', 'b4', 'vits'): 0.9072299855119876, ('resnext', 'b5', 'mobilenet', 'vit_resnext'): 0.9071832499883161, ('resnext', 'b5', 'mobilenet', 'b4', 'vit_resnext'): 0.9071365144646446, ('b5', 'mobilenet', 'b4', 'b3', 'vits'): 0.9071365144646446, ('resnext', 'b5', 'mobilenet', 'vits'): 0.90699630789363, ('resnext', 'mobilenet', 'vit2020', 'b4'): 0.90699630789363, ('mobilenet', 'vit2020', 'b4', 'b3'): 0.90699630789363, ('resnext', 'mobilenet', 'b3'): 0.9069495723699584, ('resnext', 'b5', 'mobilenet', 'vit2019'): 0.9068561013226153, ('b5', 'mobilenet', 'vit2020', 'b4'): 0.9068561013226153, ('mobilenet', 'b4'): 0.9068093657989438, ('b5', 'mobilenet', 'vit2020'): 0.9068093657989438, ('resnext', 'b5', 'mobilenet', 'b3', 'vit_resnext'): 0.9068093657989438, ('b5', 'mobilenet', 'vit2019', 'b3'): 0.9067626302752723, ('resnext', 'mobilenet', 'b4', 'b3', 'vits'): 0.9067158947516006, ('resnext', 'b5', 'mobilenet', 'vit2020', 'b3'): 0.9066691592279291, ('resnext', 'mobilenet', 'vit2020', 'b4', 'b3'): 0.9066691592279291, ('mobilenet', 'b4', 'vit2019', 'b3'): 0.9066224237042576, ('mobilenet', 'b4', 'vit2019', 'vit_resnext'): 0.9066224237042576} Differential Evolution Another technique that we also used to obtain good and stable results in the rankings was a prior optimization of the weights of the softmax logits of each model. For optimization we used the Scipy differential_evolution method . considered_models = oof_predictions_v3 [[ \"image_id\" , \"label\" , \"b4\" , \"vit2020\" , \"resnext\" , \"mobilenet\" ]] kfold = KFold ( n_splits = 4 ) yhats = considered_models . iloc [:, 2 :] . values y = considered_models . label . values n_models = yhats . shape [ 1 ] accuracy = [] for fold , ( train_idx , test_idx ) in enumerate ( kfold . split ( yhats , y )): print ( f \"Iteration { fold + 1 } \" ) weights = np . array ([ 1.0 / n_models for _ in range ( n_models )]) bounds = [( 0.0 , 1.0 ) for _ in range ( n_models )] minimizeargs = ( np . take ( yhats , train_idx , axis = 0 ), np . take ( y , train_idx , axis = 0 )) def calculate_accuracy ( y_true , y_pred ): return np . average ( y_true == y_pred ) def loss_func ( weights , Yhat , Y ): w = np . mean ( weights * Yhat , axis = 1 ) return 1 - calculate_accuracy ( Y , list ( map ( lambda x : np . argmax ( x ), w ))) sol = differential_evolution ( loss_func , bounds , minimizeargs , maxiter = 20 , tol = 1e-5 , disp = True , seed = 8 ) # Calculate oof accuracy of optimized weights oof_accuracy = calculate_accuracy ( np . take ( y , test_idx , axis = 0 ), list ( map ( lambda x : np . argmax ( x ), np . mean ( np . take ( yhats , test_idx , axis = 0 ) * sol . x , axis = 1 )))) print ( f \" { oof_accuracy } \" ) accuracy . append (( sol . x , oof_accuracy )) #weights for ensembles of four different models accuracy","title":"Finding good ensembles to submit in competition"},{"location":"deep_learning/OOF%20and%20Ensembling/General%20Ensembling%20Techniques/cassava-leaf-disease-finding-final-ensembles/#finding-good-ensembles-to-submit-in-competition","text":"We tried different types of ensembles (e.g., means and meta learners). Our best submissions used a stacked mean approach and weights found via an optimization. These submissions scored around 91.3% on the public and also private leaderboard. import pandas as pd import numpy as np from functools import partial import os import random import joblib import json from tqdm import tqdm from PIL import Image from sklearn.metrics import accuracy_score from scipy.optimize import differential_evolution import tensorflow as tf from tensorflow import keras import gc from functools import reduce from itertools import combinations , chain from tqdm import tqdm from sklearn.model_selection import KFold from itertools import chain import warnings warnings . filterwarnings ( \"ignore\" )","title":"Finding good ensembles to submit in competition"},{"location":"deep_learning/OOF%20and%20Ensembling/General%20Ensembling%20Techniques/cassava-leaf-disease-finding-final-ensembles/#loading-out-of-fold-predictions-for-some-of-our-tested-models","text":"oof_predictions_v3 = joblib . load ( \"../input/cassava-leaf-disease-ensemble-tests/oof_v04 (1).pkl\" ) oof_predictions_v3 . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_id label resnext b5 mobilenet vit2020 b4 vit2019 b3 0 1000015157.jpg 0 [0.53539234, 0.08993052, 0.032574702, 0.010111... [0.17068666, 0.28306848, 0.40483505, 0.0044726... [0.7490078, 0.014262087, 0.005231139, 0.000635... [0.42902824, 0.045752272, 0.31388378, 0.005485... [0.29917482, 0.16379356, 0.2504335, 0.06966855... [0.4443596, 0.21404053, 0.24348898, 0.01078206... [0.12975824, 0.41058695, 0.1588993, 0.02674331... 1 1000201771.jpg 3 [3.361342e-07, 5.867391e-07, 5.65951e-05, 0.99... [2.970924e-05, 0.0012548971, 1.7456967e-05, 0.... [0.00012562818, 0.00018958043, 0.0020241379, 0... [1.8922072e-05, 7.141115e-05, 3.6473248e-05, 0... [0.017034736, 0.033697814, 0.028847465, 0.8960... [8.799362e-06, 4.8696966e-05, 1.5933587e-05, 0... [0.020895468, 0.025896251, 0.037321843, 0.8942... 2 100042118.jpg 1 [0.005370396, 0.07950499, 0.017187783, 0.10000... [0.0038973298, 0.12563738, 0.008966217, 0.0198... [0.019499786, 0.06108744, 0.005322082, 0.21714... [0.0003110762, 0.00180416, 0.02432872, 0.00174... [0.054800056, 0.3077832, 0.08947, 0.08236225, ... [0.00047566573, 0.0014882263, 0.007440664, 0.0... [0.020274838, 0.11426823, 0.034628984, 0.07325...","title":"Loading Out-of-Fold Predictions for some of our tested models"},{"location":"deep_learning/OOF%20and%20Ensembling/General%20Ensembling%20Techniques/cassava-leaf-disease-finding-final-ensembles/#mean-and-stacked-mean-combinations","text":"We used itertools.combinations to check all possible combinations for mean ensembles. In addition to the individual models, we also combined some models in advance. This was an easy way to try out the different combinations including diverse levels. In most cases, the calculated cross-validation scores were quite close to the public leaderboard results. Even after the final submission, the combinations proved to be very stable. # Build some promising stacks for evaluation oof_predictions_v3 . loc [:, \"vits\" ] = oof_predictions_v3 . apply ( lambda x : [ np . mean ( v ) for v in zip ( x [ \"vit2019\" ], x [ \"vit2020\" ])], axis = 1 ) oof_predictions_v3 . loc [:, \"vit_resnext\" ] = oof_predictions_v3 . apply ( lambda x : [ np . mean ( v ) for v in zip ( x [ \"vit2020\" ], x [ \"resnext\" ])], axis = 1 ) columns = oof_predictions_v3 . columns [ 2 :] . tolist () combined = [] for i in range ( len ( columns )): combined . append ( list ( combinations ( columns , i + 1 ))) def evaluate_ensemble ( df , columns ): return df [[ * columns ]] . apply ( lambda x : np . argmax ([ np . sum ( v ) for v in zip ( * [ x [ c ] for c in columns ])]), axis = 1 ) . values results = dict () with tqdm ( total = len ( list ( chain ( * combined )))) as process_bar : for c in list ( chain ( * combined )): process_bar . update ( 1 ) results [ c ] = accuracy_score ( oof_predictions_v3 . label . values , evaluate_ensemble ( oof_predictions_v3 , c )) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 511/511 [19:03<00:00, 2.24s/it] # Get top-50 combinations { k : results [ k ] for k in sorted ( results , key = results . get , reverse = True )[ 0 : 50 ]} {('mobilenet', 'b4', 'vit_resnext'): 0.9103145300743095, ('mobilenet', 'b3', 'vit_resnext'): 0.9092863485535355, ('mobilenet', 'b4', 'b3', 'vit_resnext'): 0.9092863485535355, ('mobilenet', 'b4', 'b3'): 0.909239613029864, ('b5', 'mobilenet', 'b4'): 0.9091928775061925, ('mobilenet', 'vit_resnext'): 0.9090994064588493, ('b5', 'mobilenet', 'vit_resnext'): 0.9089591998878347, ('b5', 'mobilenet', 'b3'): 0.9087722577931485, ('b5', 'mobilenet', 'b4', 'b3'): 0.908632051222134, ('b5', 'mobilenet', 'b4', 'vit_resnext'): 0.9085385801747908, ('b5', 'mobilenet', 'b3', 'vit_resnext'): 0.9085385801747908, ('mobilenet', 'vits'): 0.9082581670327616, ('mobilenet', 'b4', 'vits'): 0.9081646959854185, ('resnext', 'b5', 'mobilenet', 'b4'): 0.9081646959854185, ('resnext', 'mobilenet', 'b4'): 0.9080712249380755, ('b5', 'mobilenet', 'b4', 'b3', 'vit_resnext'): 0.9080244894144038, ('resnext', 'mobilenet', 'b4', 'vits'): 0.9079777538907323, ('resnext', 'mobilenet', 'b4', 'b3'): 0.9079310183670608, ('mobilenet', 'b3', 'vits'): 0.9076973407487031, ('resnext', 'mobilenet', 'b4', 'vit2019'): 0.9076973407487031, ('mobilenet', 'b4', 'b3', 'vits'): 0.90760386970136, ('b5', 'mobilenet', 'b3', 'vits'): 0.9075571341776885, ('b5', 'mobilenet'): 0.907510398654017, ('resnext', 'b5', 'mobilenet'): 0.907510398654017, ('b5', 'mobilenet', 'b4', 'vits'): 0.9074636631303453, ('resnext', 'mobilenet', 'vits'): 0.9073234565593308, ('resnext', 'b5', 'mobilenet', 'b3'): 0.9073234565593308, ('resnext', 'b5', 'mobilenet', 'b4', 'vit2019'): 0.9073234565593308, ('mobilenet', 'b3'): 0.9072299855119876, ('b5', 'mobilenet', 'vits'): 0.9072299855119876, ('mobilenet', 'vit2020', 'b4'): 0.9072299855119876, ('resnext', 'b5', 'mobilenet', 'b4', 'vits'): 0.9072299855119876, ('resnext', 'b5', 'mobilenet', 'vit_resnext'): 0.9071832499883161, ('resnext', 'b5', 'mobilenet', 'b4', 'vit_resnext'): 0.9071365144646446, ('b5', 'mobilenet', 'b4', 'b3', 'vits'): 0.9071365144646446, ('resnext', 'b5', 'mobilenet', 'vits'): 0.90699630789363, ('resnext', 'mobilenet', 'vit2020', 'b4'): 0.90699630789363, ('mobilenet', 'vit2020', 'b4', 'b3'): 0.90699630789363, ('resnext', 'mobilenet', 'b3'): 0.9069495723699584, ('resnext', 'b5', 'mobilenet', 'vit2019'): 0.9068561013226153, ('b5', 'mobilenet', 'vit2020', 'b4'): 0.9068561013226153, ('mobilenet', 'b4'): 0.9068093657989438, ('b5', 'mobilenet', 'vit2020'): 0.9068093657989438, ('resnext', 'b5', 'mobilenet', 'b3', 'vit_resnext'): 0.9068093657989438, ('b5', 'mobilenet', 'vit2019', 'b3'): 0.9067626302752723, ('resnext', 'mobilenet', 'b4', 'b3', 'vits'): 0.9067158947516006, ('resnext', 'b5', 'mobilenet', 'vit2020', 'b3'): 0.9066691592279291, ('resnext', 'mobilenet', 'vit2020', 'b4', 'b3'): 0.9066691592279291, ('mobilenet', 'b4', 'vit2019', 'b3'): 0.9066224237042576, ('mobilenet', 'b4', 'vit2019', 'vit_resnext'): 0.9066224237042576}","title":"Mean and Stacked Mean combinations"},{"location":"deep_learning/OOF%20and%20Ensembling/General%20Ensembling%20Techniques/cassava-leaf-disease-finding-final-ensembles/#differential-evolution","text":"Another technique that we also used to obtain good and stable results in the rankings was a prior optimization of the weights of the softmax logits of each model. For optimization we used the Scipy differential_evolution method . considered_models = oof_predictions_v3 [[ \"image_id\" , \"label\" , \"b4\" , \"vit2020\" , \"resnext\" , \"mobilenet\" ]] kfold = KFold ( n_splits = 4 ) yhats = considered_models . iloc [:, 2 :] . values y = considered_models . label . values n_models = yhats . shape [ 1 ] accuracy = [] for fold , ( train_idx , test_idx ) in enumerate ( kfold . split ( yhats , y )): print ( f \"Iteration { fold + 1 } \" ) weights = np . array ([ 1.0 / n_models for _ in range ( n_models )]) bounds = [( 0.0 , 1.0 ) for _ in range ( n_models )] minimizeargs = ( np . take ( yhats , train_idx , axis = 0 ), np . take ( y , train_idx , axis = 0 )) def calculate_accuracy ( y_true , y_pred ): return np . average ( y_true == y_pred ) def loss_func ( weights , Yhat , Y ): w = np . mean ( weights * Yhat , axis = 1 ) return 1 - calculate_accuracy ( Y , list ( map ( lambda x : np . argmax ( x ), w ))) sol = differential_evolution ( loss_func , bounds , minimizeargs , maxiter = 20 , tol = 1e-5 , disp = True , seed = 8 ) # Calculate oof accuracy of optimized weights oof_accuracy = calculate_accuracy ( np . take ( y , test_idx , axis = 0 ), list ( map ( lambda x : np . argmax ( x ), np . mean ( np . take ( yhats , test_idx , axis = 0 ) * sol . x , axis = 1 )))) print ( f \" { oof_accuracy } \" ) accuracy . append (( sol . x , oof_accuracy )) #weights for ensembles of four different models accuracy","title":"Differential Evolution"},{"location":"deep_learning/OOF%20and%20Ensembling/General%20Ensembling%20Techniques/improve-blending-using-rankdata-ensemble%20%281%29/","text":"In this notebook, we make an attempt to improve our blending(Or ensembling) by using ranks instead of absolute numbers. Reasoning behind this logic is that the ROC-AUC metric used here only cares about the rank of the prediction(Explained Below) # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os # for dirname, _, filenames in os.walk('/kaggle/input'): # for filename in filenames: # print(os.path.join(dirname, filename)) # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session os . listdir ( '../input/efficientnets/' ) PATH = 'input/efficientnets' dfs = [] i = 0 for df_loc in os . listdir ( '../input/efficientnets/' ): print ( '../input/efficientnets/ {} ' . format ( df_loc )) df = pd . read_csv ( '../input/efficientnets/ {} ' . format ( df_loc )) # df.head() dfs . append ( df ) # dfs ../input/efficientnets/submission_b7_0.888.csv ../input/efficientnets/submission_b5.csv ../input/efficientnets/submission_b7.csv ../input/efficientnets/submission_b5 (1).csv Dmytro Danevskyi wrote: The target metric in this competition is based on ranks rather than on actual values. That means that as long as the order of your values is fixed, the metric will stay the same. To illustrate: target = [1, 0, 1, 1, 0] preds = [0.5, 0.25, 0.2, 0.3, 0.1] metric = roc_auc_score(target, preds) # 0.833 target = [1, 0, 1, 1, 0] preds = [0.7, 0.15, 0.1, 0.2, 0.05] metric = roc_auc_score(target, preds) # 0.833 As you can see, only the rank of the predictions matters. Not the actual value. That means that two different models that give the same score could actually output completely different values. They are not even required to be in (0, 1) range! target = [1, 0, 1, 1, 0] preds = [100, 25, 20, 30, 10] metric = roc_auc_score(target, preds) # 0.833 Then, if you will try to average the predictions of two non-calibrated models, you might observe that the score is not necessarily getting better and in some cases, it could become even worse! This happens because the prediction scales of these two models are not directly comparable because of the aforementioned issue. How this can be fixed? One simple solution is to bring the predictions to the same scale, e.g. with scipy.stats.rankdata function. This will turn scores into ranks, i.e. [0.7, 0.15, 0.1, 0.2, 0.05] will be turned into [5, 3, 2, 4, 1] . After this, the predictions could be blended. Note that this not always lead to better results and is highly dependent on which exactly models are blended, how strong is the bias, and so on. To illustrate, I decided to naively blend my best scoring model (ResNet18) that gives 0.914 on the public leaderboard with the best-scoring public kernel available (0.927) and I got 0.925. After I preprocessed both predictions with rankdata , my score improved to 0.933. Taking advantage of this information, let's try blending using rank (Not absolute values) Previous score was 0.888 from scipy.stats import rankdata for i in range ( 4 ) : dfs [ i ][ 'target' ] = rankdata ( dfs [ i ][ 'target' ], method = 'min' ) # dfs[0] dfs [ 0 ][ 'target' ] = ( dfs [ 0 ][ 'target' ] + dfs [ 1 ][ 'target' ] + dfs [ 2 ][ 'target' ] + dfs [ 3 ][ 'target' ]) / 4 dfs [ 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_name target 0 ISIC_0052060 4152.25 1 ISIC_0052349 2169.75 2 ISIC_0058510 5413.00 3 ISIC_0073313 3864.00 4 ISIC_0073502 7222.75 ... ... ... 10977 ISIC_9992485 3422.75 10978 ISIC_9996992 8400.00 10979 ISIC_9997917 9282.25 10980 ISIC_9998234 3649.50 10981 ISIC_9999302 7988.50 10982 rows \u00d7 2 columns Ranking again dfs [ 0 ][ 'target' ] = rankdata ( dfs [ 0 ][ 'target' ], method = 'min' ) dfs [ 0 ] . to_csv ( 'sol.csv' , index = False )","title":"Improve blending using rankdata ensemble (1)"},{"location":"deep_learning/OOF%20and%20Ensembling/General%20Ensembling%20Techniques/improve-blending-using-rankdata-ensemble%20%281%29/#in-this-notebook-we-make-an-attempt-to-improve-our-blendingor-ensembling-by-using-ranks-instead-of-absolute-numbers","text":"","title":"In this notebook, we make an attempt to improve our blending(Or ensembling) by using ranks instead of absolute numbers."},{"location":"deep_learning/OOF%20and%20Ensembling/General%20Ensembling%20Techniques/improve-blending-using-rankdata-ensemble%20%281%29/#reasoning-behind-this-logic-is-that-the-roc-auc-metric-used-here-only-cares-about-the-rank-of-the-predictionexplained-below","text":"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os # for dirname, _, filenames in os.walk('/kaggle/input'): # for filename in filenames: # print(os.path.join(dirname, filename)) # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session os . listdir ( '../input/efficientnets/' ) PATH = 'input/efficientnets' dfs = [] i = 0 for df_loc in os . listdir ( '../input/efficientnets/' ): print ( '../input/efficientnets/ {} ' . format ( df_loc )) df = pd . read_csv ( '../input/efficientnets/ {} ' . format ( df_loc )) # df.head() dfs . append ( df ) # dfs ../input/efficientnets/submission_b7_0.888.csv ../input/efficientnets/submission_b5.csv ../input/efficientnets/submission_b7.csv ../input/efficientnets/submission_b5 (1).csv Dmytro Danevskyi wrote: The target metric in this competition is based on ranks rather than on actual values. That means that as long as the order of your values is fixed, the metric will stay the same. To illustrate: target = [1, 0, 1, 1, 0] preds = [0.5, 0.25, 0.2, 0.3, 0.1] metric = roc_auc_score(target, preds) # 0.833 target = [1, 0, 1, 1, 0] preds = [0.7, 0.15, 0.1, 0.2, 0.05] metric = roc_auc_score(target, preds) # 0.833 As you can see, only the rank of the predictions matters. Not the actual value. That means that two different models that give the same score could actually output completely different values. They are not even required to be in (0, 1) range! target = [1, 0, 1, 1, 0] preds = [100, 25, 20, 30, 10] metric = roc_auc_score(target, preds) # 0.833 Then, if you will try to average the predictions of two non-calibrated models, you might observe that the score is not necessarily getting better and in some cases, it could become even worse! This happens because the prediction scales of these two models are not directly comparable because of the aforementioned issue. How this can be fixed? One simple solution is to bring the predictions to the same scale, e.g. with scipy.stats.rankdata function. This will turn scores into ranks, i.e. [0.7, 0.15, 0.1, 0.2, 0.05] will be turned into [5, 3, 2, 4, 1] . After this, the predictions could be blended. Note that this not always lead to better results and is highly dependent on which exactly models are blended, how strong is the bias, and so on. To illustrate, I decided to naively blend my best scoring model (ResNet18) that gives 0.914 on the public leaderboard with the best-scoring public kernel available (0.927) and I got 0.925. After I preprocessed both predictions with rankdata , my score improved to 0.933.","title":"Reasoning behind this logic is that the ROC-AUC metric used here only cares about the rank of the prediction(Explained Below)"},{"location":"deep_learning/OOF%20and%20Ensembling/General%20Ensembling%20Techniques/improve-blending-using-rankdata-ensemble%20%281%29/#taking-advantage-of-this-information-lets-try-blending-using-rank","text":"(Not absolute values) Previous score was 0.888 from scipy.stats import rankdata for i in range ( 4 ) : dfs [ i ][ 'target' ] = rankdata ( dfs [ i ][ 'target' ], method = 'min' ) # dfs[0] dfs [ 0 ][ 'target' ] = ( dfs [ 0 ][ 'target' ] + dfs [ 1 ][ 'target' ] + dfs [ 2 ][ 'target' ] + dfs [ 3 ][ 'target' ]) / 4 dfs [ 0 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } image_name target 0 ISIC_0052060 4152.25 1 ISIC_0052349 2169.75 2 ISIC_0058510 5413.00 3 ISIC_0073313 3864.00 4 ISIC_0073502 7222.75 ... ... ... 10977 ISIC_9992485 3422.75 10978 ISIC_9996992 8400.00 10979 ISIC_9997917 9282.25 10980 ISIC_9998234 3649.50 10981 ISIC_9999302 7988.50 10982 rows \u00d7 2 columns Ranking again dfs [ 0 ][ 'target' ] = rankdata ( dfs [ 0 ][ 'target' ], method = 'min' ) dfs [ 0 ] . to_csv ( 'sol.csv' , index = False )","title":"Taking advantage of this information, let's try blending using rank"},{"location":"deep_learning/computer_vision/PyTorch%20Utilities/","text":"Imports, Config and Seeding import timm import torch import torchvision from typing import Dict , Union , Callable , OrderedDict import os , random import numpy as np def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all ( seed = 1992 ) Using Seed Number 1992 DEVICE = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) resnet18_pretrained_true = timm . create_model ( model_name = \"resnet34\" , pretrained = True , num_classes = 10 ) . to ( DEVICE ) Toy Models I created two versions of the same model. The Sequential method has a more compact form, but often is more difficult to extract layers. class ToyModel ( torch . nn . Module ): def __init__ ( self ): super () . __init__ () self . cl1 = torch . nn . Linear ( 25 , 60 ) self . cl2 = torch . nn . Linear ( 60 , 16 ) self . fc1 = torch . nn . Linear ( 16 , 120 ) self . fc2 = torch . nn . Linear ( 120 , 84 ) self . fc3 = torch . nn . Linear ( 84 , 10 ) def forward ( self , x ): \"\"\"Forward pass of the model. Args: x ([type]): [description] Returns: [type]: [description] \"\"\" x = torch . nn . ReLU ()( self . cl1 ( x )) x = torch . nn . ReLU ()( self . cl2 ( x )) x = torch . nn . ReLU ()( self . fc1 ( x )) x = torch . nn . ReLU ()( self . fc2 ( x )) x = torch . nn . LogSoftmax ( dim = 1 )( self . fc3 ( x )) return x class ToySequentialModel ( torch . nn . Module ): # Create a sequential model pytorch same as ToyModel. def __init__ ( self ) -> None : super () . __init__ () self . backbone = torch . nn . Sequential ( OrderedDict ( [ ( \"cl1\" , torch . nn . Linear ( 25 , 60 )), ( \"cl_relu1\" , torch . nn . ReLU ()), ( \"cl2\" , torch . nn . Linear ( 60 , 16 )), ( \"cl_relu2\" , torch . nn . ReLU ()), ] ) ) self . head = torch . nn . Sequential ( OrderedDict ( [ ( \"fc1\" , torch . nn . Linear ( 16 , 120 )), ( \"fc_relu_1\" , torch . nn . ReLU ()), ( \"fc2\" , torch . nn . Linear ( 120 , 84 )), ( \"fc_relu_2\" , torch . nn . ReLU ()), ( \"fc3\" , torch . nn . Linear ( 84 , 10 )), ( \"fc_log_softmax\" , torch . nn . LogSoftmax ( dim = 1 )), ] ) ) def forward ( self , x ): \"\"\"Forward pass of the model. Args: x ([type]): [description] Returns: [type]: [description] \"\"\" x = self . backbone ( x ) x = self . head ( x ) return x Named Modules Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. for name , layer in ToySequentialModel () . named_modules (): print ( name ) backbone backbone.cl1 backbone.cl_relu1 backbone.cl2 backbone.cl_relu2 head head.fc1 head.fc_relu_1 head.fc2 head.fc_relu_2 head.fc3 head.fc_log_softmax Get Convolutional Layers def get_conv_layers ( model : Callable , layer_type : str = \"Conv2d\" ) -> Dict [ str , str ]: \"\"\"Create a function that give me the convolutional layers of PyTorch model. This function is created to be used in conjunction with Visualization of Feature Maps. Args: model (Union[torchvision.models, timm.models]): A PyTorch model. layer_type (str): The type of layer to be extracted. Returns: conv_layers (Dict[str, str]): {\"layer1.0.conv1\": layer1.0.conv1, ...} Example: >>> resnet18_pretrained_true = timm.create_model(model_name = \"resnet34\", pretrained=True, num_classes=10).to(DEVICE) >>> conv_layers = get_conv_layers(resnet18_pretrained_true, layer_type=\"Conv2d\") \"\"\" if layer_type == \"Conv2d\" : _layer_type = torch . nn . Conv2d elif layer_type == \"Conv1d\" : _layer_type = torch . nn . Conv1d conv_layers = {} for name , layer in model . named_modules (): if isinstance ( layer , _layer_type ): conv_layers [ name ] = name return conv_layers >>> resnet18_pretrained_true = timm . create_model ( model_name = \"resnet34\" , pretrained = True , num_classes = 10 ) . to ( DEVICE ) >>> conv_layers = get_conv_layers ( resnet18_pretrained_true , layer_type = \"Conv2d\" ) >>> print ( conv_layers ) {'conv1': 'conv1', 'layer1.0.conv1': 'layer1.0.conv1', 'layer1.0.conv2': 'layer1.0.conv2', 'layer1.1.conv1': 'layer1.1.conv1', 'layer1.1.conv2': 'layer1.1.conv2', 'layer1.2.conv1': 'layer1.2.conv1', 'layer1.2.conv2': 'layer1.2.conv2', 'layer2.0.conv1': 'layer2.0.conv1', 'layer2.0.conv2': 'layer2.0.conv2', 'layer2.0.downsample.0': 'layer2.0.downsample.0', 'layer2.1.conv1': 'layer2.1.conv1', 'layer2.1.conv2': 'layer2.1.conv2', 'layer2.2.conv1': 'layer2.2.conv1', 'layer2.2.conv2': 'layer2.2.conv2', 'layer2.3.conv1': 'layer2.3.conv1', 'layer2.3.conv2': 'layer2.3.conv2', 'layer3.0.conv1': 'layer3.0.conv1', 'layer3.0.conv2': 'layer3.0.conv2', 'layer3.0.downsample.0': 'layer3.0.downsample.0', 'layer3.1.conv1': 'layer3.1.conv1', 'layer3.1.conv2': 'layer3.1.conv2', 'layer3.2.conv1': 'layer3.2.conv1', 'layer3.2.conv2': 'layer3.2.conv2', 'layer3.3.conv1': 'layer3.3.conv1', 'layer3.3.conv2': 'layer3.3.conv2', 'layer3.4.conv1': 'layer3.4.conv1', 'layer3.4.conv2': 'layer3.4.conv2', 'layer3.5.conv1': 'layer3.5.conv1', 'layer3.5.conv2': 'layer3.5.conv2', 'layer4.0.conv1': 'layer4.0.conv1', 'layer4.0.conv2': 'layer4.0.conv2', 'layer4.0.downsample.0': 'layer4.0.downsample.0', 'layer4.1.conv1': 'layer4.1.conv1', 'layer4.1.conv2': 'layer4.1.conv2', 'layer4.2.conv1': 'layer4.2.conv1', 'layer4.2.conv2': 'layer4.2.conv2'} activation = {} def get_intermediate_features ( name : str ) -> Callable : \"\"\"Get the intermediate features of a model. Forward Hook. This is using forward hook with reference https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5 Args: name (str): name of the layer. Returns: Callable: [description] \"\"\" def hook ( model , input , output ): activation [ name ] = output . detach () return hook # The below is testing the forward hook functionalities, especially getting intermediate features. # Note that both models are same organically but created differently. # Due to seeding issues, you can check whether they are the same output or not by running them separately. # We also used assertion to check that the output from model(x) is same as torch.nn.LogSoftmax(dim=1)(fc3_output) use_sequential_model = True x = torch . randn ( 1 , 25 ) if not use_sequential_model : model = ToyModel () model . fc2 . register_forward_hook ( get_intermediate_features ( \"fc2\" )) model . fc3 . register_forward_hook ( get_intermediate_features ( \"fc3\" )) output = model ( x ) print ( activation ) fc2_output = activation [ \"fc2\" ] fc3_output = activation [ \"fc3\" ] # assert output and logsoftmax fc3_output are the same assert torch . allclose ( output , torch . nn . LogSoftmax ( dim = 1 )( fc3_output )) else : sequential_model = ToySequentialModel () # Do this if you want all, if not you can see below. # for name, layer in sequential_model.named_modules(): # layer.register_forward_hook(get_intermediate_features(name)) sequential_model . head . fc2 . register_forward_hook ( get_intermediate_features ( \"head.fc2\" ) ) sequential_model . head . fc3 . register_forward_hook ( get_intermediate_features ( \"head.fc3\" ) ) sequential_model_output = sequential_model ( x ) print ( activation ) fc2_output = activation [ \"head.fc2\" ] fc3_output = activation [ \"head.fc3\" ] assert torch . allclose ( sequential_model_output , torch . nn . LogSoftmax ( dim = 1 )( fc3_output ) ) {'head.fc2': tensor([[ 0.0697, 0.0544, -0.0157, -0.1059, -0.0464, -0.0090, 0.0532, -0.1273, -0.0286, -0.0151, 0.0963, 0.2205, 0.0745, -0.0110, -0.1127, -0.0367, -0.0681, 0.0463, -0.0833, 0.1288, 0.1058, 0.0976, -0.0251, 0.0980, -0.0110, 0.1170, -0.0650, 0.2091, -0.1773, 0.0363, -0.1452, 0.0036, 0.0112, -0.0304, -0.0620, -0.0658, -0.0543, 0.0072, 0.0436, 0.0703, 0.0254, -0.0614, 0.0164, -0.1003, -0.0396, 0.0349, 0.0089, -0.1243, -0.1037, -0.0491, 0.0627, -0.1347, 0.0010, -0.1290, -0.0280, -0.0344, 0.1487, -0.1764, -0.0233, 0.0082, 0.1270, 0.0368, 0.0103, -0.0929, 0.0038, 0.1346, -0.0688, -0.0437, -0.1205, -0.1596, -0.0240, -0.1001, -0.0300, -0.1119, 0.0344, -0.1587, 0.0329, -0.0424, 0.0999, 0.0732, 0.1116, 0.0220, -0.0570, 0.0232]]), 'head.fc3': tensor([[ 0.0256, -0.0924, 0.0456, 0.0972, 0.0107, 0.0527, 0.0208, 0.0373, 0.0451, 0.0712]])}","title":"PyTorch Utilities"},{"location":"deep_learning/computer_vision/PyTorch%20Utilities/#imports-config-and-seeding","text":"import timm import torch import torchvision from typing import Dict , Union , Callable , OrderedDict import os , random import numpy as np def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all ( seed = 1992 ) Using Seed Number 1992 DEVICE = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) resnet18_pretrained_true = timm . create_model ( model_name = \"resnet34\" , pretrained = True , num_classes = 10 ) . to ( DEVICE )","title":"Imports, Config and Seeding"},{"location":"deep_learning/computer_vision/PyTorch%20Utilities/#toy-models","text":"I created two versions of the same model. The Sequential method has a more compact form, but often is more difficult to extract layers. class ToyModel ( torch . nn . Module ): def __init__ ( self ): super () . __init__ () self . cl1 = torch . nn . Linear ( 25 , 60 ) self . cl2 = torch . nn . Linear ( 60 , 16 ) self . fc1 = torch . nn . Linear ( 16 , 120 ) self . fc2 = torch . nn . Linear ( 120 , 84 ) self . fc3 = torch . nn . Linear ( 84 , 10 ) def forward ( self , x ): \"\"\"Forward pass of the model. Args: x ([type]): [description] Returns: [type]: [description] \"\"\" x = torch . nn . ReLU ()( self . cl1 ( x )) x = torch . nn . ReLU ()( self . cl2 ( x )) x = torch . nn . ReLU ()( self . fc1 ( x )) x = torch . nn . ReLU ()( self . fc2 ( x )) x = torch . nn . LogSoftmax ( dim = 1 )( self . fc3 ( x )) return x class ToySequentialModel ( torch . nn . Module ): # Create a sequential model pytorch same as ToyModel. def __init__ ( self ) -> None : super () . __init__ () self . backbone = torch . nn . Sequential ( OrderedDict ( [ ( \"cl1\" , torch . nn . Linear ( 25 , 60 )), ( \"cl_relu1\" , torch . nn . ReLU ()), ( \"cl2\" , torch . nn . Linear ( 60 , 16 )), ( \"cl_relu2\" , torch . nn . ReLU ()), ] ) ) self . head = torch . nn . Sequential ( OrderedDict ( [ ( \"fc1\" , torch . nn . Linear ( 16 , 120 )), ( \"fc_relu_1\" , torch . nn . ReLU ()), ( \"fc2\" , torch . nn . Linear ( 120 , 84 )), ( \"fc_relu_2\" , torch . nn . ReLU ()), ( \"fc3\" , torch . nn . Linear ( 84 , 10 )), ( \"fc_log_softmax\" , torch . nn . LogSoftmax ( dim = 1 )), ] ) ) def forward ( self , x ): \"\"\"Forward pass of the model. Args: x ([type]): [description] Returns: [type]: [description] \"\"\" x = self . backbone ( x ) x = self . head ( x ) return x","title":"Toy Models"},{"location":"deep_learning/computer_vision/PyTorch%20Utilities/#named-modules","text":"Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. for name , layer in ToySequentialModel () . named_modules (): print ( name ) backbone backbone.cl1 backbone.cl_relu1 backbone.cl2 backbone.cl_relu2 head head.fc1 head.fc_relu_1 head.fc2 head.fc_relu_2 head.fc3 head.fc_log_softmax","title":"Named Modules"},{"location":"deep_learning/computer_vision/PyTorch%20Utilities/#get-convolutional-layers","text":"def get_conv_layers ( model : Callable , layer_type : str = \"Conv2d\" ) -> Dict [ str , str ]: \"\"\"Create a function that give me the convolutional layers of PyTorch model. This function is created to be used in conjunction with Visualization of Feature Maps. Args: model (Union[torchvision.models, timm.models]): A PyTorch model. layer_type (str): The type of layer to be extracted. Returns: conv_layers (Dict[str, str]): {\"layer1.0.conv1\": layer1.0.conv1, ...} Example: >>> resnet18_pretrained_true = timm.create_model(model_name = \"resnet34\", pretrained=True, num_classes=10).to(DEVICE) >>> conv_layers = get_conv_layers(resnet18_pretrained_true, layer_type=\"Conv2d\") \"\"\" if layer_type == \"Conv2d\" : _layer_type = torch . nn . Conv2d elif layer_type == \"Conv1d\" : _layer_type = torch . nn . Conv1d conv_layers = {} for name , layer in model . named_modules (): if isinstance ( layer , _layer_type ): conv_layers [ name ] = name return conv_layers >>> resnet18_pretrained_true = timm . create_model ( model_name = \"resnet34\" , pretrained = True , num_classes = 10 ) . to ( DEVICE ) >>> conv_layers = get_conv_layers ( resnet18_pretrained_true , layer_type = \"Conv2d\" ) >>> print ( conv_layers ) {'conv1': 'conv1', 'layer1.0.conv1': 'layer1.0.conv1', 'layer1.0.conv2': 'layer1.0.conv2', 'layer1.1.conv1': 'layer1.1.conv1', 'layer1.1.conv2': 'layer1.1.conv2', 'layer1.2.conv1': 'layer1.2.conv1', 'layer1.2.conv2': 'layer1.2.conv2', 'layer2.0.conv1': 'layer2.0.conv1', 'layer2.0.conv2': 'layer2.0.conv2', 'layer2.0.downsample.0': 'layer2.0.downsample.0', 'layer2.1.conv1': 'layer2.1.conv1', 'layer2.1.conv2': 'layer2.1.conv2', 'layer2.2.conv1': 'layer2.2.conv1', 'layer2.2.conv2': 'layer2.2.conv2', 'layer2.3.conv1': 'layer2.3.conv1', 'layer2.3.conv2': 'layer2.3.conv2', 'layer3.0.conv1': 'layer3.0.conv1', 'layer3.0.conv2': 'layer3.0.conv2', 'layer3.0.downsample.0': 'layer3.0.downsample.0', 'layer3.1.conv1': 'layer3.1.conv1', 'layer3.1.conv2': 'layer3.1.conv2', 'layer3.2.conv1': 'layer3.2.conv1', 'layer3.2.conv2': 'layer3.2.conv2', 'layer3.3.conv1': 'layer3.3.conv1', 'layer3.3.conv2': 'layer3.3.conv2', 'layer3.4.conv1': 'layer3.4.conv1', 'layer3.4.conv2': 'layer3.4.conv2', 'layer3.5.conv1': 'layer3.5.conv1', 'layer3.5.conv2': 'layer3.5.conv2', 'layer4.0.conv1': 'layer4.0.conv1', 'layer4.0.conv2': 'layer4.0.conv2', 'layer4.0.downsample.0': 'layer4.0.downsample.0', 'layer4.1.conv1': 'layer4.1.conv1', 'layer4.1.conv2': 'layer4.1.conv2', 'layer4.2.conv1': 'layer4.2.conv1', 'layer4.2.conv2': 'layer4.2.conv2'} activation = {} def get_intermediate_features ( name : str ) -> Callable : \"\"\"Get the intermediate features of a model. Forward Hook. This is using forward hook with reference https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5 Args: name (str): name of the layer. Returns: Callable: [description] \"\"\" def hook ( model , input , output ): activation [ name ] = output . detach () return hook # The below is testing the forward hook functionalities, especially getting intermediate features. # Note that both models are same organically but created differently. # Due to seeding issues, you can check whether they are the same output or not by running them separately. # We also used assertion to check that the output from model(x) is same as torch.nn.LogSoftmax(dim=1)(fc3_output) use_sequential_model = True x = torch . randn ( 1 , 25 ) if not use_sequential_model : model = ToyModel () model . fc2 . register_forward_hook ( get_intermediate_features ( \"fc2\" )) model . fc3 . register_forward_hook ( get_intermediate_features ( \"fc3\" )) output = model ( x ) print ( activation ) fc2_output = activation [ \"fc2\" ] fc3_output = activation [ \"fc3\" ] # assert output and logsoftmax fc3_output are the same assert torch . allclose ( output , torch . nn . LogSoftmax ( dim = 1 )( fc3_output )) else : sequential_model = ToySequentialModel () # Do this if you want all, if not you can see below. # for name, layer in sequential_model.named_modules(): # layer.register_forward_hook(get_intermediate_features(name)) sequential_model . head . fc2 . register_forward_hook ( get_intermediate_features ( \"head.fc2\" ) ) sequential_model . head . fc3 . register_forward_hook ( get_intermediate_features ( \"head.fc3\" ) ) sequential_model_output = sequential_model ( x ) print ( activation ) fc2_output = activation [ \"head.fc2\" ] fc3_output = activation [ \"head.fc3\" ] assert torch . allclose ( sequential_model_output , torch . nn . LogSoftmax ( dim = 1 )( fc3_output ) ) {'head.fc2': tensor([[ 0.0697, 0.0544, -0.0157, -0.1059, -0.0464, -0.0090, 0.0532, -0.1273, -0.0286, -0.0151, 0.0963, 0.2205, 0.0745, -0.0110, -0.1127, -0.0367, -0.0681, 0.0463, -0.0833, 0.1288, 0.1058, 0.0976, -0.0251, 0.0980, -0.0110, 0.1170, -0.0650, 0.2091, -0.1773, 0.0363, -0.1452, 0.0036, 0.0112, -0.0304, -0.0620, -0.0658, -0.0543, 0.0072, 0.0436, 0.0703, 0.0254, -0.0614, 0.0164, -0.1003, -0.0396, 0.0349, 0.0089, -0.1243, -0.1037, -0.0491, 0.0627, -0.1347, 0.0010, -0.1290, -0.0280, -0.0344, 0.1487, -0.1764, -0.0233, 0.0082, 0.1270, 0.0368, 0.0103, -0.0929, 0.0038, 0.1346, -0.0688, -0.0437, -0.1205, -0.1596, -0.0240, -0.1001, -0.0300, -0.1119, 0.0344, -0.1587, 0.0329, -0.0424, 0.0999, 0.0732, 0.1116, 0.0220, -0.0570, 0.0232]]), 'head.fc3': tensor([[ 0.0256, -0.0924, 0.0456, 0.0972, 0.0107, 0.0527, 0.0208, 0.0373, 0.0451, 0.0712]])}","title":"Get Convolutional Layers"},{"location":"deep_learning/computer_vision/general/freeze_layers/freezing_layers/","text":"Imports, Config and Seeding import timm import torch import torchvision from typing import Dict , Union , Callable , OrderedDict import os , random import numpy as np def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all ( seed = 1992 ) Using Seed Number 1992 DEVICE = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) # resnet18_pretrained_true = timm.create_model(model_name = \"resnet34\", pretrained=True, num_classes=10).to(DEVICE) norm = torch . nn . InstanceNorm2d ( num_features = 3 , track_running_stats = True ) print ( norm . running_mean , norm . running_var ) tensor([0., 0., 0.]) tensor([1., 1., 1.]) x = torch . randn ( 2 , 3 , 24 , 24 ) out = norm ( x ) print ( norm . running_mean , norm . running_var ) out = norm ( x ) print ( norm . running_mean , norm . running_var ) out = norm ( x ) print ( norm . running_mean , norm . running_var ) tensor([-1.3414e-03, -4.7338e-05, 1.1239e-03]) tensor([1.0010, 0.9984, 0.9989]) tensor([-2.5486e-03, -8.9943e-05, 2.1355e-03]) tensor([1.0018, 0.9969, 0.9979]) tensor([-0.0036, -0.0001, 0.0030]) tensor([1.0026, 0.9956, 0.9970]) norm . eval () out = norm ( x ) print ( norm . running_mean , norm . running_var ) tensor([-0.0160, -0.0018, 0.0068]) tensor([1.0002, 1.0082, 0.9904]) def freeze_batchnorm_layers ( model : Callable ) -> None : \"\"\"Freeze the batchnorm layers of a PyTorch model. Args: model (CustomNeuralNet): model to be frozen. Example: >>> model = timm.create_model(\"efficientnet_b0\", pretrained=True) >>> model.apply(freeze_batchnorm_layers) # to freeze during training \"\"\" # https://discuss.pytorch.org/t/how-to-freeze-bn-layers-while-training-the-rest-of-network-mean-and-var-wont-freeze/89736/19 # https://discuss.pytorch.org/t/should-i-use-model-eval-when-i-freeze-batchnorm-layers-to-finetune/39495/3 classname = model . __class__ . __name__ for module in model . modules (): if isinstance ( module , torch . nn . InstanceNorm2d ): module . eval () if isinstance ( module , torch . nn . BatchNorm2d ): if hasattr ( module , \"weight\" ): module . weight . requires_grad_ ( False ) if hasattr ( module , \"bias\" ): module . bias . requires_grad_ ( False ) module . eval () norm . apply ( freeze_batchnorm_layers ) InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True) out = norm ( x ) norm . running_mean , norm . running_var (tensor([-0.0036, -0.0001, 0.0030]), tensor([1.0026, 0.9956, 0.9970]))","title":"How to freeze Batch Norm Layers"},{"location":"deep_learning/computer_vision/general/freeze_layers/freezing_layers/#imports-config-and-seeding","text":"import timm import torch import torchvision from typing import Dict , Union , Callable , OrderedDict import os , random import numpy as np def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all ( seed = 1992 ) Using Seed Number 1992 DEVICE = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) # resnet18_pretrained_true = timm.create_model(model_name = \"resnet34\", pretrained=True, num_classes=10).to(DEVICE) norm = torch . nn . InstanceNorm2d ( num_features = 3 , track_running_stats = True ) print ( norm . running_mean , norm . running_var ) tensor([0., 0., 0.]) tensor([1., 1., 1.]) x = torch . randn ( 2 , 3 , 24 , 24 ) out = norm ( x ) print ( norm . running_mean , norm . running_var ) out = norm ( x ) print ( norm . running_mean , norm . running_var ) out = norm ( x ) print ( norm . running_mean , norm . running_var ) tensor([-1.3414e-03, -4.7338e-05, 1.1239e-03]) tensor([1.0010, 0.9984, 0.9989]) tensor([-2.5486e-03, -8.9943e-05, 2.1355e-03]) tensor([1.0018, 0.9969, 0.9979]) tensor([-0.0036, -0.0001, 0.0030]) tensor([1.0026, 0.9956, 0.9970]) norm . eval () out = norm ( x ) print ( norm . running_mean , norm . running_var ) tensor([-0.0160, -0.0018, 0.0068]) tensor([1.0002, 1.0082, 0.9904]) def freeze_batchnorm_layers ( model : Callable ) -> None : \"\"\"Freeze the batchnorm layers of a PyTorch model. Args: model (CustomNeuralNet): model to be frozen. Example: >>> model = timm.create_model(\"efficientnet_b0\", pretrained=True) >>> model.apply(freeze_batchnorm_layers) # to freeze during training \"\"\" # https://discuss.pytorch.org/t/how-to-freeze-bn-layers-while-training-the-rest-of-network-mean-and-var-wont-freeze/89736/19 # https://discuss.pytorch.org/t/should-i-use-model-eval-when-i-freeze-batchnorm-layers-to-finetune/39495/3 classname = model . __class__ . __name__ for module in model . modules (): if isinstance ( module , torch . nn . InstanceNorm2d ): module . eval () if isinstance ( module , torch . nn . BatchNorm2d ): if hasattr ( module , \"weight\" ): module . weight . requires_grad_ ( False ) if hasattr ( module , \"bias\" ): module . bias . requires_grad_ ( False ) module . eval () norm . apply ( freeze_batchnorm_layers ) InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True) out = norm ( x ) norm . running_mean , norm . running_var (tensor([-0.0036, -0.0001, 0.0030]), tensor([1.0026, 0.9956, 0.9970]))","title":"Imports, Config and Seeding"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/","text":"EDA Visualizations for Image Recognition (Conv Filter Edition) Dependencies and Imports ! pip install - q timm from typing import Dict import matplotlib.pyplot as plt import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import timm import torch import torchvision from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) % matplotlib inline import glob import os from math import ceil import random import cv2 import PIL from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" from typing import * # importing modules import urllib.request from PIL import Image Config and Logging import logging from logging import INFO , FileHandler , Formatter , StreamHandler , getLogger def init_logger ( log_file : str = \"info.log\" ) -> logging . Logger : \"\"\"Initialize logger and save to file. Consider having more log_file paths to save, eg: debug.log, error.log, etc. Args: log_file (str, optional): [description]. Defaults to Path(LOGS_DIR, \"info.log\"). Returns: logging.Logger: [description] \"\"\" logger = getLogger ( __name__ ) logger . setLevel ( INFO ) stream_handler = StreamHandler () stream_handler . setFormatter ( Formatter ( \" %(asctime)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" ) ) file_handler = FileHandler ( filename = log_file ) file_handler . setFormatter ( Formatter ( \" %(asctime)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" ) ) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger logger = init_logger () Utils def plot_multiple_img ( img_matrix_list , title_list , ncols , main_title = \"\" ): fig , myaxes = plt . subplots ( figsize = ( 20 , 15 ), nrows = ceil ( len ( img_matrix_list ) / ncols ), ncols = ncols , squeeze = False , ) fig . suptitle ( main_title , fontsize = 30 ) fig . subplots_adjust ( wspace = 0.3 ) fig . subplots_adjust ( hspace = 0.3 ) for i , ( img , title ) in enumerate ( zip ( img_matrix_list , title_list )): myaxes [ i // ncols ][ i % ncols ] . imshow ( img ) myaxes [ i // ncols ][ i % ncols ] . set_title ( title , fontsize = 15 ) plt . show () Seeding def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all () Using Seed Number 1992 Transforms Params mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ] std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] image_size : int = 224 transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), torchvision . transforms . Normalize ( mean = mean , std = std ), ] ) pre_normalize_transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), ] ) Visualizations cat_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/cat.jpg\" dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/dog.jpg\" from urllib.request import urlopen # plot cat and dog with title using PIL plt . figure ( figsize = ( 10 , 10 )) plt . subplot ( 1 , 2 , 1 ) cat = PIL . Image . open ( urlopen ( cat_p )) plt . imshow ( cat . resize (( 1024 , 1024 ))) plt . title ( \"Cat\" ) plt . subplot ( 1 , 2 , 2 ) dog = PIL . Image . open ( urlopen ( dog_p )) plt . imshow ( dog . resize (( 1024 , 1024 ))) plt . title ( \"Dog\" ) plt . show (); Convolution Layers The image and content are referenced with courtesy from [Tarun's notebook](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models). Convolution is a rather simple algorithm which involves a kernel (a 2D matrix) which moves over the entire image, calculating dot products with each window along the way. The GIF below demonstrates convolution in action. The above process can be summarized with an equation, where f is the image and h is the kernel. The dimensions of f are (m, n) and the kernel is a square matrix with dimensions smaller than f : \\( \\(\\text{conv}(f, h) = \\sum_{j}\\sum_{k}h_{jk} \\cdot f_{(m-j)(n-k)}\\) \\) In the above equation, the kernel h is moving across the length and breadth of the image. The dot product of h with a sub-matrix or window of matrix f is taken at each step, hence the double summation (rows and columns). I have always remembered from the revered Andrew Ng about how he taught us about what convolutional layers do. In the beginning, the conv layers are of low level abstraction, detailing a image's features such as shapes and sizes. In particular, he described to us the horizontal and vertical conv filters. As the conv layers go later, it will pick up on many abstract features, which is not really easily distinguished by human eyes. Below, we see an example of horizontal and vertical filters. def conv_horizontal ( image : np . ndarray ) -> None : \"\"\"Plot the horizontal convolution of the image. Args: image (torch.Tensor): [description] \"\"\" fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 )) kernel = np . ones (( 3 , 3 ), np . float32 ) kernel [ 1 ] = np . array ([ 0 , 0 , 0 ], np . float32 ) kernel [ 2 ] = np . array ([ - 1 , - 1 , - 1 ], np . float32 ) conv = cv2 . filter2D ( image , - 1 , kernel ) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( \"Original Image\" , fontsize = 24 ) ax [ 1 ] . imshow ( conv ) ax [ 1 ] . set_title ( \"Convolved Image with horizontal edges\" , fontsize = 24 ) plt . show () def conv_vertical ( image : np . ndarray ) -> None : \"\"\"Plot the vertical convolution of the image. Args: image (torch.Tensor): [description] \"\"\" fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 )) kernel = np . ones (( 3 , 3 ), np . float32 ) kernel [ 0 ] = np . array ([ 1 , 0 , - 1 ]) kernel [ 1 ] = np . array ([ 1 , 0 , - 1 ]) kernel [ 2 ] = np . array ([ 1 , 0 , - 1 ]) conv = cv2 . filter2D ( image , - 1 , kernel ) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( \"Original Image\" , fontsize = 24 ) ax [ 1 ] . imshow ( conv ) ax [ 1 ] . set_title ( \"Convolved Image with vertical edges\" , fontsize = 24 ) plt . show () Well, I can easily make out the horizontal and vertical edges from the dog image! Actually, not so obvious if you don't look closely! conv_horizontal ( np . asarray ( dog )) conv_vertical ( np . asarray ( dog )) The issue is, I want to visualize what our models' conv layers are seeing, like for example, the first conv layer usually has 64 filters, that is a whooping 64 different combinations of filters, each doing a slightly different thing. A mental model that I have for the first conv layer looks something like the following. conv_1_filters = [ \"vertical edge detector\" , \"horizontal edge detector\" , \"slanted 45 degrees detector\" , \"slanted 180 degrees detector\" , ... ] Feature Extractor using PyTorch's native Feature Extraction Module In order to visualize properly, I made use of PyTorch's newest feature_extraction module to do so. Note that the new feature is still in development, but it does make my life easier and reduces overhead. I no longer need use hooks or what not to plot layer information! We just need to import from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) def get_conv_layers ( model : torchvision . models ) -> Dict [ str , str ]: \"\"\"Create a function that give me the conv layers of PyTorch model. Args: model (Union[torchvision.models, timm.models]): A PyTorch model. Returns: conv_layers (Dict[str, str]): {\"layer1.0.conv1\": layer1.0.conv1, ...} \"\"\" conv_layers = {} for name , layer in model . named_modules (): if isinstance ( layer , torch . nn . Conv2d ): conv_layers [ name ] = name return conv_layers def get_feature_maps ( model_name : str , image : torch . Tensor , reduction : str = \"mean\" , pretrained : bool = True ) -> Union [ Dict [ str , torch . Tensor ], List [ torch . Tensor ], List [ str ]]: \"\"\"Function to plot feature maps from PyTorch models. Args: model_name (str): Name of the model to use. image (torch.Tensor): image should be a tensor of shape (1, 3, H, W) reduction (str, optional): Defaults to \"mean\". One of [\"mean\", \"max\", \"sum\"] pretrained (bool): whether the model is pretrained or not Raises: ValueError: Must use Torchvision models. Returns: model_feature_maps (Dict[str, torch.Tensor]): {\"conv_1\": conv_1_feature_map, ...} processed_feature_maps (List[torch.Tensor]): [conv_1_feature_map, ...] processed using a reduction method. feature_map_names (List[str]): [conv_1, ...] Example: >>> from torchvision.models.vgg import vgg16 >>> model = vgg16(pretrained=True) >>> image = torch.rand(1, 3, 224, 224) >>> feature_maps = get_feature_maps(model, image, reduction=\"mean\") Reduction: If a feature map has 4 filters, in the shape of (4, H, W) = (4, 32, 32), then the reduction can be done as follows: >>> reduction = \"mean\": There are 4 filters in this feature map, you can imagine it as 4 32x32 images. We sum up all 4 filters element-wise and get a single 32x32 image. Then we take the mean of all 32x32 images by dividing by num of kernels to get a single 32x32 image, which is reduction=\"mean\". \"\"\" try : model = getattr ( torchvision . models , model_name )( pretrained = pretrained ) except AttributeError : raise ValueError ( f \"Model { model_name } not found.\" ) train_nodes , eval_nodes = get_graph_node_names ( model ) logger . info ( f \"The train nodes of the model graph is: \\n\\n { train_nodes } \" ) return_conv_nodes = get_conv_layers ( model ) feature_extractor = create_feature_extractor ( model , return_nodes = return_conv_nodes ) # `model_feature_maps` will be a dict of Tensors, each representing a feature map model_feature_maps = feature_extractor ( image ) processed_feature_maps = [] feature_map_names = [] for conv_name , conv_feature_map in model_feature_maps . items (): conv_feature_map = conv_feature_map . squeeze ( dim = 0 ) num_filters = conv_feature_map . shape [ 0 ] if reduction == \"mean\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) / num_filters elif reduction == \"max\" : gray_scale = torch . max ( conv_feature_map , dim = 0 ) elif reduction == \"sum\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) processed_feature_maps . append ( gray_scale . data . cpu () . numpy ()) feature_map_names . append ( conv_name ) return model_feature_maps , processed_feature_maps , feature_map_names Visualizing VGG16 and ResNet18 Step 1: Initialize the models. As of now, I recommend using torchvision 's models. Ideally, I will want to use timm library for a more detailed list, but there are some bugs that is not easily integrated with the module. device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) import torchvision.models as models vgg16_pretrained_true = models . vgg16 ( pretrained = True ) vgg16_pretrained_true = vgg16_pretrained_true . to ( device ) resnet18_pretrained_true = models . resnet18 ( pretrained = True ) resnet18_pretrained_true = resnet18_pretrained_true . to ( device ) # Get node names train_nodes , eval_nodes = get_graph_node_names ( vgg16_pretrained_true ) logger . info ( f \"Train nodes of VGG16: \\n\\n { train_nodes } \" ) train_nodes , eval_nodes = get_graph_node_names ( resnet18_pretrained_true ) logger . info ( f \"Train nodes of ResNet18: \\n\\n { train_nodes } \" ) 2021-12-29 19:06:08: Train nodes of VGG16: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:08: Train nodes of ResNet18: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] Good God! When I saw the layer names from vgg16 , I nearly fainted, I see no easy way to know which layer belongs to a Conv layer. I understand that get_graph_node_names will get all the nodes on the model's graph, but it is difficult to map the node names to a layer if it is named as such, seeing resnet18 's node names is much easier for one to identify which is conv layer or not. train_nodes , eval_nodes = get_graph_node_names ( model ) logger . info ( f \"The train nodes of the model graph is: \\n\\n { train_nodes } \" ) Thus I wrote a small function get_conv_layers to get the conv layer names. It is not perfect, as downsample layers (1x1 conv layers) are tagged under Conv2d but we may not really need to use them to visualize our feature maps. One can tweak a bit if need be, but for now, I will get all layers that use the Conv2d blocks. If the feature names in vgg16 are named with conv, then we can simply use a small loop below to find the conv layer names. conv_layers = [] for node in nodes : if \"conv\" in node : conv_layers . append ( node ) I actually thought ResNet18 has 18 conv layers, but even minusing to 3 downsample layers, it's 17 conv layers, wonder why? Step 2: Transform the Tensors The PyTorch feature_extraction expects the image input to be of shape [B,C,H,W] . # We use torchvision's transform to transform the cat image to channels first. cat_tensor = transform ( cat ) # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) # We use torchvision's transform to transform the cat image with resize and normalization. # Conveniently, also making it channel first! cat_tensor = transform ( cat ) dog_tensor = transform ( dog ) assert cat_tensor . shape [ 0 ] == dog_tensor . shape [ 0 ] == 3 , \"PyTorch expects Channel First!\" # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) dog_tensor = dog_tensor . unsqueeze ( dim = 0 ) . to ( device ) logger . info ( f \" \\n\\n cat_tensor's shape: \\n { cat_tensor . shape } \\n\\n dog_tensor's shape: \\n { dog_tensor . shape } \" ) 2021-12-29 19:06:10: cat_tensor's shape: torch.Size([1, 3, 224, 224]) dog_tensor's shape: torch.Size([1, 3, 224, 224]) Step 3: Plotting the Feature Maps We first walk through get_feature_maps and see what my function is doing. # Get node names train_nodes , eval_nodes = get_graph_node_names ( model ) # Since get node names do not indicate properly which is a conv layer or not, # we use get_conv_layer instead to do the job, which returns a dict {\"conv_layer_name\": \"conv_layer_name\"} return_conv_nodes = get_conv_layers ( model ) # call create_feature_extractor on the model and its corresponding conv layer names. feature_extractor = create_feature_extractor ( model , return_nodes = return_conv_nodes ) # `model_feature_maps` will be a dict of Tensors, each representing a feature map # {\"conv_layer_1\": output filter map,...} model_feature_maps = feature_extractor ( image ) # we need to further process the feature maps processed_feature_maps , feature_map_names = [], [] for conv_name , conv_feature_map in model_feature_maps . items (): # Squeeze the dimension from [1, 64, 32, 32] to [64, 32, 32] # This means we have 64 filters of 32x32 \"images\" or kernels conv_feature_map = conv_feature_map . squeeze ( dim = 0 ) # get number of feature/kernels in this layer num_filters = conv_feature_map . shape [ 0 ] # If a feature map has 4 filters, in the shape of (4, H, W) = (4, 32, 32), then the reduction mean can be done as follows: There are 4 filters in this feature map, you can imagine it as 4 32x32 images. # Step 1: We sum up all 4 filters element-wise and get a single 32x32 image. # Step 2: Then we take the mean of all 32x32 images to get a single 32x32 image, which is reduction=\"mean\". if reduction == \"mean\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) / num_filters elif reduction == \"max\" : gray_scale = torch . max ( conv_feature_map , dim = 0 ) elif reduction == \"sum\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) processed_feature_maps . append ( gray_scale . data . cpu () . numpy ()) feature_map_names . append ( conv_name ) _ , vgg16_processed_feature_maps , vgg16_feature_map_names = get_feature_maps ( model_name = \"vgg16\" , image = cat_tensor , reduction = \"mean\" , pretrained = True ) _ , resnet18_processed_feature_maps , resnet18_feature_map_names = get_feature_maps ( model_name = \"resnet18\" , image = cat_tensor , reduction = \"mean\" , pretrained = True ) 2021-12-29 19:06:12: The train nodes of the model graph is: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:12: The train nodes of the model graph is: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] Then we create a simple plot_feature_maps that take in the processed_feature_maps and feature_map_names to plot them. def plot_feature_maps ( processed_feature_maps : List [ torch . Tensor ], feature_map_names : List [ str ], nrows : int , title : str = None ) -> None : \"\"\"Plot the feature maps. Args: processed_feature_maps (List[torch.Tensor]): [description] feature_map_names (List[str]): [description] nrows (int): [description] \"\"\" fig = plt . figure ( figsize = ( 30 , 50 )) ncols = len ( processed_feature_maps ) // nrows + 1 for i in range ( len ( processed_feature_maps )): a = fig . add_subplot ( nrows , ncols , i + 1 ) imgplot = plt . imshow ( processed_feature_maps [ i ]) a . axis ( \"off\" ) a . set_title ( feature_map_names [ i ] . split ( \"(\" )[ 0 ], fontsize = 30 ) fig . suptitle ( title , fontsize = 50 ) fig . tight_layout () fig . subplots_adjust ( top = 0.95 ) plt . savefig ( title , bbox_inches = 'tight' ) plt . show (); plot_feature_maps ( vgg16_processed_feature_maps , vgg16_feature_map_names , nrows = 5 , title = \"VGG16 Pretrained Feature Maps\" , ) plot_feature_maps ( resnet18_processed_feature_maps , resnet18_feature_map_names , nrows = 5 , title = \"ResNet18 Pretrained Feature Maps\" , ) Comparison with Randomly Initialized Weights We know that if the model is not pretrained, it will initialize with random weights using weight initialization methods such as Kaimin or Xavier. I expect the edges to be not so \"smooth\" as the ones that are pretrained! This is logical, as the filters in the conv layers are mostly random, and we have not trained any epochs yet, so let's see what it gives us. _ , vgg16_processed_feature_maps , vgg16_feature_map_names = get_feature_maps ( model_name = \"vgg16\" , image = cat_tensor , reduction = \"mean\" , pretrained = False ) _ , resnet18_processed_feature_maps , resnet18_feature_map_names = get_feature_maps ( model_name = \"resnet18\" , image = cat_tensor , reduction = \"mean\" , pretrained = False ) 2021-12-29 19:06:21: The train nodes of the model graph is: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:21: The train nodes of the model graph is: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] plot_feature_maps ( vgg16_processed_feature_maps , vgg16_feature_map_names , nrows = 5 , title = \"VGG16 NOT Pretrained Feature Maps\" , ) plot_feature_maps ( resnet18_processed_feature_maps , resnet18_feature_map_names , nrows = 5 , title = \"ResNet18 NOT Pretrained Feature Maps\" , ) References: https://pytorch.org/vision/stable/feature_extraction.html https://ravivaishnav20.medium.com/visualizing-feature-maps-using-pytorch https://pytorch.org/blog/FX-feature-extraction-torchvision/ https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models","title":"Visualizing Convolutional Filters"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#eda-visualizations-for-image-recognition-conv-filter-edition","text":"","title":"EDA Visualizations for Image Recognition (Conv Filter Edition)"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#dependencies-and-imports","text":"! pip install - q timm from typing import Dict import matplotlib.pyplot as plt import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import timm import torch import torchvision from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) % matplotlib inline import glob import os from math import ceil import random import cv2 import PIL from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" from typing import * # importing modules import urllib.request from PIL import Image","title":"Dependencies and Imports"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#config-and-logging","text":"import logging from logging import INFO , FileHandler , Formatter , StreamHandler , getLogger def init_logger ( log_file : str = \"info.log\" ) -> logging . Logger : \"\"\"Initialize logger and save to file. Consider having more log_file paths to save, eg: debug.log, error.log, etc. Args: log_file (str, optional): [description]. Defaults to Path(LOGS_DIR, \"info.log\"). Returns: logging.Logger: [description] \"\"\" logger = getLogger ( __name__ ) logger . setLevel ( INFO ) stream_handler = StreamHandler () stream_handler . setFormatter ( Formatter ( \" %(asctime)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" ) ) file_handler = FileHandler ( filename = log_file ) file_handler . setFormatter ( Formatter ( \" %(asctime)s : %(message)s \" , \"%Y-%m- %d %H:%M:%S\" ) ) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger logger = init_logger ()","title":"Config and Logging"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#utils","text":"def plot_multiple_img ( img_matrix_list , title_list , ncols , main_title = \"\" ): fig , myaxes = plt . subplots ( figsize = ( 20 , 15 ), nrows = ceil ( len ( img_matrix_list ) / ncols ), ncols = ncols , squeeze = False , ) fig . suptitle ( main_title , fontsize = 30 ) fig . subplots_adjust ( wspace = 0.3 ) fig . subplots_adjust ( hspace = 0.3 ) for i , ( img , title ) in enumerate ( zip ( img_matrix_list , title_list )): myaxes [ i // ncols ][ i % ncols ] . imshow ( img ) myaxes [ i // ncols ][ i % ncols ] . set_title ( title , fontsize = 15 ) plt . show ()","title":"Utils"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#seeding","text":"def seed_all ( seed : int = 1992 ) -> None : \"\"\"Seed all random number generators.\"\"\" print ( f \"Using Seed Number { seed } \" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) # set PYTHONHASHSEED env var at fixed value torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . cuda . manual_seed ( seed ) # pytorch (both CPU and CUDA) np . random . seed ( seed ) # for numpy pseudo-random generator # set fixed value for python built-in pseudo-random generator random . seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True torch . backends . cudnn . enabled = True def seed_worker ( _worker_id ) -> None : \"\"\"Seed a worker with the given ID.\"\"\" worker_seed = torch . initial_seed () % 2 ** 32 np . random . seed ( worker_seed ) random . seed ( worker_seed ) seed_all () Using Seed Number 1992","title":"Seeding"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#transforms-params","text":"mean : List [ float ] = [ 0.485 , 0.456 , 0.406 ] std : List [ float ] = [ 0.229 , 0.224 , 0.225 ] image_size : int = 224 transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), torchvision . transforms . Normalize ( mean = mean , std = std ), ] ) pre_normalize_transform = torchvision . transforms . Compose ( [ torchvision . transforms . Resize (( image_size , image_size )), torchvision . transforms . ToTensor (), ] )","title":"Transforms Params"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#visualizations","text":"cat_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/cat.jpg\" dog_p = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/data/misc/dog.jpg\" from urllib.request import urlopen # plot cat and dog with title using PIL plt . figure ( figsize = ( 10 , 10 )) plt . subplot ( 1 , 2 , 1 ) cat = PIL . Image . open ( urlopen ( cat_p )) plt . imshow ( cat . resize (( 1024 , 1024 ))) plt . title ( \"Cat\" ) plt . subplot ( 1 , 2 , 2 ) dog = PIL . Image . open ( urlopen ( dog_p )) plt . imshow ( dog . resize (( 1024 , 1024 ))) plt . title ( \"Dog\" ) plt . show ();","title":"Visualizations"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#convolution-layers","text":"The image and content are referenced with courtesy from [Tarun's notebook](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models). Convolution is a rather simple algorithm which involves a kernel (a 2D matrix) which moves over the entire image, calculating dot products with each window along the way. The GIF below demonstrates convolution in action. The above process can be summarized with an equation, where f is the image and h is the kernel. The dimensions of f are (m, n) and the kernel is a square matrix with dimensions smaller than f : \\( \\(\\text{conv}(f, h) = \\sum_{j}\\sum_{k}h_{jk} \\cdot f_{(m-j)(n-k)}\\) \\) In the above equation, the kernel h is moving across the length and breadth of the image. The dot product of h with a sub-matrix or window of matrix f is taken at each step, hence the double summation (rows and columns). I have always remembered from the revered Andrew Ng about how he taught us about what convolutional layers do. In the beginning, the conv layers are of low level abstraction, detailing a image's features such as shapes and sizes. In particular, he described to us the horizontal and vertical conv filters. As the conv layers go later, it will pick up on many abstract features, which is not really easily distinguished by human eyes. Below, we see an example of horizontal and vertical filters. def conv_horizontal ( image : np . ndarray ) -> None : \"\"\"Plot the horizontal convolution of the image. Args: image (torch.Tensor): [description] \"\"\" fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 )) kernel = np . ones (( 3 , 3 ), np . float32 ) kernel [ 1 ] = np . array ([ 0 , 0 , 0 ], np . float32 ) kernel [ 2 ] = np . array ([ - 1 , - 1 , - 1 ], np . float32 ) conv = cv2 . filter2D ( image , - 1 , kernel ) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( \"Original Image\" , fontsize = 24 ) ax [ 1 ] . imshow ( conv ) ax [ 1 ] . set_title ( \"Convolved Image with horizontal edges\" , fontsize = 24 ) plt . show () def conv_vertical ( image : np . ndarray ) -> None : \"\"\"Plot the vertical convolution of the image. Args: image (torch.Tensor): [description] \"\"\" fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 20 )) kernel = np . ones (( 3 , 3 ), np . float32 ) kernel [ 0 ] = np . array ([ 1 , 0 , - 1 ]) kernel [ 1 ] = np . array ([ 1 , 0 , - 1 ]) kernel [ 2 ] = np . array ([ 1 , 0 , - 1 ]) conv = cv2 . filter2D ( image , - 1 , kernel ) ax [ 0 ] . imshow ( image ) ax [ 0 ] . set_title ( \"Original Image\" , fontsize = 24 ) ax [ 1 ] . imshow ( conv ) ax [ 1 ] . set_title ( \"Convolved Image with vertical edges\" , fontsize = 24 ) plt . show () Well, I can easily make out the horizontal and vertical edges from the dog image! Actually, not so obvious if you don't look closely! conv_horizontal ( np . asarray ( dog )) conv_vertical ( np . asarray ( dog )) The issue is, I want to visualize what our models' conv layers are seeing, like for example, the first conv layer usually has 64 filters, that is a whooping 64 different combinations of filters, each doing a slightly different thing. A mental model that I have for the first conv layer looks something like the following. conv_1_filters = [ \"vertical edge detector\" , \"horizontal edge detector\" , \"slanted 45 degrees detector\" , \"slanted 180 degrees detector\" , ... ]","title":"Convolution Layers "},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#feature-extractor-using-pytorchs-native-feature-extraction-module","text":"In order to visualize properly, I made use of PyTorch's newest feature_extraction module to do so. Note that the new feature is still in development, but it does make my life easier and reduces overhead. I no longer need use hooks or what not to plot layer information! We just need to import from torchvision.models.feature_extraction import ( create_feature_extractor , get_graph_node_names ) def get_conv_layers ( model : torchvision . models ) -> Dict [ str , str ]: \"\"\"Create a function that give me the conv layers of PyTorch model. Args: model (Union[torchvision.models, timm.models]): A PyTorch model. Returns: conv_layers (Dict[str, str]): {\"layer1.0.conv1\": layer1.0.conv1, ...} \"\"\" conv_layers = {} for name , layer in model . named_modules (): if isinstance ( layer , torch . nn . Conv2d ): conv_layers [ name ] = name return conv_layers def get_feature_maps ( model_name : str , image : torch . Tensor , reduction : str = \"mean\" , pretrained : bool = True ) -> Union [ Dict [ str , torch . Tensor ], List [ torch . Tensor ], List [ str ]]: \"\"\"Function to plot feature maps from PyTorch models. Args: model_name (str): Name of the model to use. image (torch.Tensor): image should be a tensor of shape (1, 3, H, W) reduction (str, optional): Defaults to \"mean\". One of [\"mean\", \"max\", \"sum\"] pretrained (bool): whether the model is pretrained or not Raises: ValueError: Must use Torchvision models. Returns: model_feature_maps (Dict[str, torch.Tensor]): {\"conv_1\": conv_1_feature_map, ...} processed_feature_maps (List[torch.Tensor]): [conv_1_feature_map, ...] processed using a reduction method. feature_map_names (List[str]): [conv_1, ...] Example: >>> from torchvision.models.vgg import vgg16 >>> model = vgg16(pretrained=True) >>> image = torch.rand(1, 3, 224, 224) >>> feature_maps = get_feature_maps(model, image, reduction=\"mean\") Reduction: If a feature map has 4 filters, in the shape of (4, H, W) = (4, 32, 32), then the reduction can be done as follows: >>> reduction = \"mean\": There are 4 filters in this feature map, you can imagine it as 4 32x32 images. We sum up all 4 filters element-wise and get a single 32x32 image. Then we take the mean of all 32x32 images by dividing by num of kernels to get a single 32x32 image, which is reduction=\"mean\". \"\"\" try : model = getattr ( torchvision . models , model_name )( pretrained = pretrained ) except AttributeError : raise ValueError ( f \"Model { model_name } not found.\" ) train_nodes , eval_nodes = get_graph_node_names ( model ) logger . info ( f \"The train nodes of the model graph is: \\n\\n { train_nodes } \" ) return_conv_nodes = get_conv_layers ( model ) feature_extractor = create_feature_extractor ( model , return_nodes = return_conv_nodes ) # `model_feature_maps` will be a dict of Tensors, each representing a feature map model_feature_maps = feature_extractor ( image ) processed_feature_maps = [] feature_map_names = [] for conv_name , conv_feature_map in model_feature_maps . items (): conv_feature_map = conv_feature_map . squeeze ( dim = 0 ) num_filters = conv_feature_map . shape [ 0 ] if reduction == \"mean\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) / num_filters elif reduction == \"max\" : gray_scale = torch . max ( conv_feature_map , dim = 0 ) elif reduction == \"sum\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) processed_feature_maps . append ( gray_scale . data . cpu () . numpy ()) feature_map_names . append ( conv_name ) return model_feature_maps , processed_feature_maps , feature_map_names","title":"Feature Extractor using PyTorch's native Feature Extraction Module"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#visualizing-vgg16-and-resnet18","text":"","title":"Visualizing VGG16 and ResNet18"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#step-1-initialize-the-models","text":"As of now, I recommend using torchvision 's models. Ideally, I will want to use timm library for a more detailed list, but there are some bugs that is not easily integrated with the module. device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) import torchvision.models as models vgg16_pretrained_true = models . vgg16 ( pretrained = True ) vgg16_pretrained_true = vgg16_pretrained_true . to ( device ) resnet18_pretrained_true = models . resnet18 ( pretrained = True ) resnet18_pretrained_true = resnet18_pretrained_true . to ( device ) # Get node names train_nodes , eval_nodes = get_graph_node_names ( vgg16_pretrained_true ) logger . info ( f \"Train nodes of VGG16: \\n\\n { train_nodes } \" ) train_nodes , eval_nodes = get_graph_node_names ( resnet18_pretrained_true ) logger . info ( f \"Train nodes of ResNet18: \\n\\n { train_nodes } \" ) 2021-12-29 19:06:08: Train nodes of VGG16: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:08: Train nodes of ResNet18: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] Good God! When I saw the layer names from vgg16 , I nearly fainted, I see no easy way to know which layer belongs to a Conv layer. I understand that get_graph_node_names will get all the nodes on the model's graph, but it is difficult to map the node names to a layer if it is named as such, seeing resnet18 's node names is much easier for one to identify which is conv layer or not. train_nodes , eval_nodes = get_graph_node_names ( model ) logger . info ( f \"The train nodes of the model graph is: \\n\\n { train_nodes } \" ) Thus I wrote a small function get_conv_layers to get the conv layer names. It is not perfect, as downsample layers (1x1 conv layers) are tagged under Conv2d but we may not really need to use them to visualize our feature maps. One can tweak a bit if need be, but for now, I will get all layers that use the Conv2d blocks. If the feature names in vgg16 are named with conv, then we can simply use a small loop below to find the conv layer names. conv_layers = [] for node in nodes : if \"conv\" in node : conv_layers . append ( node ) I actually thought ResNet18 has 18 conv layers, but even minusing to 3 downsample layers, it's 17 conv layers, wonder why?","title":"Step 1: Initialize the models."},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#step-2-transform-the-tensors","text":"The PyTorch feature_extraction expects the image input to be of shape [B,C,H,W] . # We use torchvision's transform to transform the cat image to channels first. cat_tensor = transform ( cat ) # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) # We use torchvision's transform to transform the cat image with resize and normalization. # Conveniently, also making it channel first! cat_tensor = transform ( cat ) dog_tensor = transform ( dog ) assert cat_tensor . shape [ 0 ] == dog_tensor . shape [ 0 ] == 3 , \"PyTorch expects Channel First!\" # Now feature_extractor expects batch_size x C x H x W, so we expand one dimension in the 0th dim cat_tensor = cat_tensor . unsqueeze ( dim = 0 ) . to ( device ) dog_tensor = dog_tensor . unsqueeze ( dim = 0 ) . to ( device ) logger . info ( f \" \\n\\n cat_tensor's shape: \\n { cat_tensor . shape } \\n\\n dog_tensor's shape: \\n { dog_tensor . shape } \" ) 2021-12-29 19:06:10: cat_tensor's shape: torch.Size([1, 3, 224, 224]) dog_tensor's shape: torch.Size([1, 3, 224, 224])","title":"Step 2: Transform the Tensors"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#step-3-plotting-the-feature-maps","text":"We first walk through get_feature_maps and see what my function is doing. # Get node names train_nodes , eval_nodes = get_graph_node_names ( model ) # Since get node names do not indicate properly which is a conv layer or not, # we use get_conv_layer instead to do the job, which returns a dict {\"conv_layer_name\": \"conv_layer_name\"} return_conv_nodes = get_conv_layers ( model ) # call create_feature_extractor on the model and its corresponding conv layer names. feature_extractor = create_feature_extractor ( model , return_nodes = return_conv_nodes ) # `model_feature_maps` will be a dict of Tensors, each representing a feature map # {\"conv_layer_1\": output filter map,...} model_feature_maps = feature_extractor ( image ) # we need to further process the feature maps processed_feature_maps , feature_map_names = [], [] for conv_name , conv_feature_map in model_feature_maps . items (): # Squeeze the dimension from [1, 64, 32, 32] to [64, 32, 32] # This means we have 64 filters of 32x32 \"images\" or kernels conv_feature_map = conv_feature_map . squeeze ( dim = 0 ) # get number of feature/kernels in this layer num_filters = conv_feature_map . shape [ 0 ] # If a feature map has 4 filters, in the shape of (4, H, W) = (4, 32, 32), then the reduction mean can be done as follows: There are 4 filters in this feature map, you can imagine it as 4 32x32 images. # Step 1: We sum up all 4 filters element-wise and get a single 32x32 image. # Step 2: Then we take the mean of all 32x32 images to get a single 32x32 image, which is reduction=\"mean\". if reduction == \"mean\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) / num_filters elif reduction == \"max\" : gray_scale = torch . max ( conv_feature_map , dim = 0 ) elif reduction == \"sum\" : gray_scale = torch . sum ( conv_feature_map , dim = 0 ) processed_feature_maps . append ( gray_scale . data . cpu () . numpy ()) feature_map_names . append ( conv_name ) _ , vgg16_processed_feature_maps , vgg16_feature_map_names = get_feature_maps ( model_name = \"vgg16\" , image = cat_tensor , reduction = \"mean\" , pretrained = True ) _ , resnet18_processed_feature_maps , resnet18_feature_map_names = get_feature_maps ( model_name = \"resnet18\" , image = cat_tensor , reduction = \"mean\" , pretrained = True ) 2021-12-29 19:06:12: The train nodes of the model graph is: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:12: The train nodes of the model graph is: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] Then we create a simple plot_feature_maps that take in the processed_feature_maps and feature_map_names to plot them. def plot_feature_maps ( processed_feature_maps : List [ torch . Tensor ], feature_map_names : List [ str ], nrows : int , title : str = None ) -> None : \"\"\"Plot the feature maps. Args: processed_feature_maps (List[torch.Tensor]): [description] feature_map_names (List[str]): [description] nrows (int): [description] \"\"\" fig = plt . figure ( figsize = ( 30 , 50 )) ncols = len ( processed_feature_maps ) // nrows + 1 for i in range ( len ( processed_feature_maps )): a = fig . add_subplot ( nrows , ncols , i + 1 ) imgplot = plt . imshow ( processed_feature_maps [ i ]) a . axis ( \"off\" ) a . set_title ( feature_map_names [ i ] . split ( \"(\" )[ 0 ], fontsize = 30 ) fig . suptitle ( title , fontsize = 50 ) fig . tight_layout () fig . subplots_adjust ( top = 0.95 ) plt . savefig ( title , bbox_inches = 'tight' ) plt . show (); plot_feature_maps ( vgg16_processed_feature_maps , vgg16_feature_map_names , nrows = 5 , title = \"VGG16 Pretrained Feature Maps\" , ) plot_feature_maps ( resnet18_processed_feature_maps , resnet18_feature_map_names , nrows = 5 , title = \"ResNet18 Pretrained Feature Maps\" , )","title":"Step 3: Plotting the Feature Maps"},{"location":"deep_learning/computer_vision/general/visualizing_convolutional_filters/Visualizing%20Convolutional%20Filters/#comparison-with-randomly-initialized-weights","text":"We know that if the model is not pretrained, it will initialize with random weights using weight initialization methods such as Kaimin or Xavier. I expect the edges to be not so \"smooth\" as the ones that are pretrained! This is logical, as the filters in the conv layers are mostly random, and we have not trained any epochs yet, so let's see what it gives us. _ , vgg16_processed_feature_maps , vgg16_feature_map_names = get_feature_maps ( model_name = \"vgg16\" , image = cat_tensor , reduction = \"mean\" , pretrained = False ) _ , resnet18_processed_feature_maps , resnet18_feature_map_names = get_feature_maps ( model_name = \"resnet18\" , image = cat_tensor , reduction = \"mean\" , pretrained = False ) 2021-12-29 19:06:21: The train nodes of the model graph is: ['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] 2021-12-29 19:06:21: The train nodes of the model graph is: ['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc'] plot_feature_maps ( vgg16_processed_feature_maps , vgg16_feature_map_names , nrows = 5 , title = \"VGG16 NOT Pretrained Feature Maps\" , ) plot_feature_maps ( resnet18_processed_feature_maps , resnet18_feature_map_names , nrows = 5 , title = \"ResNet18 NOT Pretrained Feature Maps\" , ) References: https://pytorch.org/vision/stable/feature_extraction.html https://ravivaishnav20.medium.com/visualizing-feature-maps-using-pytorch https://pytorch.org/blog/FX-feature-extraction-torchvision/ https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models","title":"Comparison with Randomly Initialized Weights"},{"location":"deep_learning/fundamental_concepts/activation_functions/Activation_Functions/","text":"Regularization https://cs231n.github.io/neural-networks-1/ https://machinelearningmastery.com/activation-regularization-for-reducing-generalization-error-in-deep-learning-neural-networks/ CS231N: In the diagram above, we can see that Neural Networks with more neurons can express more complicated functions. However, this is both a blessing (since we can learn to classify more complicated data) and a curse (since it is easier to overfit the training data). Overfitting occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship. For example, the model with 20 hidden neurons fits all the training data but at the cost of segmenting the space into many disjoint red and green decision regions. The model with 3 hidden neurons only has the representational power to classify the data in broad strokes. It models the data as two blobs and interprets the few red points inside the green cluster as outliers (noise). In practice, this could lead to better generalization on the test set. Softmax The softmax function takes as input a vector \\(z\\) of \\(K\\) real numbers, and normalizes it into a probability distribution consisting of \\(K\\) probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval \\([0, 1]\\) and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities. One key difference is softmax takes in a vector of inputs, while sigmoid can only take in one real value. Sigmoid The sigmoid function takes as input a real value and output one real value as well. In Binary classification case, with class 0 and 1, we only need one output neuron (positive class neuron), and when applied sigmoid will get a number between 0 and 1, say \\(p^{+}\\) , then \\(p^{-} = 1 - p^{+}\\) . However the catch is that sigmoid in Binarcy Classification setting works just like softmax, but not when in multi-label! One key difference is softmax takes in a vector of inputs, while sigmoid can only take in one real value. Softmax vs Sigmoid Sigmoid vs Softmax I've noticed people often get directed to this question when searching whether to use sigmoid vs softmax in neural networks. If you are one of those people building a neural network classifier, here is how to decide whether to apply sigmoid or softmax to the raw output values from your network: If you have a multi-label classification problem = there is more than one \"right answer\" = the outputs are NOT mutually exclusive, then use a sigmoid function on each raw output independently. The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them. Example: classifying diseases in a chest x-ray image. The image might contain pneumonia, emphysema, and/or cancer, or none of those findings. If you have a multi-class classification problem = there is only one \"right answer\" = the outputs are mutually exclusive, then use a softmax function. The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes. Example: classifying images from the MNIST data set of handwritten digits. A single picture of a digit has only one true identity - the picture cannot be a 7 and an 8 at the same time. More reading ReLU Properties https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec \\(g(z) = \\max(0,z)\\) Differentiable over all points except \\(z = 0\\) . Swish https://stats.stackexchange.com/questions/544739/why-does-being-bounded-below-in-swish-reduces-overfitting Indeed relu is also bounded below, they didn't claim otherwise. The difference is, that swish allows small negative values for small negative inputs, which according to them, increases expressivity and improve gradient flow. The reason behind improving generalization is that, as in regularization, small, approaching zero, weights improve generalization as the function become more smooth and it reduces the effect of fitting the noise. They claim that by bounding large negative vales in the activation function, the effect is that the network \"forgets\" large negative inputs and thus helping the weights to approach to zero. See the image they added, large negative values, which are common before training are forgotten and after training the negative scale is much smaller. There is a tradeoff between bounded which improve generaliztion and unbounded that avoids saturation of gradients, and help the network to stay in the linear regime.","title":"Regularization"},{"location":"deep_learning/fundamental_concepts/activation_functions/Activation_Functions/#regularization","text":"https://cs231n.github.io/neural-networks-1/ https://machinelearningmastery.com/activation-regularization-for-reducing-generalization-error-in-deep-learning-neural-networks/ CS231N: In the diagram above, we can see that Neural Networks with more neurons can express more complicated functions. However, this is both a blessing (since we can learn to classify more complicated data) and a curse (since it is easier to overfit the training data). Overfitting occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship. For example, the model with 20 hidden neurons fits all the training data but at the cost of segmenting the space into many disjoint red and green decision regions. The model with 3 hidden neurons only has the representational power to classify the data in broad strokes. It models the data as two blobs and interprets the few red points inside the green cluster as outliers (noise). In practice, this could lead to better generalization on the test set.","title":"Regularization"},{"location":"deep_learning/fundamental_concepts/activation_functions/Activation_Functions/#softmax","text":"The softmax function takes as input a vector \\(z\\) of \\(K\\) real numbers, and normalizes it into a probability distribution consisting of \\(K\\) probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval \\([0, 1]\\) and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities. One key difference is softmax takes in a vector of inputs, while sigmoid can only take in one real value.","title":"Softmax"},{"location":"deep_learning/fundamental_concepts/activation_functions/Activation_Functions/#sigmoid","text":"The sigmoid function takes as input a real value and output one real value as well. In Binary classification case, with class 0 and 1, we only need one output neuron (positive class neuron), and when applied sigmoid will get a number between 0 and 1, say \\(p^{+}\\) , then \\(p^{-} = 1 - p^{+}\\) . However the catch is that sigmoid in Binarcy Classification setting works just like softmax, but not when in multi-label! One key difference is softmax takes in a vector of inputs, while sigmoid can only take in one real value.","title":"Sigmoid"},{"location":"deep_learning/fundamental_concepts/activation_functions/Activation_Functions/#softmax-vs-sigmoid","text":"Sigmoid vs Softmax I've noticed people often get directed to this question when searching whether to use sigmoid vs softmax in neural networks. If you are one of those people building a neural network classifier, here is how to decide whether to apply sigmoid or softmax to the raw output values from your network: If you have a multi-label classification problem = there is more than one \"right answer\" = the outputs are NOT mutually exclusive, then use a sigmoid function on each raw output independently. The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them. Example: classifying diseases in a chest x-ray image. The image might contain pneumonia, emphysema, and/or cancer, or none of those findings. If you have a multi-class classification problem = there is only one \"right answer\" = the outputs are mutually exclusive, then use a softmax function. The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes. Example: classifying images from the MNIST data set of handwritten digits. A single picture of a digit has only one true identity - the picture cannot be a 7 and an 8 at the same time. More reading","title":"Softmax vs Sigmoid"},{"location":"deep_learning/fundamental_concepts/activation_functions/Activation_Functions/#relu","text":"","title":"ReLU"},{"location":"deep_learning/fundamental_concepts/activation_functions/Activation_Functions/#properties","text":"https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec \\(g(z) = \\max(0,z)\\) Differentiable over all points except \\(z = 0\\) .","title":"Properties"},{"location":"deep_learning/fundamental_concepts/activation_functions/Activation_Functions/#swish","text":"https://stats.stackexchange.com/questions/544739/why-does-being-bounded-below-in-swish-reduces-overfitting Indeed relu is also bounded below, they didn't claim otherwise. The difference is, that swish allows small negative values for small negative inputs, which according to them, increases expressivity and improve gradient flow. The reason behind improving generalization is that, as in regularization, small, approaching zero, weights improve generalization as the function become more smooth and it reduces the effect of fitting the noise. They claim that by bounding large negative vales in the activation function, the effect is that the network \"forgets\" large negative inputs and thus helping the weights to approach to zero. See the image they added, large negative values, which are common before training are forgotten and after training the negative scale is much smaller. There is a tradeoff between bounded which improve generaliztion and unbounded that avoids saturation of gradients, and help the network to stay in the linear regime.","title":"Swish"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/","text":"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname , _ , filenames in os . walk ( '/kaggle/input' ): for filename in filenames : print ( os . path . join ( dirname , filename )) # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session import torch import matplotlib.pyplot as plt 1. LAMBDA LR Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}} = l r_{\\text {initial}} * Lambda(epoch) \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) lambda1 = lambda epoch : 0.65 ** epoch scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lr_lambda = lambda1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \", round(0.65 ** i,3),\" , Learning Rate = \",round(optimizer.param_groups[0][\"lr\"],3)) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f53451ba510>] 2. MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}} = l r_{\\text {epoch - 1}} * Lambda(epoch) \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) lmbda = lambda epoch : 0.65 ** epoch scheduler = torch . optim . lr_scheduler . MultiplicativeLR ( optimizer , lr_lambda = lmbda ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.95,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f53450df590>] 3. StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}}=\\left\\{\\begin{array}{ll} Gamma * l r_{\\text {epoch - 1}}, & \\text { if } {\\text {epoch % step_size}}=0 \\\\ l r_{\\text {epoch - 1}}, & \\text { otherwise } \\end{array}\\right. \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . StepLR ( optimizer , step_size = 2 , gamma = 0.1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.1 if i!=0 and i%2!=0 else 1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f534505af10>] 4. MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}}=\\left\\{\\begin{array}{ll} Gamma * l r_{\\text {epoch - 1}}, & \\text { if } {\\text{ epoch in [milestones]}} \\\\ l r_{\\text {epoch - 1}}, & \\text { otherwise } \\end{array}\\right. \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . MultiStepLR ( optimizer , milestones = [ 6 , 8 , 9 ], gamma = 0.1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.1 if i in [6,8,9] else 1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f5344fc5ad0>] 5. ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}}= Gamma * l r_{\\text {epoch - 1}} \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . ExponentialLR ( optimizer , gamma = 0.1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344fad990>] 6. CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule. When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes: \\[ \\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{c u r}}{T_{\\max }} \\pi\\right)\\right) \\] It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.https://arxiv.org/abs/1608.03983 model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . CosineAnnealingLR ( optimizer , T_max = 10 , eta_min = 0 ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f534503ce90>] 7. CyclicLR - triangular model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . CyclicLR ( optimizer , base_lr = 0.001 , max_lr = 0.1 , step_size_up = 5 , mode = \"triangular\" ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344e82a10>] 7. CyclicLR - triangular2 model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . CyclicLR ( optimizer , base_lr = 0.001 , max_lr = 0.1 , step_size_up = 5 , mode = \"triangular2\" ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344e16690>] 7. CyclicLR - exp_range model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . CyclicLR ( optimizer , base_lr = 0.001 , max_lr = 0.1 , step_size_up = 5 , mode = \"exp_range\" , gamma = 0.85 ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344dd4d90>] 8.OneCycleLR - cos Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. The 1cycle learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training. This scheduler is not chainable. model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . OneCycleLR ( optimizer , max_lr = 0.1 , steps_per_epoch = 10 , epochs = 10 ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344d64c50>] 8.OneCycleLR - linear model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . OneCycleLR ( optimizer , max_lr = 0.1 , steps_per_epoch = 10 , epochs = 10 , anneal_strategy = 'linear' ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344d283d0>] 9.CosineAnnealingWarmRestarts Set the learning rate of each parameter group using a cosine annealing schedule, and restarts after Ti epochs. \\[ \\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{\\operatorname{cur}}}{T_{i}} \\pi\\right)\\right) \\] import torch import matplotlib.pyplot as plt model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) lr_sched = torch . optim . lr_scheduler . CosineAnnealingWarmRestarts ( optimizer , T_0 = 10 , T_mult = 1 , eta_min = 0.001 , last_epoch =- 1 ) lrs = [] for i in range ( 100 ): lr_sched . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ] ) plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344c93ad0>] import torch import matplotlib.pyplot as plt model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) lr_sched = torch . optim . lr_scheduler . CosineAnnealingWarmRestarts ( optimizer , T_0 = 10 , T_mult = 2 , eta_min = 0.01 , last_epoch =- 1 ) lrs = [] for i in range ( 300 ): lr_sched . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ] ) plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344bffa10>]","title":"Guide to pytorch learning rate scheduling"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#1-lambda-lr","text":"Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}} = l r_{\\text {initial}} * Lambda(epoch) \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) lambda1 = lambda epoch : 0.65 ** epoch scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lr_lambda = lambda1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \", round(0.65 ** i,3),\" , Learning Rate = \",round(optimizer.param_groups[0][\"lr\"],3)) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f53451ba510>]","title":"1. LAMBDA LR"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#2-multiplicativelr","text":"Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}} = l r_{\\text {epoch - 1}} * Lambda(epoch) \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) lmbda = lambda epoch : 0.65 ** epoch scheduler = torch . optim . lr_scheduler . MultiplicativeLR ( optimizer , lr_lambda = lmbda ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.95,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f53450df590>]","title":"2. MultiplicativeLR"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#3-steplr","text":"Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}}=\\left\\{\\begin{array}{ll} Gamma * l r_{\\text {epoch - 1}}, & \\text { if } {\\text {epoch % step_size}}=0 \\\\ l r_{\\text {epoch - 1}}, & \\text { otherwise } \\end{array}\\right. \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . StepLR ( optimizer , step_size = 2 , gamma = 0.1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.1 if i!=0 and i%2!=0 else 1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f534505af10>]","title":"3. StepLR"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#4-multisteplr","text":"Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}}=\\left\\{\\begin{array}{ll} Gamma * l r_{\\text {epoch - 1}}, & \\text { if } {\\text{ epoch in [milestones]}} \\\\ l r_{\\text {epoch - 1}}, & \\text { otherwise } \\end{array}\\right. \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . MultiStepLR ( optimizer , milestones = [ 6 , 8 , 9 ], gamma = 0.1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.1 if i in [6,8,9] else 1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( range ( 10 ), lrs ) [<matplotlib.lines.Line2D at 0x7f5344fc5ad0>]","title":"4. MultiStepLR"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#5-exponentiallr","text":"Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr. \\[ l r_{\\text {epoch}}= Gamma * l r_{\\text {epoch - 1}} \\] model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . ExponentialLR ( optimizer , gamma = 0.1 ) lrs = [] for i in range ( 10 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",0.1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344fad990>]","title":"5. ExponentialLR"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#6-cosineannealinglr","text":"Set the learning rate of each parameter group using a cosine annealing schedule. When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes: \\[ \\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{c u r}}{T_{\\max }} \\pi\\right)\\right) \\] It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.https://arxiv.org/abs/1608.03983 model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . CosineAnnealingLR ( optimizer , T_max = 10 , eta_min = 0 ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f534503ce90>]","title":"6. CosineAnnealingLR"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#7-cycliclr-triangular","text":"model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . CyclicLR ( optimizer , base_lr = 0.001 , max_lr = 0.1 , step_size_up = 5 , mode = \"triangular\" ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344e82a10>]","title":"7. CyclicLR - triangular"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#7-cycliclr-triangular2","text":"model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . CyclicLR ( optimizer , base_lr = 0.001 , max_lr = 0.1 , step_size_up = 5 , mode = \"triangular2\" ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344e16690>]","title":"7. CyclicLR - triangular2"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#7-cycliclr-exp_range","text":"model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 100 ) scheduler = torch . optim . lr_scheduler . CyclicLR ( optimizer , base_lr = 0.001 , max_lr = 0.1 , step_size_up = 5 , mode = \"exp_range\" , gamma = 0.85 ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344dd4d90>]","title":"7. CyclicLR - exp_range"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#8onecyclelr-cos","text":"Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. The 1cycle learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training. This scheduler is not chainable. model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . OneCycleLR ( optimizer , max_lr = 0.1 , steps_per_epoch = 10 , epochs = 10 ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344d64c50>]","title":"8.OneCycleLR - cos"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#8onecyclelr-linear","text":"model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) scheduler = torch . optim . lr_scheduler . OneCycleLR ( optimizer , max_lr = 0.1 , steps_per_epoch = 10 , epochs = 10 , anneal_strategy = 'linear' ) lrs = [] for i in range ( 100 ): optimizer . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ]) # print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"]) scheduler . step () plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344d283d0>]","title":"8.OneCycleLR - linear"},{"location":"deep_learning/fundamental_concepts/learning_rate_schedulers/pytorch_gradual_warmup_scheduler/notebook/guide-to-pytorch-learning-rate-scheduling/#9cosineannealingwarmrestarts","text":"Set the learning rate of each parameter group using a cosine annealing schedule, and restarts after Ti epochs. \\[ \\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{\\operatorname{cur}}}{T_{i}} \\pi\\right)\\right) \\] import torch import matplotlib.pyplot as plt model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) lr_sched = torch . optim . lr_scheduler . CosineAnnealingWarmRestarts ( optimizer , T_0 = 10 , T_mult = 1 , eta_min = 0.001 , last_epoch =- 1 ) lrs = [] for i in range ( 100 ): lr_sched . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ] ) plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344c93ad0>] import torch import matplotlib.pyplot as plt model = torch . nn . Linear ( 2 , 1 ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) lr_sched = torch . optim . lr_scheduler . CosineAnnealingWarmRestarts ( optimizer , T_0 = 10 , T_mult = 2 , eta_min = 0.01 , last_epoch =- 1 ) lrs = [] for i in range ( 300 ): lr_sched . step () lrs . append ( optimizer . param_groups [ 0 ][ \"lr\" ] ) plt . plot ( lrs ) [<matplotlib.lines.Line2D at 0x7f5344bffa10>]","title":"9.CosineAnnealingWarmRestarts"},{"location":"misc/gcp/","text":"GCP Bucket This is a tutorial for myself on how to setup GCP Bucket and use python to upload/download files. These are references here 2 . Create GCP Bucket We first create GCP Bucket here by following the steps here 1 . Python Google Cloud Storage Install the Cloud Client Libraries for Python for an individual API like Cloud Storage !pip install --upgrade google-cloud-storage Install Cloud SDK Install Cloud SDK which can be used to access Cloud Storage services from the command line and then do gcloud auth application-default login . Note that this command generates credentials for client libraries. The steps are detailed here 3 . After installation, we need to install gcloud . !pip install gcloud You can also install Cloud SDK in command line: # Windows ( New-Object Net.WebClient ) .DownloadFile ( \"https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe\" , \" $env :Temp\\GoogleCloudSDKInstaller.exe\" ) & $env :Temp \\G oogleCloudSDKInstaller.exe Follow the prompts to install the Cloud SDK. Note it will also ask you to set the default project. You can choose the project you want to use. Setup Service Account We open cmd prompt, cd to the directory where you are working on, then type gcloud auth login . Since I do not have a service account, we follow this link 4 and follow the steps using either the Cloud Consoler or Command Line. (I prefer the Cloud Console). The documentation is clear and you just need to follow the steps. Create a service account key by the following: 1. In the Cloud Console, click the email address for the service account that you created. 2. Click Keys. 3. Click Add key, then click Create new key. 4. Click Create. A JSON key file is downloaded to your computer. 5. Click Close. Setup Authenticated Environment Now you have a json file from previous step. Put the json file in a folder. Then everytime you start a terminal or new window, you can use $env :GOOGLE_APPLICATION_CREDENTIALS = \" $PATH$TO$JSON \" It seems a hassle to type the command everytime. May look into this link 5 and this 6 to see how to setup the environment. Upload and Download Files from gcloud import storage def return_bucket ( project_id : str ) -> List : \"\"\"Return a list of buckets for a given project. Args: project_id (str): The project id. Returns: List: A list of buckets. \"\"\" storage_client = storage . Client ( project = project_id ) buckets = list ( storage_client . list_buckets ()) return buckets def upload_to_bucket ( source_file_name : str , destination_blob_name : str , bucket_name : str , project_id : str , ) -> str : \"\"\"Uploads a file to the bucket and returns the public url. Args: source_file_name (str): The file in local that you want to upload. destination_blob_name (str): The name of the file in the bucket. To include full path. bucket_name (str): The name of the bucket. \"\"\" storage_client = storage . Client ( project = project_id ) bucket = storage_client . bucket ( bucket_name ) blob = bucket . blob ( destination_blob_name ) blob . upload_from_filename ( source_file_name ) print ( f \"file { source_file_name } uploaded to bucket { bucket_name } successfully!\" ) return blob . public_url def download_from_bucket ( source_file_name : str , destination_blob_name : str , bucket_name : str , project_id : str , ) -> None : \"\"\"Download file from GCP bucket. Just do the opposite of upload_to_bucket.\"\"\" storage_client = storage . Client ( project = project_id ) bucket = storage_client . bucket ( bucket_name ) blob = bucket . blob ( destination_blob_name ) blob . download_to_filename ( source_file_name ) if __name__ == \"__main__\" : PROJECT_ID = \"Your Project ID\" BUCKET_NAME = \"Bucket Name\" SOURCE_FILE_NAME = \"Source File Name stored Locally\" DESTINATION_BLOB_NAME = \"Destination File Name in GCP Bucket\" upload_to_bucket ( SOURCE_FILE_NAME , DESTINATION_BLOB_NAME , BUCKET_NAME , PROJECT_ID ) download_from_bucket ( SOURCE_FILE_NAME , DESTINATION_BLOB_NAME , BUCKET_NAME , PROJECT_ID ) If you want to mass upload or download, you just need to create a loop as such: for file in os . listdir ( path ): upload_to_bucket ( SOURCE_FILE_NAME , DESTINATION_BLOB_NAME , BUCKET_NAME , PROJECT_ID ) Creating GCP Buckets \u21a9 How to write files from Local to GCP using Python \u21a9 Install Cloud SDK \u21a9 Service Account \u21a9 How to upload a file to Google Cloud Storage on Python 3? \u21a9 Setting GOOGLE_APPLICATION_CREDENTIALS for BigQuery Python CLI \u21a9","title":"GCP"},{"location":"misc/gcp/#gcp-bucket","text":"This is a tutorial for myself on how to setup GCP Bucket and use python to upload/download files. These are references here 2 .","title":"GCP Bucket"},{"location":"misc/gcp/#create-gcp-bucket","text":"We first create GCP Bucket here by following the steps here 1 .","title":"Create GCP Bucket"},{"location":"misc/gcp/#python-google-cloud-storage","text":"Install the Cloud Client Libraries for Python for an individual API like Cloud Storage !pip install --upgrade google-cloud-storage","title":"Python Google Cloud Storage"},{"location":"misc/gcp/#install-cloud-sdk","text":"Install Cloud SDK which can be used to access Cloud Storage services from the command line and then do gcloud auth application-default login . Note that this command generates credentials for client libraries. The steps are detailed here 3 . After installation, we need to install gcloud . !pip install gcloud You can also install Cloud SDK in command line: # Windows ( New-Object Net.WebClient ) .DownloadFile ( \"https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe\" , \" $env :Temp\\GoogleCloudSDKInstaller.exe\" ) & $env :Temp \\G oogleCloudSDKInstaller.exe Follow the prompts to install the Cloud SDK. Note it will also ask you to set the default project. You can choose the project you want to use.","title":"Install Cloud SDK"},{"location":"misc/gcp/#setup-service-account","text":"We open cmd prompt, cd to the directory where you are working on, then type gcloud auth login . Since I do not have a service account, we follow this link 4 and follow the steps using either the Cloud Consoler or Command Line. (I prefer the Cloud Console). The documentation is clear and you just need to follow the steps. Create a service account key by the following: 1. In the Cloud Console, click the email address for the service account that you created. 2. Click Keys. 3. Click Add key, then click Create new key. 4. Click Create. A JSON key file is downloaded to your computer. 5. Click Close.","title":"Setup Service Account"},{"location":"misc/gcp/#setup-authenticated-environment","text":"Now you have a json file from previous step. Put the json file in a folder. Then everytime you start a terminal or new window, you can use $env :GOOGLE_APPLICATION_CREDENTIALS = \" $PATH$TO$JSON \" It seems a hassle to type the command everytime. May look into this link 5 and this 6 to see how to setup the environment.","title":"Setup Authenticated Environment"},{"location":"misc/gcp/#upload-and-download-files","text":"from gcloud import storage def return_bucket ( project_id : str ) -> List : \"\"\"Return a list of buckets for a given project. Args: project_id (str): The project id. Returns: List: A list of buckets. \"\"\" storage_client = storage . Client ( project = project_id ) buckets = list ( storage_client . list_buckets ()) return buckets def upload_to_bucket ( source_file_name : str , destination_blob_name : str , bucket_name : str , project_id : str , ) -> str : \"\"\"Uploads a file to the bucket and returns the public url. Args: source_file_name (str): The file in local that you want to upload. destination_blob_name (str): The name of the file in the bucket. To include full path. bucket_name (str): The name of the bucket. \"\"\" storage_client = storage . Client ( project = project_id ) bucket = storage_client . bucket ( bucket_name ) blob = bucket . blob ( destination_blob_name ) blob . upload_from_filename ( source_file_name ) print ( f \"file { source_file_name } uploaded to bucket { bucket_name } successfully!\" ) return blob . public_url def download_from_bucket ( source_file_name : str , destination_blob_name : str , bucket_name : str , project_id : str , ) -> None : \"\"\"Download file from GCP bucket. Just do the opposite of upload_to_bucket.\"\"\" storage_client = storage . Client ( project = project_id ) bucket = storage_client . bucket ( bucket_name ) blob = bucket . blob ( destination_blob_name ) blob . download_to_filename ( source_file_name ) if __name__ == \"__main__\" : PROJECT_ID = \"Your Project ID\" BUCKET_NAME = \"Bucket Name\" SOURCE_FILE_NAME = \"Source File Name stored Locally\" DESTINATION_BLOB_NAME = \"Destination File Name in GCP Bucket\" upload_to_bucket ( SOURCE_FILE_NAME , DESTINATION_BLOB_NAME , BUCKET_NAME , PROJECT_ID ) download_from_bucket ( SOURCE_FILE_NAME , DESTINATION_BLOB_NAME , BUCKET_NAME , PROJECT_ID ) If you want to mass upload or download, you just need to create a loop as such: for file in os . listdir ( path ): upload_to_bucket ( SOURCE_FILE_NAME , DESTINATION_BLOB_NAME , BUCKET_NAME , PROJECT_ID ) Creating GCP Buckets \u21a9 How to write files from Local to GCP using Python \u21a9 Install Cloud SDK \u21a9 Service Account \u21a9 How to upload a file to Google Cloud Storage on Python 3? \u21a9 Setting GOOGLE_APPLICATION_CREDENTIALS for BigQuery Python CLI \u21a9","title":"Upload and Download Files"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/","text":"Notations Input Space: \\(\\mathcal{X}\\) The input space contains the set of all possible examples/instances in a population . This is generally unknown. Output Space: \\(\\mathcal{Y}\\) The output space is the set of all possible labels/targets that corresponds to each point in \\(\\mathcal{X}\\) . Distribution : \\(\\mathcal{P}\\) Reference: Learning From Data p43. The unknown distribution that generated our input space \\(\\mathcal{X}\\) . In general, instead of the mapping \\(\\mathrm{y} = f(\\mathrm{x})\\) , we can take the output \\(\\mathrm{y}\\) to be a random variable that is affected by, rather than determined by, the input \\(\\mathrm{x}\\) . Formally, we have a target distribution \\(\\mathcal{P}(\\mathrm{y} | \\mathrm{x})\\) instead of just \\(\\mathrm{y} = f(\\mathrm{x})\\) . Now we say that any point \\((\\mathrm{x}, \\mathrm{y})\\) in \\(\\mathcal{X}\\) is now generated by the joint distribution \\( \\(\\mathcal{P}(\\mathrm{x}, \\mathrm{y}) = \\mathcal{P}(\\mathrm{x})\\mathcal{P}(\\mathrm{y} | \\mathrm{x})\\) \\) Data: \\(\\mathcal{D}\\) This is the set of samples drawn from \\(\\mathcal{X} \\times \\mathcal{Y}\\) over a distribution \\(\\mathcal{P}\\) . The general notation is as follows: \\( \\(\\mathcal{D} = [(\\mathrm{x^{(1)}}, \\mathrm{y^{(1)}}), (\\mathrm{x^{(2)}}, \\mathrm{y^{(2)}}), ..., (\\mathrm{x^{(N)}}, \\mathrm{y^{(N)}}))]\\) \\) where \\(N\\) denotes the number of training samples, and each \\(\\mathrm{x}^{(i)} \\in \\mathbb{R}^{n}\\) with \\(n\\) features. In general, \\(\\mathrm{y}^{(i)} \\in \\mathbb{R}\\) and is a single label. We can split \\(\\mathcal{D}\\) into two sets respectively, where \\(\\mathrm{X}\\) consists of all the \\(\\mathrm{x}\\) , and \\(\\mathrm{Y}\\) consists of all the \\(\\mathrm{y}\\) . We will see this next. Design Matrix: \\(\\mathrm{X}\\) Let \\(\\mathrm{X}\\) be the design matrix of dimensions \\(m\u2005\\times\u2005(n\u2005+\u20051)\\) where \\(m\\) is the number of observations (training samples) and \\(n\\) independent feature/input variables. Note the inconsistency in the matrix size, I just want to point out that the second matrix, has a column of one in the first row because we usually have a bias term \\(\\mathrm{x_0}\\) , which we set to 1. \\[\\mathrm{X} = \\begin{bmatrix} (\\mathbf{x^{(1)}})^{T} \\\\ (\\mathbf{x^{(2)}})^{T} \\\\ \\vdots \\\\ (\\mathbf{x^{(m)}})^{T}\\end{bmatrix}_{m \\times n} = \\begin{bmatrix} 1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\\\\ 1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\ 1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\end{bmatrix}_{m \\times (n+1)} \\] Single Training Vector: \\(\\mathrm{x}\\) It is worth noting the \\(\\mathrm{x}^{(i)}\\) defined above is formally defined to be the \\(i\\) -th column of \\(\\mathrm{X}\\) , which is the \\(i\\) -th training sample, represented as a \\(n \\times 1\\) column vector . However, the way we define the Design Matrix is that each row of \\(\\mathrm{X}\\) is the transpose of \\(\\mathrm{x}^{(i)}\\) . Note \\(x^{(i)}_j\\) is the value of feature/attribute j in the ith training instance. \\[\\mathbf{x^{(i)}} = \\begin{bmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\\\ \\vdots \\\\ x_n^{(i)} \\end{bmatrix}_{n \\times 1}\\] Target/Label: \\(\\mathrm{Y}\\) This is the target vector. By default, it is a column vector of size \\(m \\times 1\\) . \\[\\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}_{m \\times 1}\\] Hypothesis Set: \\(\\mathcal{H}\\) The set where it contains all possible functions to approximate our true function \\(f\\) . Note that the Hypothesis Set can be either continuous or discrete, means to say it can be either a finite or infinite set. But in reality, it is almost always infinite. Hypothesis: \\(\\mathcal{h}: \\mathrm{X} \\to \\mathrm{Y}\\) where \\(\\mathrm{x} \\mapsto \\mathrm{y}\\) Note that this \\(\\mathcal{h} \\in \\mathcal{H}\\) is the hypothesis function, The final best hypothesis function is called \\(g\\) , which approximates the true function \\(f\\) . Learning Algorithm: \\(\\mathcal{A}\\) What this does is from the set of Hypothesis \\(\\mathcal{H}\\) , the learning algorithm's role is to pick one \\(\\mathcal{h} \\in \\mathcal{H}\\) such that this \\(h\\) is the hypothesis function. More often, we also call our final hypothesis learned from \\(\\mathcal{A}\\) \\(g\\) . Hypothesis Subscript \\(\\mathcal{D}\\) : \\(h_{\\mathcal{D}}\\) This is no different from the previous hypothesis, instead the previous \\(h\\) is a shorthand for this notation. This means that the hypothesis we choose is dependent on the sample data given to us, that is to say, given a \\(\\mathcal{D}\\) , we will use \\(\\mathcal{A}\\) to learn a \\(h_{\\mathcal{D}}\\) from \\(\\mathcal{H}\\) . Generalization Error/Test Error/Out-of-Sample Error: \\(\\mathcal{E}_{\\text{out}}(h)\\) Reference from Foundations of Machine Learning . Given a hypothesis \\(h \\in \\mathcal{H}\\) , a true function \\(f \\in \\mathcal{C}\\) , and an underlying distribution \\(\\mathcal{P}\\) , the test/out-of-sample error of \\(h\\) is defined by \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\underset{x \\sim \\mathcal{P}}{\\mathrm{Pr}}[h(\\mathrm{x}) \\neq f(\\mathrm{x})]\\end{aligned}\\) \\) Note that the above equation is just the error rate between the hypothesis function \\(h\\) and the true function \\(f\\) and as a result, the test error of a hypothesis is not known because both the distribution \\(\\mathcal{P}\\) and the true function \\(f\\) are unknown. This brings us to the next best thing we can measure, the In-sample/Empirical/Training Error. More formally, in a regression setting where we Mean Squared Error, \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right] \\end{aligned}\\) \\) This is difficult and confusing to understand. To water down the formal definition, it is worth taking an example, in \\(\\mathcal{E}_{\\text{out}}(h)\\) we are only talking about the Expected Test Error over the Test Set and nothing else. Think of a test set with only one query point , we call it \\(\\mathrm{x}_{q}\\) , then the above equation is just \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}_{q}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 \\right] \\end{aligned}\\) \\) over a single point over the distribution \\(\\mathrm{x}_{q}\\) . That is if \\(\\mathrm{x}_{q} = 3\\) and \\(h_{\\mathcal{D}}(\\mathrm{x}_{q}) = 2\\) and \\(f(\\mathrm{x}_{q}) = 5\\) , then \\((h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 = 9\\) and it follows that \\( \\(\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}_{q}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 \\right] = \\mathbb{E}_{\\mathrm{x}_{q}}[9] = \\frac{9}{1} = 9\\) \\) Note that I purposely denoted the denominator to be 1 because we have only 1 test point, if we were to have 2 test point, say \\(\\mathrm{x} = [x_{p}, x_{q}] = [3, 6]\\) , then if \\(h_{\\mathcal{D}}(x_{p}) = 4\\) and \\(f(x_{p}) = 6\\) , then our \\((h_{\\mathcal{D}}(\\mathrm{x}_{p}) - f(\\mathrm{x}_{p}))^2 = 4\\) . Then our \\( \\(\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right] = \\mathbb{E}_{\\mathrm{x}_{q}}[[9, 4]] = \\frac{1}{2} [9 + 4] = 6.5\\) \\) Note how I secretly removed the subscript in \\(\\mathrm{x}\\) , and how when there are two points, we are taking expectation over the 2 points. So if we have \\(m\\) test points, then the expectation is taken over all the test points. Till now, our hypothesis \\(h\\) is fixed over a particular sample set \\(\\mathcal{D}\\) . We will now move on to the next concept on Expected Generalization Error (adding a word Expected in front makes a lot of difference). Expected Generalization Error/Test Error/Out-of-Sample Error: \\(\\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)]\\) For the previous generalization error, we are only talking a fixed hypothesis generated by one particular \\(\\mathcal{D}\\) . In order to remove this dependency, we can simply take the expectation of Generalization Error of \\(h\\) over a particular \\(\\mathcal{D}\\) by simply taking the expectation over all such \\(\\mathcal{D}_{i}\\) , \\(i = 1,2,3,...K\\) . Then the Expected Generalization Test Error is independent of any particular realization of \\(\\mathcal{D}\\) : \\[\\begin{aligned}\\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)] = \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right]] \\end{aligned}\\] In the following example, we can calculate the Expected Generalization Error, where we are using the Error to be Mean Squared Error, so in essence, we are finding the expected MSE. Empirical Error/Training Error/In-Sample Error: \\(\\mathcal{E}_{\\text{in}}(h)\\) Given a hypothesis \\(h \\in \\mathcal{H}\\) , a true function \\(f \\in \\mathcal{C}\\) , and an underlying distribution \\(\\mathcal{P}\\) , and a sample \\(\\mathrm{X}\\) drawn from \\(\\mathcal{X}\\) i.i.d with distribution \\(\\mathcal{P}\\) , the test/out-of-sample error of \\(h\\) is defined by \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{in}}(h) = \\frac{1}{\\mathrm{m}}\\sum_{i=1}^{\\mathrm{m}}\\text{sign}[h(\\mathrm{x}^{(i)}) \\neq f(\\mathrm{x}^{(i)})]\\end{aligned}\\) \\) Here the sign function is mainly used for binary classification, where if \\(h\\) and \\(f\\) disagrees at any point \\(x^{(i)}\\) , then \\(\\text{sign}[h(\\mathrm{x}^{(i)}) \\neq f(\\mathrm{x}^{(i)})]\\) evaluates to 1. We take the sum of all disagreements and divide by the total number of samples. In short, that is just the misclassification/error rate. The empirical error of \\(h \\in \\mathcal{H}\\) is its average error over the sample \\(\\mathcal{X}\\) , in contrast, the generalization error is its expected error based on the distribution \\(\\mathcal{P}\\) . Take careful note here that \\(h(x^{(i)})\\) is the prediction made by our hypothesis (model), we can conventionally call it \\(\\hat{y}^{(i)}\\) whereby our \\(f(x^{(i)})\\) is our ground truth label \\(y^{(i)}\\) . I believe that this ground truth label is realized once we draw the sample from \\(\\mathcal{X}\\) even though we do not know what \\(f\\) is. An additional note here, is that the summand of the in-sample error function is not fixated to the sign function. In fact, I believe you can define any loss function to calculate the \"error\". As an example, if we are dealing with regression, then we can modify the summand to our favourite Mean Squared Error. \\[\\begin{aligned}\\mathcal{E}_{\\text{in}}(h) = \\frac{1}{\\mathrm{m}}\\sum_{i=1}^{\\mathrm{m}}[h(\\mathrm{x}^{(i)}) - f(\\mathrm{x}^{(i)})]^2\\end{aligned}\\] Bias - Variance Decomposition This is a decomposition of the Expected Generalization Error. Formal Proof please read Learning From Data. Unless otherwise stated, we consider only the univariate case where \\(\\mathrm{x}\\) is a single test point. \\[\\begin{align*} \\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)] &= \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right]] \\\\ &= \\big(\\;\\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 + \\mathbb{E}_{\\mathcal{D}}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big] + \\mathbb{E}\\big[(y-f(x))^2\\big] \\\\ &= \\big(\\;\\bar{h}(\\mathrm{x}) - f(x)\\;\\big)^2 + \\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\bar{h}(\\mathrm{x}) \\;])^2\\;\\big]+ \\mathbb{E}\\big[(y-f(x))^2\\big] \\end{align*} \\] Where $\\big(\\;\\mathbb{E} {\\mathcal{D}}[\\;h {\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 $ is the Bias, \\(\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big]\\) is the Variance and \\(\\mathbb{E}\\big[(y-f(x))^2\\big]\\) is the irreducible error \\(\\epsilon\\) . Bias: \\(\\big(\\;\\mathbb{E}_\\mathcal{D}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2\\) In other form, we can express Bias as \\( \\(\\big(\\;\\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 = \\big(\\;\\bar{h}(\\mathrm{x}) - f(x)\\;\\big)^2\\) \\) See simulation on Bias-Variance Tradeoff to understand. If our test point is \\(x_{q} = 0.9\\) , then our bias is as such: \\[ \\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) \\right) - f(0.90) \\] Variance: \\(\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big]\\) This is more confusing, but we first express Variance as: \\[\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big] = \\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\bar{h}(\\mathrm{x}) \\;])^2\\;\\big]\\] If our test point is \\(x_{q} = 0.9\\) , then our variance is as such: \\[ \\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k^{[i]}(0.90) \\right)^2 \\] Pseudo Code Cross-Validation Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\) Logistic Regression Given target variable \\(Y \\in \\{0, 1\\}\\) and predictors \\(X\\) , denote \\(\\mathbb{P}(X) = P(Y = 1 | X)\\) to estimate the probability of \\(Y\\) is of positive (malignant) class. LR expresses \\(\\mathbb{P}\\) as a function the predictors \\(X\\) as \\(\\mathbb{P}(X) = \\sigma(\\hat{\\mathrm{\\beta}}^T X) = \\frac{1}{1 + \\exp(\\hat{\\mathrm{\\beta}}^T X)}\\) where \\(\\hat{\\beta}\\) is the estimated coefficients of the model. One thing worth mentioning is the logistic function \\(\\sigma(z) = \\frac{1}{1 + \\exp(-z)}\\) outputs values from 0 to 1 which is actually the functional form of our hypothesis, and therefore makes up the \\textbf{Hypothesis Space} \\(\\mathcal{H}\\) . We then uses a learning algorithm \\(\\mathcal{A}\\) , \\textbf{Maximum Likelihood Estimation (MLE)}, to estimate the coefficients of our predictors; however, since there is no closed form solution to MLE, the learning algorithm will use optimization techniques like \\textbf{Gradient Descent}\\footnote{We can use Gradient Descent if we instead minimze the negative loglikehood function which is the same as maximizing MLE} to find \\(\\hat{\\beta}\\) . Readings and References False-positive and false-negative cases of fine-needle aspiration cytology for palpable breast lesions What is a Dendrogram? Breast Biopsy - Mayo Clinic Data Centric - Andrew Ng When is Multicollinearity not an issue - Paul Allison Intuitive Explanation of Multicollinearity in Linear Regression - Stackoverflow Hypothesis Testing Across Models Hypothesis Test for Comparing ML Algorithms - Jason Brownlee Regression Modelling Strategies - Professor Frank Harrell Damage Caused by Classification Accuracy and Other Discontinuous Improper Accuracy Scoring Rules - Professor Frank Harrell On a reliable cross validation split 1 On a reliable cross validation split 2 Estimate Generalization Error Using Boxplot to compare Model's Performance Common Pitfalls - Scikit-Learn Common pitfalls in the interpretation of coefficients of linear models - Scikit-Learn Calibrated Classification - Jason Brownlee scikit learn calibration Are you sure your models return probabilities? cambridge's probability calibration calibration in ML Terms Brier Score and Model Calibration - Neptune AI Google's take on calibrated models IMPORTANT: WHAT IS CALIBRATION Hands on sklearn calibration Hands on sklearn calibration v2 Examples of scoring rules Logistic Regression is well calibrated","title":"Notations"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#notations","text":"","title":"Notations"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#input-space-mathcalx","text":"The input space contains the set of all possible examples/instances in a population . This is generally unknown.","title":"Input Space: \\(\\mathcal{X}\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#output-space-mathcaly","text":"The output space is the set of all possible labels/targets that corresponds to each point in \\(\\mathcal{X}\\) .","title":"Output Space: \\(\\mathcal{Y}\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#distribution-mathcalp","text":"Reference: Learning From Data p43. The unknown distribution that generated our input space \\(\\mathcal{X}\\) . In general, instead of the mapping \\(\\mathrm{y} = f(\\mathrm{x})\\) , we can take the output \\(\\mathrm{y}\\) to be a random variable that is affected by, rather than determined by, the input \\(\\mathrm{x}\\) . Formally, we have a target distribution \\(\\mathcal{P}(\\mathrm{y} | \\mathrm{x})\\) instead of just \\(\\mathrm{y} = f(\\mathrm{x})\\) . Now we say that any point \\((\\mathrm{x}, \\mathrm{y})\\) in \\(\\mathcal{X}\\) is now generated by the joint distribution \\( \\(\\mathcal{P}(\\mathrm{x}, \\mathrm{y}) = \\mathcal{P}(\\mathrm{x})\\mathcal{P}(\\mathrm{y} | \\mathrm{x})\\) \\)","title":"Distribution: \\(\\mathcal{P}\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#data-mathcald","text":"This is the set of samples drawn from \\(\\mathcal{X} \\times \\mathcal{Y}\\) over a distribution \\(\\mathcal{P}\\) . The general notation is as follows: \\( \\(\\mathcal{D} = [(\\mathrm{x^{(1)}}, \\mathrm{y^{(1)}}), (\\mathrm{x^{(2)}}, \\mathrm{y^{(2)}}), ..., (\\mathrm{x^{(N)}}, \\mathrm{y^{(N)}}))]\\) \\) where \\(N\\) denotes the number of training samples, and each \\(\\mathrm{x}^{(i)} \\in \\mathbb{R}^{n}\\) with \\(n\\) features. In general, \\(\\mathrm{y}^{(i)} \\in \\mathbb{R}\\) and is a single label. We can split \\(\\mathcal{D}\\) into two sets respectively, where \\(\\mathrm{X}\\) consists of all the \\(\\mathrm{x}\\) , and \\(\\mathrm{Y}\\) consists of all the \\(\\mathrm{y}\\) . We will see this next.","title":"Data: \\(\\mathcal{D}\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#design-matrix-mathrmx","text":"Let \\(\\mathrm{X}\\) be the design matrix of dimensions \\(m\u2005\\times\u2005(n\u2005+\u20051)\\) where \\(m\\) is the number of observations (training samples) and \\(n\\) independent feature/input variables. Note the inconsistency in the matrix size, I just want to point out that the second matrix, has a column of one in the first row because we usually have a bias term \\(\\mathrm{x_0}\\) , which we set to 1. \\[\\mathrm{X} = \\begin{bmatrix} (\\mathbf{x^{(1)}})^{T} \\\\ (\\mathbf{x^{(2)}})^{T} \\\\ \\vdots \\\\ (\\mathbf{x^{(m)}})^{T}\\end{bmatrix}_{m \\times n} = \\begin{bmatrix} 1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\\\\ 1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\\\\ 1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\end{bmatrix}_{m \\times (n+1)} \\]","title":"Design Matrix: \\(\\mathrm{X}\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#single-training-vector-mathrmx","text":"It is worth noting the \\(\\mathrm{x}^{(i)}\\) defined above is formally defined to be the \\(i\\) -th column of \\(\\mathrm{X}\\) , which is the \\(i\\) -th training sample, represented as a \\(n \\times 1\\) column vector . However, the way we define the Design Matrix is that each row of \\(\\mathrm{X}\\) is the transpose of \\(\\mathrm{x}^{(i)}\\) . Note \\(x^{(i)}_j\\) is the value of feature/attribute j in the ith training instance. \\[\\mathbf{x^{(i)}} = \\begin{bmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\\\ \\vdots \\\\ x_n^{(i)} \\end{bmatrix}_{n \\times 1}\\]","title":"Single Training Vector: \\(\\mathrm{x}\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#targetlabel-mathrmy","text":"This is the target vector. By default, it is a column vector of size \\(m \\times 1\\) . \\[\\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}_{m \\times 1}\\]","title":"Target/Label: \\(\\mathrm{Y}\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#hypothesis-set-mathcalh","text":"The set where it contains all possible functions to approximate our true function \\(f\\) . Note that the Hypothesis Set can be either continuous or discrete, means to say it can be either a finite or infinite set. But in reality, it is almost always infinite.","title":"Hypothesis Set: \\(\\mathcal{H}\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#hypothesis-mathcalh-mathrmx-to-mathrmy-where-mathrmx-mapsto-mathrmy","text":"Note that this \\(\\mathcal{h} \\in \\mathcal{H}\\) is the hypothesis function, The final best hypothesis function is called \\(g\\) , which approximates the true function \\(f\\) .","title":"Hypothesis: \\(\\mathcal{h}: \\mathrm{X} \\to \\mathrm{Y}\\) where \\(\\mathrm{x} \\mapsto \\mathrm{y}\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#learning-algorithm-mathcala","text":"What this does is from the set of Hypothesis \\(\\mathcal{H}\\) , the learning algorithm's role is to pick one \\(\\mathcal{h} \\in \\mathcal{H}\\) such that this \\(h\\) is the hypothesis function. More often, we also call our final hypothesis learned from \\(\\mathcal{A}\\) \\(g\\) .","title":"Learning Algorithm: \\(\\mathcal{A}\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#hypothesis-subscript-mathcald-h_mathcald","text":"This is no different from the previous hypothesis, instead the previous \\(h\\) is a shorthand for this notation. This means that the hypothesis we choose is dependent on the sample data given to us, that is to say, given a \\(\\mathcal{D}\\) , we will use \\(\\mathcal{A}\\) to learn a \\(h_{\\mathcal{D}}\\) from \\(\\mathcal{H}\\) .","title":"Hypothesis Subscript \\(\\mathcal{D}\\): \\(h_{\\mathcal{D}}\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#generalization-errortest-errorout-of-sample-error-mathcale_textouth","text":"Reference from Foundations of Machine Learning . Given a hypothesis \\(h \\in \\mathcal{H}\\) , a true function \\(f \\in \\mathcal{C}\\) , and an underlying distribution \\(\\mathcal{P}\\) , the test/out-of-sample error of \\(h\\) is defined by \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\underset{x \\sim \\mathcal{P}}{\\mathrm{Pr}}[h(\\mathrm{x}) \\neq f(\\mathrm{x})]\\end{aligned}\\) \\) Note that the above equation is just the error rate between the hypothesis function \\(h\\) and the true function \\(f\\) and as a result, the test error of a hypothesis is not known because both the distribution \\(\\mathcal{P}\\) and the true function \\(f\\) are unknown. This brings us to the next best thing we can measure, the In-sample/Empirical/Training Error. More formally, in a regression setting where we Mean Squared Error, \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right] \\end{aligned}\\) \\) This is difficult and confusing to understand. To water down the formal definition, it is worth taking an example, in \\(\\mathcal{E}_{\\text{out}}(h)\\) we are only talking about the Expected Test Error over the Test Set and nothing else. Think of a test set with only one query point , we call it \\(\\mathrm{x}_{q}\\) , then the above equation is just \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}_{q}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 \\right] \\end{aligned}\\) \\) over a single point over the distribution \\(\\mathrm{x}_{q}\\) . That is if \\(\\mathrm{x}_{q} = 3\\) and \\(h_{\\mathcal{D}}(\\mathrm{x}_{q}) = 2\\) and \\(f(\\mathrm{x}_{q}) = 5\\) , then \\((h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 = 9\\) and it follows that \\( \\(\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}_{q}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}_{q}) - f(\\mathrm{x}_{q}))^2 \\right] = \\mathbb{E}_{\\mathrm{x}_{q}}[9] = \\frac{9}{1} = 9\\) \\) Note that I purposely denoted the denominator to be 1 because we have only 1 test point, if we were to have 2 test point, say \\(\\mathrm{x} = [x_{p}, x_{q}] = [3, 6]\\) , then if \\(h_{\\mathcal{D}}(x_{p}) = 4\\) and \\(f(x_{p}) = 6\\) , then our \\((h_{\\mathcal{D}}(\\mathrm{x}_{p}) - f(\\mathrm{x}_{p}))^2 = 4\\) . Then our \\( \\(\\mathcal{E}_{\\text{out}}(h) = \\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right] = \\mathbb{E}_{\\mathrm{x}_{q}}[[9, 4]] = \\frac{1}{2} [9 + 4] = 6.5\\) \\) Note how I secretly removed the subscript in \\(\\mathrm{x}\\) , and how when there are two points, we are taking expectation over the 2 points. So if we have \\(m\\) test points, then the expectation is taken over all the test points. Till now, our hypothesis \\(h\\) is fixed over a particular sample set \\(\\mathcal{D}\\) . We will now move on to the next concept on Expected Generalization Error (adding a word Expected in front makes a lot of difference).","title":"Generalization Error/Test Error/Out-of-Sample Error: \\(\\mathcal{E}_{\\text{out}}(h)\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#expected-generalization-errortest-errorout-of-sample-error-mathbbe_mathcaldmathcale_textouth","text":"For the previous generalization error, we are only talking a fixed hypothesis generated by one particular \\(\\mathcal{D}\\) . In order to remove this dependency, we can simply take the expectation of Generalization Error of \\(h\\) over a particular \\(\\mathcal{D}\\) by simply taking the expectation over all such \\(\\mathcal{D}_{i}\\) , \\(i = 1,2,3,...K\\) . Then the Expected Generalization Test Error is independent of any particular realization of \\(\\mathcal{D}\\) : \\[\\begin{aligned}\\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)] = \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right]] \\end{aligned}\\] In the following example, we can calculate the Expected Generalization Error, where we are using the Error to be Mean Squared Error, so in essence, we are finding the expected MSE.","title":"Expected Generalization Error/Test Error/Out-of-Sample Error: \\(\\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)]\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#empirical-errortraining-errorin-sample-error-mathcale_textinh","text":"Given a hypothesis \\(h \\in \\mathcal{H}\\) , a true function \\(f \\in \\mathcal{C}\\) , and an underlying distribution \\(\\mathcal{P}\\) , and a sample \\(\\mathrm{X}\\) drawn from \\(\\mathcal{X}\\) i.i.d with distribution \\(\\mathcal{P}\\) , the test/out-of-sample error of \\(h\\) is defined by \\( \\(\\begin{aligned}\\mathcal{E}_{\\text{in}}(h) = \\frac{1}{\\mathrm{m}}\\sum_{i=1}^{\\mathrm{m}}\\text{sign}[h(\\mathrm{x}^{(i)}) \\neq f(\\mathrm{x}^{(i)})]\\end{aligned}\\) \\) Here the sign function is mainly used for binary classification, where if \\(h\\) and \\(f\\) disagrees at any point \\(x^{(i)}\\) , then \\(\\text{sign}[h(\\mathrm{x}^{(i)}) \\neq f(\\mathrm{x}^{(i)})]\\) evaluates to 1. We take the sum of all disagreements and divide by the total number of samples. In short, that is just the misclassification/error rate. The empirical error of \\(h \\in \\mathcal{H}\\) is its average error over the sample \\(\\mathcal{X}\\) , in contrast, the generalization error is its expected error based on the distribution \\(\\mathcal{P}\\) . Take careful note here that \\(h(x^{(i)})\\) is the prediction made by our hypothesis (model), we can conventionally call it \\(\\hat{y}^{(i)}\\) whereby our \\(f(x^{(i)})\\) is our ground truth label \\(y^{(i)}\\) . I believe that this ground truth label is realized once we draw the sample from \\(\\mathcal{X}\\) even though we do not know what \\(f\\) is. An additional note here, is that the summand of the in-sample error function is not fixated to the sign function. In fact, I believe you can define any loss function to calculate the \"error\". As an example, if we are dealing with regression, then we can modify the summand to our favourite Mean Squared Error. \\[\\begin{aligned}\\mathcal{E}_{\\text{in}}(h) = \\frac{1}{\\mathrm{m}}\\sum_{i=1}^{\\mathrm{m}}[h(\\mathrm{x}^{(i)}) - f(\\mathrm{x}^{(i)})]^2\\end{aligned}\\]","title":"Empirical Error/Training Error/In-Sample Error: \\(\\mathcal{E}_{\\text{in}}(h)\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#bias-variance-decomposition","text":"This is a decomposition of the Expected Generalization Error. Formal Proof please read Learning From Data. Unless otherwise stated, we consider only the univariate case where \\(\\mathrm{x}\\) is a single test point. \\[\\begin{align*} \\mathbb{E}_{\\mathcal{D}}[\\mathcal{E}_{\\text{out}}(h)] &= \\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_{\\mathrm{x}}\\left[(h_{\\mathcal{D}}(\\mathrm{x}) - f(\\mathrm{x}))^2 \\right]] \\\\ &= \\big(\\;\\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 + \\mathbb{E}_{\\mathcal{D}}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big] + \\mathbb{E}\\big[(y-f(x))^2\\big] \\\\ &= \\big(\\;\\bar{h}(\\mathrm{x}) - f(x)\\;\\big)^2 + \\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\bar{h}(\\mathrm{x}) \\;])^2\\;\\big]+ \\mathbb{E}\\big[(y-f(x))^2\\big] \\end{align*} \\] Where $\\big(\\;\\mathbb{E} {\\mathcal{D}}[\\;h {\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 $ is the Bias, \\(\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big]\\) is the Variance and \\(\\mathbb{E}\\big[(y-f(x))^2\\big]\\) is the irreducible error \\(\\epsilon\\) .","title":"Bias - Variance Decomposition"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#bias-bigmathbbe_mathcaldh_mathcaldx-fxbig2","text":"In other form, we can express Bias as \\( \\(\\big(\\;\\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2 = \\big(\\;\\bar{h}(\\mathrm{x}) - f(x)\\;\\big)^2\\) \\) See simulation on Bias-Variance Tradeoff to understand. If our test point is \\(x_{q} = 0.9\\) , then our bias is as such: \\[ \\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) \\right) - f(0.90) \\]","title":"Bias: \\(\\big(\\;\\mathbb{E}_\\mathcal{D}[\\;h_{\\mathcal{D}}(x)\\;] - f(x)\\;\\big)^2\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#variance-mathbbe_mathcaldbigh_mathcaldx-mathbbe_mathcaldh_mathcaldx2big","text":"This is more confusing, but we first express Variance as: \\[\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big] = \\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\bar{h}(\\mathrm{x}) \\;])^2\\;\\big]\\] If our test point is \\(x_{q} = 0.9\\) , then our variance is as such: \\[ \\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k^{[i]}(0.90) \\right)^2 \\]","title":"Variance: \\(\\mathbb{E}_\\mathcal{D}\\big[\\;(\\;h_{\\mathcal{D}}(x) - \\mathbb{E}_{\\mathcal{D}}[\\;h_{\\mathcal{D}}(x)\\;])^2\\;\\big]\\)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#pseudo-code","text":"","title":"Pseudo Code"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#cross-validation","text":"Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\)","title":"Cross-Validation"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#logistic-regression","text":"Given target variable \\(Y \\in \\{0, 1\\}\\) and predictors \\(X\\) , denote \\(\\mathbb{P}(X) = P(Y = 1 | X)\\) to estimate the probability of \\(Y\\) is of positive (malignant) class. LR expresses \\(\\mathbb{P}\\) as a function the predictors \\(X\\) as \\(\\mathbb{P}(X) = \\sigma(\\hat{\\mathrm{\\beta}}^T X) = \\frac{1}{1 + \\exp(\\hat{\\mathrm{\\beta}}^T X)}\\) where \\(\\hat{\\beta}\\) is the estimated coefficients of the model. One thing worth mentioning is the logistic function \\(\\sigma(z) = \\frac{1}{1 + \\exp(-z)}\\) outputs values from 0 to 1 which is actually the functional form of our hypothesis, and therefore makes up the \\textbf{Hypothesis Space} \\(\\mathcal{H}\\) . We then uses a learning algorithm \\(\\mathcal{A}\\) , \\textbf{Maximum Likelihood Estimation (MLE)}, to estimate the coefficients of our predictors; however, since there is no closed form solution to MLE, the learning algorithm will use optimization techniques like \\textbf{Gradient Descent}\\footnote{We can use Gradient Descent if we instead minimze the negative loglikehood function which is the same as maximizing MLE} to find \\(\\hat{\\beta}\\) .","title":"Logistic Regression"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Appendix/#readings-and-references","text":"False-positive and false-negative cases of fine-needle aspiration cytology for palpable breast lesions What is a Dendrogram? Breast Biopsy - Mayo Clinic Data Centric - Andrew Ng When is Multicollinearity not an issue - Paul Allison Intuitive Explanation of Multicollinearity in Linear Regression - Stackoverflow Hypothesis Testing Across Models Hypothesis Test for Comparing ML Algorithms - Jason Brownlee Regression Modelling Strategies - Professor Frank Harrell Damage Caused by Classification Accuracy and Other Discontinuous Improper Accuracy Scoring Rules - Professor Frank Harrell On a reliable cross validation split 1 On a reliable cross validation split 2 Estimate Generalization Error Using Boxplot to compare Model's Performance Common Pitfalls - Scikit-Learn Common pitfalls in the interpretation of coefficients of linear models - Scikit-Learn Calibrated Classification - Jason Brownlee scikit learn calibration Are you sure your models return probabilities? cambridge's probability calibration calibration in ML Terms Brier Score and Model Calibration - Neptune AI Google's take on calibrated models IMPORTANT: WHAT IS CALIBRATION Hands on sklearn calibration Hands on sklearn calibration v2 Examples of scoring rules Logistic Regression is well calibrated","title":"Readings and References"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%200%20-%20Introduction%20and%20Problem%20Statement/","text":"Stage 0: Defining Problem and Assumptions by Hongnan Gao Introduction Note Breast cancer accounts for 30% of the cancers in females, making it the most common cancer in women. Breast cancer is also one of the most curable disease if detected early and there are many measures in place to help. Courtesy of Singapore Cancer Society In our context, we are presented with a dataset that are taken from a biopsy procedure called Fine Needle Aspiration (FNA) performed on the breast. The tissue taken from the biopsy will then be sent to a lab and be examined by a pathologist, a report will be written if cancerous cells are spotted or not and be sent to the specialist to further explain the results to the patient. However, there may be disagreements whereby the pathologist report shows there are no signs of cancerous cells, while radiologist may disagree as he/she might find suspicious lesions from the mammogram/CT/MRI scans. This can happen if the biopsy taken is only on the benign cells and if there is dispute, a more thorough of biopsy may be performed again. Although our aim in Machine Learning is to classify whether a tumor is benign or malignant, we should bear in mind that we are not trying to dispute the expertise of the doctors/pathologists/radiologists. Instead, we develop models to aid their understanding, and also to come up with a more systematic benchmark for one to refer to. More concretely, the dataset has features that are computed from a digitized image from FNA , and each observation describes statistics/characteristics of the cell nucleus. There are 10 base features, and 3 different measurements are taken for each feature, namely, the mean, standard error and the \"worst/largest\" . One thing to note is that worst means the mean of the three largest values . Attribute Information: ID number Diagnosis (M = malignant, B = benign) Ten real-valued features are computed for each cell nucleus: radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness ( \\(\\text{perimeter}^2 / \\text{area} - 1.0\\) ) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (\"coastline approximation\" - 1) With these in mind, let us move on to defining the problem and state some initial assumptions. Problem Statement Informal Description To develop a Machine Learning Model that can classify whether a tumor is benign or malignant. We also note that we care more about whether a cancer patient is classified correctly. Formal Description Given a dataset \\(\\mathcal{D}\\) describing characteristics of a tumor, the task \\(\\mathcal{T}\\) is a binary classification problem where we aim to find an optimal hypothesis \\(g \\in \\mathcal{H}\\) using a learning algorithm \\(\\mathcal{A}\\) . The optimal hypothesis \\(g\\) should generalize well, that is to say, has a low expected generalization error \\(\\mathcal{E}\\) over a performance measure \\(\\mathrm{M}\\) . We will choose the performance measure in the later sections (not accuracy). Considerations Info Size of Dataset: The dataset is not too large, we need to be wary of an overly complex model which may easily overfit, but may not generalize well. Model Interpretation: There is a tradeoff between Model's complexity/flexibility and it's interpretability. If we need to explain our model to our business stakeholders, then it is a good idea to choose a model that can be interpreted well, models like Logistic Regression with Lasso may be a good choice as the model itself has better interpretation, and with lasso we can reduce the number of features. If we only care about our model's ability to predict, then interpretability may not be so important and we may choose a model that performs well, but the weights may be more difficult to understand. Time and Space Complexity: Practically speaking, we need to strike a balance between the speed of the training and the performance measure of the model. Data Centric vs Model Centric: From the one and only Andrew Ng 1 we understood that data plays a critical role in the Machine Learning world. https://analyticsindiamag.com/big-data-to-good-data-andrew-ng-urges-ml-community-to-be-more-data-centric-and-less-model-centric/ \u21a9","title":"Introduction"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%200%20-%20Introduction%20and%20Problem%20Statement/#introduction","text":"Note Breast cancer accounts for 30% of the cancers in females, making it the most common cancer in women. Breast cancer is also one of the most curable disease if detected early and there are many measures in place to help. Courtesy of Singapore Cancer Society In our context, we are presented with a dataset that are taken from a biopsy procedure called Fine Needle Aspiration (FNA) performed on the breast. The tissue taken from the biopsy will then be sent to a lab and be examined by a pathologist, a report will be written if cancerous cells are spotted or not and be sent to the specialist to further explain the results to the patient. However, there may be disagreements whereby the pathologist report shows there are no signs of cancerous cells, while radiologist may disagree as he/she might find suspicious lesions from the mammogram/CT/MRI scans. This can happen if the biopsy taken is only on the benign cells and if there is dispute, a more thorough of biopsy may be performed again. Although our aim in Machine Learning is to classify whether a tumor is benign or malignant, we should bear in mind that we are not trying to dispute the expertise of the doctors/pathologists/radiologists. Instead, we develop models to aid their understanding, and also to come up with a more systematic benchmark for one to refer to. More concretely, the dataset has features that are computed from a digitized image from FNA , and each observation describes statistics/characteristics of the cell nucleus. There are 10 base features, and 3 different measurements are taken for each feature, namely, the mean, standard error and the \"worst/largest\" . One thing to note is that worst means the mean of the three largest values . Attribute Information: ID number Diagnosis (M = malignant, B = benign) Ten real-valued features are computed for each cell nucleus: radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness ( \\(\\text{perimeter}^2 / \\text{area} - 1.0\\) ) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (\"coastline approximation\" - 1) With these in mind, let us move on to defining the problem and state some initial assumptions.","title":"Introduction"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%200%20-%20Introduction%20and%20Problem%20Statement/#problem-statement","text":"Informal Description To develop a Machine Learning Model that can classify whether a tumor is benign or malignant. We also note that we care more about whether a cancer patient is classified correctly. Formal Description Given a dataset \\(\\mathcal{D}\\) describing characteristics of a tumor, the task \\(\\mathcal{T}\\) is a binary classification problem where we aim to find an optimal hypothesis \\(g \\in \\mathcal{H}\\) using a learning algorithm \\(\\mathcal{A}\\) . The optimal hypothesis \\(g\\) should generalize well, that is to say, has a low expected generalization error \\(\\mathcal{E}\\) over a performance measure \\(\\mathrm{M}\\) . We will choose the performance measure in the later sections (not accuracy).","title":"Problem Statement"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%200%20-%20Introduction%20and%20Problem%20Statement/#considerations","text":"Info Size of Dataset: The dataset is not too large, we need to be wary of an overly complex model which may easily overfit, but may not generalize well. Model Interpretation: There is a tradeoff between Model's complexity/flexibility and it's interpretability. If we need to explain our model to our business stakeholders, then it is a good idea to choose a model that can be interpreted well, models like Logistic Regression with Lasso may be a good choice as the model itself has better interpretation, and with lasso we can reduce the number of features. If we only care about our model's ability to predict, then interpretability may not be so important and we may choose a model that performs well, but the weights may be more difficult to understand. Time and Space Complexity: Practically speaking, we need to strike a balance between the speed of the training and the performance measure of the model. Data Centric vs Model Centric: From the one and only Andrew Ng 1 we understood that data plays a critical role in the Machine Learning world. https://analyticsindiamag.com/big-data-to-good-data-andrew-ng-urges-ml-community-to-be-more-data-centric-and-less-model-centric/ \u21a9","title":"Considerations"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/","text":"Stage 1: Preliminary Data Inspection and Clearning by Hongnan Gao Quick Navigation Dependencies and Configuration Stage 1: Preliminary Data Inspection and Cleaning Load the dataset A brief look at the dataset Drop, drop, drop the columns! Data Types Summary Statistics Missing Data Save Data Dependencies and Configuration import random from dataclasses import dataclass , field from typing import List , Dict import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed , } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) # set config config = config () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) Stage 1: Preliminary Data Inspection and Cleaning Load the dataset df = pd . read_csv ( config . raw_data ) A brief look at the dataset Info We will query the first five rows of the dataframe to get a feel on the dataset we are working on. We also call df.info() to see the data types of the columns, and to briefly check if there is any missing values in our data (more on that later). # Column Non-Null Count Dtype --- ------ -------------- ----- 0 diagnosis 569 non - null int64 1 radius_mean 569 non - null float64 2 texture_mean 569 non - null float64 3 perimeter_mean 569 non - null float64 Importance of data types We must be sharp and ensure that each column is indeed stored in their respective data types! In the real world, we may often query \"dirty\" data from say, the database, where numeric data are represented in string. It is now our duty to ensure sanity checks are in place! display ( df . head ()) display ( df . info ( verbose = True )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 1.0950 0.9053 8.589 153.40 0.006399 0.04904 0.05373 0.01587 0.03003 0.006193 25.38 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 NaN 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 0.5435 0.7339 3.398 74.08 0.005225 0.01308 0.01860 0.01340 0.01389 0.003532 24.99 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 NaN 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 0.7456 0.7869 4.585 94.03 0.006150 0.04006 0.03832 0.02058 0.02250 0.004571 23.57 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 NaN 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744 0.4956 1.1560 3.445 27.23 0.009110 0.07458 0.05661 0.01867 0.05963 0.009208 14.91 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 NaN 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883 0.7572 0.7813 5.438 94.44 0.011490 0.02461 0.05688 0.01885 0.01756 0.005115 22.54 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 NaN <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 569 non-null int64 1 diagnosis 569 non-null object 2 radius_mean 569 non-null float64 3 texture_mean 569 non-null float64 4 perimeter_mean 569 non-null float64 5 area_mean 569 non-null float64 6 smoothness_mean 569 non-null float64 7 compactness_mean 569 non-null float64 8 concavity_mean 569 non-null float64 9 concave points_mean 569 non-null float64 10 symmetry_mean 569 non-null float64 11 fractal_dimension_mean 569 non-null float64 12 radius_se 569 non-null float64 13 texture_se 569 non-null float64 14 perimeter_se 569 non-null float64 15 area_se 569 non-null float64 16 smoothness_se 569 non-null float64 17 compactness_se 569 non-null float64 18 concavity_se 569 non-null float64 19 concave points_se 569 non-null float64 20 symmetry_se 569 non-null float64 21 fractal_dimension_se 569 non-null float64 22 radius_worst 569 non-null float64 23 texture_worst 569 non-null float64 24 perimeter_worst 569 non-null float64 25 area_worst 569 non-null float64 26 smoothness_worst 569 non-null float64 27 compactness_worst 569 non-null float64 28 concavity_worst 569 non-null float64 29 concave points_worst 569 non-null float64 30 symmetry_worst 569 non-null float64 31 fractal_dimension_worst 569 non-null float64 32 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB None A brief overview tells us our data is alright! There is, however, a column which is unnamed and has no values. This can be of various data source issues, for now, we quickly check the definition given by the dataset from UCI's Breast Cancer Wisconsin (Diagnostic) Data Set and confirm that there should only be 32 columns. With this in mind, we can safely delete the column. We also note that from the above, that the id column is the identifier for each patient. We will also drop this column as it holds no predictive power. When can ID be important? We should try to question our move and justify it. In this dataset, we have to ensure that each ID is unique, if it is not, it may suggest that there are patient records with multiple observation, which is a violation of i.i.d assumption and we may take note when doing cross-validation, so as to avoid information leakage. Since the ID column is unique, we will delete it. We will keep this at the back of our mind in the event that we ever need them for feature engineering. print ( f \"The ID column is unique : { df [ 'id' ] . is_unique } \" ) The ID column is unique : True Drop, drop, drop the columns! Here we define a drop_columns function to drop the unwanted columns. def drop_columns ( df : pd . DataFrame , columns : List ) -> pd . DataFrame : \"\"\"Drop unwanted columns from dataframe. Args: df (pd.DataFrame): Dataframe to be cleaned columns (List): list of columns to be dropped Returns: df_copy (pd.DataFrame): Dataframe with unwanted columns dropped. \"\"\" df_copy = df . copy () df_copy = df_copy . drop ( columns = columns , axis = 1 , inplace = False ) return df_copy . reset_index ( drop = True ) df = drop_columns ( df , columns = config . unwanted_cols ) Data Types Let us split the data types into a few unbrellas: Info Categorical Variables: diagnosis: The target variable diagnosis, although represented as a string in the dataframe, should be categorical! This is because machines do not really like working with \"strings\" and prefer your type to be of \"numbers\". We will map them to 0 and 1, representing benign and malignant respectively. Since the target variable is just two unique values, we can use a simple map from pandas to do the job. class_dict = { \"B\" : 0 , \"M\" : 1 } df [ 'diagnosis' ] = df [ 'diagnosis' ] . map ( class_dict ) We will make sure that our mapping is accurate by asserting the following. assert df [ 'diagnosis' ] . value_counts () . to_dict ()[ 0 ] == 357 assert df [ 'diagnosis' ] . value_counts () . to_dict ()[ 1 ] == 212 Info Continuous Variables: A preliminary look seems to suggest all our predictors are continuous. Success From the brief overview, there does not seem to be any Ordinal or Nominal Predictors. This suggest that we may not need to perform encoding in our preprocessing. Summary Statistics We will use a simple, yet powerful function call to check on the summary statistics of our dataframe. We note to the readers that there are much more powerful libraries like pandas-profiling to give us an even more thorough summary, but for our purpose, we will use the good ol' df.describe() . display ( df . describe ( include = 'all' )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst count 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 mean 0.372583 14.127292 19.289649 91.969033 654.889104 0.096360 0.104341 0.088799 0.048919 0.181162 0.062798 0.405172 1.216853 2.866059 40.337079 0.007041 0.025478 0.031894 0.011796 0.020542 0.003795 16.269190 25.677223 107.261213 880.583128 0.132369 0.254265 0.272188 0.114606 0.290076 0.083946 std 0.483918 3.524049 4.301036 24.298981 351.914129 0.014064 0.052813 0.079720 0.038803 0.027414 0.007060 0.277313 0.551648 2.021855 45.491006 0.003003 0.017908 0.030186 0.006170 0.008266 0.002646 4.833242 6.146258 33.602542 569.356993 0.022832 0.157336 0.208624 0.065732 0.061867 0.018061 min 0.000000 6.981000 9.710000 43.790000 143.500000 0.052630 0.019380 0.000000 0.000000 0.106000 0.049960 0.111500 0.360200 0.757000 6.802000 0.001713 0.002252 0.000000 0.000000 0.007882 0.000895 7.930000 12.020000 50.410000 185.200000 0.071170 0.027290 0.000000 0.000000 0.156500 0.055040 25% 0.000000 11.700000 16.170000 75.170000 420.300000 0.086370 0.064920 0.029560 0.020310 0.161900 0.057700 0.232400 0.833900 1.606000 17.850000 0.005169 0.013080 0.015090 0.007638 0.015160 0.002248 13.010000 21.080000 84.110000 515.300000 0.116600 0.147200 0.114500 0.064930 0.250400 0.071460 50% 0.000000 13.370000 18.840000 86.240000 551.100000 0.095870 0.092630 0.061540 0.033500 0.179200 0.061540 0.324200 1.108000 2.287000 24.530000 0.006380 0.020450 0.025890 0.010930 0.018730 0.003187 14.970000 25.410000 97.660000 686.500000 0.131300 0.211900 0.226700 0.099930 0.282200 0.080040 75% 1.000000 15.780000 21.800000 104.100000 782.700000 0.105300 0.130400 0.130700 0.074000 0.195700 0.066120 0.478900 1.474000 3.357000 45.190000 0.008146 0.032450 0.042050 0.014710 0.023480 0.004558 18.790000 29.720000 125.400000 1084.000000 0.146000 0.339100 0.382900 0.161400 0.317900 0.092080 max 1.000000 28.110000 39.280000 188.500000 2501.000000 0.163400 0.345400 0.426800 0.201200 0.304000 0.097440 2.873000 4.885000 21.980000 542.200000 0.031130 0.135400 0.396000 0.052790 0.078950 0.029840 36.040000 49.540000 251.200000 4254.000000 0.222600 1.058000 1.252000 0.291000 0.663800 0.207500 The table does give us a good overview: for example, a brief glance give me the following observations: The features do not seem to be of the same scale . This is going to be a problem as some models do not perform well if your features are not on the same scale. A prime example is a KNN model with Euclidean Distance as the distance metric, the difference in range of different features will be amplified with the squared term, and the feature with wider range will dominate the one with smaller range. From our dataset it we see that area_mean is very large and there is likely to be a squared term (possibly from radius_mean ), we can look into them later through EDA. Humans are more visual and that is why we still need EDA later to capture our attention on any anomaly from the dataset, and of course, if the dataset has many columns, then this summary statistics may even clog your progress if you were to read it line by line. Missing Data Missing Alert? Although from our analysis, we did not see any missing data, it is always good to remind ourselves to check it. A simple function that does the job is as follows. def report_missing ( df : pd . DataFrame , columns : List ) -> pd . DataFrame : \"\"\"A function to check for missing data. Args: df (pd.DataFrame): The DataFrame to check. columns (List): The columns to check. Returns: missing_data_df (pd.DataFrame): Returns a DataFrame that reports missing data. \"\"\" missing_dict = { \"missing num\" : [], \"missing percentage\" : []} for col in columns : num_missing = df [ col ] . isnull () . sum () percentage_missing = num_missing / len ( df ) missing_dict [ \"missing num\" ] . append ( num_missing ) missing_dict [ \"missing percentage\" ] . append ( percentage_missing ) missing_data_df = pd . DataFrame ( index = columns , data = missing_dict ) return missing_data_df missing_df = report_missing ( df , columns = df . columns ) display ( missing_df . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } missing num missing percentage diagnosis 0 0.0 radius_mean 0 0.0 texture_mean 0 0.0 perimeter_mean 0 0.0 area_mean 0 0.0 Save data We save the data to processed and we can call it later on in subsequent notebooks.","title":"Preliminary Data Inspection and Cleaning"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#dependencies-and-configuration","text":"import random from dataclasses import dataclass , field from typing import List , Dict import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed , } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) # set config config = config () # set seeding for reproducibility _ = set_seeds ( seed = config . seed )","title":"Dependencies and Configuration"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#stage-1-preliminary-data-inspection-and-cleaning","text":"","title":"Stage 1: Preliminary Data Inspection and Cleaning"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#load-the-dataset","text":"df = pd . read_csv ( config . raw_data )","title":"Load the dataset"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#a-brief-look-at-the-dataset","text":"Info We will query the first five rows of the dataframe to get a feel on the dataset we are working on. We also call df.info() to see the data types of the columns, and to briefly check if there is any missing values in our data (more on that later). # Column Non-Null Count Dtype --- ------ -------------- ----- 0 diagnosis 569 non - null int64 1 radius_mean 569 non - null float64 2 texture_mean 569 non - null float64 3 perimeter_mean 569 non - null float64 Importance of data types We must be sharp and ensure that each column is indeed stored in their respective data types! In the real world, we may often query \"dirty\" data from say, the database, where numeric data are represented in string. It is now our duty to ensure sanity checks are in place! display ( df . head ()) display ( df . info ( verbose = True )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 1.0950 0.9053 8.589 153.40 0.006399 0.04904 0.05373 0.01587 0.03003 0.006193 25.38 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 NaN 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 0.5435 0.7339 3.398 74.08 0.005225 0.01308 0.01860 0.01340 0.01389 0.003532 24.99 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 NaN 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 0.7456 0.7869 4.585 94.03 0.006150 0.04006 0.03832 0.02058 0.02250 0.004571 23.57 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 NaN 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744 0.4956 1.1560 3.445 27.23 0.009110 0.07458 0.05661 0.01867 0.05963 0.009208 14.91 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 NaN 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883 0.7572 0.7813 5.438 94.44 0.011490 0.02461 0.05688 0.01885 0.01756 0.005115 22.54 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 NaN <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 569 non-null int64 1 diagnosis 569 non-null object 2 radius_mean 569 non-null float64 3 texture_mean 569 non-null float64 4 perimeter_mean 569 non-null float64 5 area_mean 569 non-null float64 6 smoothness_mean 569 non-null float64 7 compactness_mean 569 non-null float64 8 concavity_mean 569 non-null float64 9 concave points_mean 569 non-null float64 10 symmetry_mean 569 non-null float64 11 fractal_dimension_mean 569 non-null float64 12 radius_se 569 non-null float64 13 texture_se 569 non-null float64 14 perimeter_se 569 non-null float64 15 area_se 569 non-null float64 16 smoothness_se 569 non-null float64 17 compactness_se 569 non-null float64 18 concavity_se 569 non-null float64 19 concave points_se 569 non-null float64 20 symmetry_se 569 non-null float64 21 fractal_dimension_se 569 non-null float64 22 radius_worst 569 non-null float64 23 texture_worst 569 non-null float64 24 perimeter_worst 569 non-null float64 25 area_worst 569 non-null float64 26 smoothness_worst 569 non-null float64 27 compactness_worst 569 non-null float64 28 concavity_worst 569 non-null float64 29 concave points_worst 569 non-null float64 30 symmetry_worst 569 non-null float64 31 fractal_dimension_worst 569 non-null float64 32 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB None A brief overview tells us our data is alright! There is, however, a column which is unnamed and has no values. This can be of various data source issues, for now, we quickly check the definition given by the dataset from UCI's Breast Cancer Wisconsin (Diagnostic) Data Set and confirm that there should only be 32 columns. With this in mind, we can safely delete the column. We also note that from the above, that the id column is the identifier for each patient. We will also drop this column as it holds no predictive power. When can ID be important? We should try to question our move and justify it. In this dataset, we have to ensure that each ID is unique, if it is not, it may suggest that there are patient records with multiple observation, which is a violation of i.i.d assumption and we may take note when doing cross-validation, so as to avoid information leakage. Since the ID column is unique, we will delete it. We will keep this at the back of our mind in the event that we ever need them for feature engineering. print ( f \"The ID column is unique : { df [ 'id' ] . is_unique } \" ) The ID column is unique : True","title":"A brief look at the dataset"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#drop-drop-drop-the-columns","text":"Here we define a drop_columns function to drop the unwanted columns. def drop_columns ( df : pd . DataFrame , columns : List ) -> pd . DataFrame : \"\"\"Drop unwanted columns from dataframe. Args: df (pd.DataFrame): Dataframe to be cleaned columns (List): list of columns to be dropped Returns: df_copy (pd.DataFrame): Dataframe with unwanted columns dropped. \"\"\" df_copy = df . copy () df_copy = df_copy . drop ( columns = columns , axis = 1 , inplace = False ) return df_copy . reset_index ( drop = True ) df = drop_columns ( df , columns = config . unwanted_cols )","title":"Drop, drop, drop the columns!"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#data-types","text":"Let us split the data types into a few unbrellas: Info Categorical Variables: diagnosis: The target variable diagnosis, although represented as a string in the dataframe, should be categorical! This is because machines do not really like working with \"strings\" and prefer your type to be of \"numbers\". We will map them to 0 and 1, representing benign and malignant respectively. Since the target variable is just two unique values, we can use a simple map from pandas to do the job. class_dict = { \"B\" : 0 , \"M\" : 1 } df [ 'diagnosis' ] = df [ 'diagnosis' ] . map ( class_dict ) We will make sure that our mapping is accurate by asserting the following. assert df [ 'diagnosis' ] . value_counts () . to_dict ()[ 0 ] == 357 assert df [ 'diagnosis' ] . value_counts () . to_dict ()[ 1 ] == 212 Info Continuous Variables: A preliminary look seems to suggest all our predictors are continuous. Success From the brief overview, there does not seem to be any Ordinal or Nominal Predictors. This suggest that we may not need to perform encoding in our preprocessing.","title":"Data Types"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#summary-statistics","text":"We will use a simple, yet powerful function call to check on the summary statistics of our dataframe. We note to the readers that there are much more powerful libraries like pandas-profiling to give us an even more thorough summary, but for our purpose, we will use the good ol' df.describe() . display ( df . describe ( include = 'all' )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst count 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 569.000000 mean 0.372583 14.127292 19.289649 91.969033 654.889104 0.096360 0.104341 0.088799 0.048919 0.181162 0.062798 0.405172 1.216853 2.866059 40.337079 0.007041 0.025478 0.031894 0.011796 0.020542 0.003795 16.269190 25.677223 107.261213 880.583128 0.132369 0.254265 0.272188 0.114606 0.290076 0.083946 std 0.483918 3.524049 4.301036 24.298981 351.914129 0.014064 0.052813 0.079720 0.038803 0.027414 0.007060 0.277313 0.551648 2.021855 45.491006 0.003003 0.017908 0.030186 0.006170 0.008266 0.002646 4.833242 6.146258 33.602542 569.356993 0.022832 0.157336 0.208624 0.065732 0.061867 0.018061 min 0.000000 6.981000 9.710000 43.790000 143.500000 0.052630 0.019380 0.000000 0.000000 0.106000 0.049960 0.111500 0.360200 0.757000 6.802000 0.001713 0.002252 0.000000 0.000000 0.007882 0.000895 7.930000 12.020000 50.410000 185.200000 0.071170 0.027290 0.000000 0.000000 0.156500 0.055040 25% 0.000000 11.700000 16.170000 75.170000 420.300000 0.086370 0.064920 0.029560 0.020310 0.161900 0.057700 0.232400 0.833900 1.606000 17.850000 0.005169 0.013080 0.015090 0.007638 0.015160 0.002248 13.010000 21.080000 84.110000 515.300000 0.116600 0.147200 0.114500 0.064930 0.250400 0.071460 50% 0.000000 13.370000 18.840000 86.240000 551.100000 0.095870 0.092630 0.061540 0.033500 0.179200 0.061540 0.324200 1.108000 2.287000 24.530000 0.006380 0.020450 0.025890 0.010930 0.018730 0.003187 14.970000 25.410000 97.660000 686.500000 0.131300 0.211900 0.226700 0.099930 0.282200 0.080040 75% 1.000000 15.780000 21.800000 104.100000 782.700000 0.105300 0.130400 0.130700 0.074000 0.195700 0.066120 0.478900 1.474000 3.357000 45.190000 0.008146 0.032450 0.042050 0.014710 0.023480 0.004558 18.790000 29.720000 125.400000 1084.000000 0.146000 0.339100 0.382900 0.161400 0.317900 0.092080 max 1.000000 28.110000 39.280000 188.500000 2501.000000 0.163400 0.345400 0.426800 0.201200 0.304000 0.097440 2.873000 4.885000 21.980000 542.200000 0.031130 0.135400 0.396000 0.052790 0.078950 0.029840 36.040000 49.540000 251.200000 4254.000000 0.222600 1.058000 1.252000 0.291000 0.663800 0.207500 The table does give us a good overview: for example, a brief glance give me the following observations: The features do not seem to be of the same scale . This is going to be a problem as some models do not perform well if your features are not on the same scale. A prime example is a KNN model with Euclidean Distance as the distance metric, the difference in range of different features will be amplified with the squared term, and the feature with wider range will dominate the one with smaller range. From our dataset it we see that area_mean is very large and there is likely to be a squared term (possibly from radius_mean ), we can look into them later through EDA. Humans are more visual and that is why we still need EDA later to capture our attention on any anomaly from the dataset, and of course, if the dataset has many columns, then this summary statistics may even clog your progress if you were to read it line by line.","title":"Summary Statistics"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#missing-data","text":"Missing Alert? Although from our analysis, we did not see any missing data, it is always good to remind ourselves to check it. A simple function that does the job is as follows. def report_missing ( df : pd . DataFrame , columns : List ) -> pd . DataFrame : \"\"\"A function to check for missing data. Args: df (pd.DataFrame): The DataFrame to check. columns (List): The columns to check. Returns: missing_data_df (pd.DataFrame): Returns a DataFrame that reports missing data. \"\"\" missing_dict = { \"missing num\" : [], \"missing percentage\" : []} for col in columns : num_missing = df [ col ] . isnull () . sum () percentage_missing = num_missing / len ( df ) missing_dict [ \"missing num\" ] . append ( num_missing ) missing_dict [ \"missing percentage\" ] . append ( percentage_missing ) missing_data_df = pd . DataFrame ( index = columns , data = missing_dict ) return missing_data_df missing_df = report_missing ( df , columns = df . columns ) display ( missing_df . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } missing num missing percentage diagnosis 0 0.0 radius_mean 0 0.0 texture_mean 0 0.0 perimeter_mean 0 0.0 area_mean 0 0.0","title":"Missing Data"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%201%20-%20Preliminary%20Data%20Inspection%20and%20Cleaning/#save-data","text":"We save the data to processed and we can call it later on in subsequent notebooks.","title":"Save data"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/","text":"Stage 2: Preliminary EDA by Hongnan Gao Quick Navigation Dependencies and Configuration Stage 2: EDA: Preliminary Stage Distribution of Target and Predictor Target Distribution Univariate Distribution of Predictors Histogram and KDE Distribution Box Plots Correlation Plots Heatmap Cluster Plot Bivariate Analysis PCE and TSNE Save the Data Dependencies and Configuration import random from dataclasses import dataclass , field from typing import Dict , List import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from scipy.cluster import hierarchy from scipy.spatial.distance import squareform from scipy.stats import pearsonr , spearmanr from sklearn import decomposition , manifold , preprocessing @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed , } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) # set config config = config () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data ) Stage 2: EDA (Preliminary Stage) Terminology Alert I call this stage the preliminary EDA for the following reasons (but not limited to!) - EDA is an iterative process! Data is never clean! So we scrub -> send in for modelling -> scrub -> get new data -> scrub scrub scrub again! - Typically, I do a basic EDA analysis after Stage 1. As an example, in a classical regression setting, I would check for some assumptions like normality checks. And if some variables are of a \"bad distribution\", we can perform a transformation on it first, and plot them again in the next round of EDA to check if we corrected the issues. The below image summarizes the process. Courtesy of Farcaster In summary, we will do the following EDA here: Target and Predictors Distribution. Univariate Analysis. Bivariate Analysis. Multivariate Analysis. Some fancy looking Dimensionality Reduction Techniques to see how our features can be visualized in lower dimensions. Distribution of Target and Predictor Target Let us first visualize our distribution of target variable. Since the target is discrete, we will just plot a simple count plot to check. Class Imbalance For Classification problems, we often plot the target count to get a good gauge on how imbalanced the dataset is. From there, we can think of whether the class imbalance is going to be a factor in our modelling process. df [ 'diagnosis' ] . value_counts ( normalize = True ) * 100 Malignant : 37.3 % Benign : 62.7 % There is to certain degree a class imbalance issue, although this is not what I have envisioned in the first place. After all, it is just natural that there should be more negative (benign) than positive (malignant) cases, given that in the general population, there are much more people without cancer than those who do. Nevertheless, despite the slight imbalance issue, we should still take them into account and perform stratification during cross-validation later. def plot_target_distribution ( df : pd . DataFrame , target : str , colors : List [ str ]) -> None : \"\"\"Plot Target Distribution with percentage labels. Args: df (pd.DataFrame): DataFrame to plot. target (str): Target column name. colors (List[str]): List of colors to use for plotting. \"\"\" plt . rcParams [ \"figure.dpi\" ] = 100 plt . rcParams [ \"savefig.dpi\" ] = 300 x_axis = df [ \"diagnosis\" ] . value_counts () . index y_axis = df [ \"diagnosis\" ] . value_counts () figure , target_bar = plt . subplots ( figsize = ( 6 , 4 )) bar = sns . barplot ( x = x_axis , y = y_axis , ax = target_bar , palette = { 1 : colors [ 0 ], 0 : colors [ 3 ]}) target_bar . set_xticklabels ([ \"Benign\" , \"Malignant\" ]) target_bar . set_ylabel ( \"Frequency Count\" ) target_bar . legend ([ \"Benign\" , \"Malignant\" ], loc = \"upper right\" ) target_bar . set_title ( \"Count of Target (Diagnosis)\" , fontsize = 16 ) figure . text ( x = 0.27 , y = 0.8 , s = \" {:.1f} %\" . format ( df [ \"diagnosis\" ] . value_counts ( normalize = True )[ 0 ] * 100 ), ** { \"weight\" : \"bold\" , \"color\" : \"black\" }, ) figure . text ( x = 0.66 , y = 0.5 , s = \" {:.1f} %\" . format ( df [ \"diagnosis\" ] . value_counts ( normalize = True )[ 1 ] * 100 ), ** { \"weight\" : \"bold\" , \"color\" : \"black\" }, ) plt . show () _ = plot_target_distribution ( df = df , target = config . target_col , colors = config . colors ) Univariate Distributions of Predictors Univariate Plot on Continuous Variables The predictors are all continuous variables. We can do a univariate plot to show the histogram/kde distribution of these features, we color the hue so that the distribution is parametrized by the target. In addition, we will also perform a box plot to check for potential outliers. predictor_cols = df . columns . to_list ()[ 1 :] Histogram and KDE distribution We can plot a histogram with KDE for each of the feature, parametrized by the target. The aim of this visual is to briefly see how skewed the features are, whether the features are gaussian, and also the distribution of each feature with respect to the target. def plot_univariate ( df : pd . DataFrame , predictor : str , colors : List [ str ]) -> None : \"\"\"Take in continuous predictors and plot univariate distribution. Note in this setting, we have kde=True. Args: df (pd.DataFrame): Dataframe. predictor (str): Predictor name. \"\"\" univariate_params = { \"nrows\" : 10 , \"ncols\" : 3 , \"figsize\" : ( 12 , 24 ), \"dpi\" : 80 } fig , axs = plt . subplots ( ** univariate_params ) for i , col in enumerate ( predictor ): sns . histplot ( data = df , x = col , kde = True , hue = \"diagnosis\" , ax = axs [ i % univariate_params [ \"nrows\" ]][ i // univariate_params [ \"nrows\" ]], legend = False , palette = { 1 : colors [ 0 ], 0 : colors [ 3 ]}, ) plt . subplots_adjust ( hspace = 2 ) fig . suptitle ( \"Breast Cancer Predictors Univariate Distribution\" , y = 1.01 , fontsize = \"x-large\" ) fig . legend ( df [ \"diagnosis\" ] . unique ()) fig . tight_layout () plt . show () _ = plot_univariate ( df = df , predictor = predictor_cols , colors = config . colors ) Univariate Insights From the plots above, we can form very basic hypothesis on the features, we select a few features to explain: - radius_mean: seems to have a clear boundary to discriminate between benign and malignant classes; in general, the bigger the radius mean, the higher the likelihood of the tumor being malignant; - smoothness_mean: judging the smoothness mean (jaggedness) alone , we can see there is quite a fair bit of overlaps in between the class distributions, the boundary in between both classes are not so clear when compared to radius_mean . This suggests that the feature alone may not distinguish whether a patient's tumor is malignant or not. However, seemingly \"non-informative\" features may become extremely useful when coupled with other features. We can see most graphs are skewed towards the right. Box Plots Although a good alternative to box plots is the violin plot, but we do have the the distribution of KDE earlier on, so we can just zoom in at the box plots to check for outliers. There are some outliers present in the features. Outliers Alert! Outliers are tricky, without domain knowledge, it is sometimes hard to tell whether or not an outlier should be removed. A rule of thumb is that if you are sure the outliers are caused by a labelling or human error, then you can remove them. Otherwise, we may need to investigate further to check if these outliers should be retained during modelling. def plot_univariate_boxplot ( df : pd . DataFrame , predictor : str ) -> None : \"\"\"Take in continuous predictors and plot univariate boxplot distribution. Note in this setting, we have kde=True. Args: df (pd.DataFrame): DataFrame. predictor (str): Predictor name. \"\"\" univariate_params = { \"nrows\" : 10 , \"ncols\" : 3 , \"figsize\" : ( 12 , 24 ), \"dpi\" : 80 } fig , axs = plt . subplots ( ** univariate_params ) for i , col in enumerate ( predictor ): sns . boxplot ( data = df , x = col , hue = \"diagnosis\" , ax = axs [ i % univariate_params [ \"nrows\" ]][ i // univariate_params [ \"nrows\" ]], ) plt . subplots_adjust ( hspace = 2 ) fig . suptitle ( \"Breast Cancer Predictors Boxplot Distribution\" , y = 1.01 , fontsize = \"x-large\" ) fig . legend ( df [ \"diagnosis\" ] . unique ()) fig . tight_layout () plt . show () predictor_cols = df . columns . to_list ()[ 1 :] _ = plot_univariate_boxplot ( df = df , predictor = predictor_cols ) Correlation Plots Through the definitions given on the features of the dataset, we know that there are some features that are correlated to each other. For example, we can even make some hypothesis before we plot. Hypothesis Radius Mean, Area Mean and Perimeter Mean are correlated. This makes sense as if a cell nucleus is approximately circle, then the radius \\(r\\) is related linearly with perimeter \\(2\\pi r\\) and quadratically related with area \\(\\pi r ^2\\) . Heatmap We can plot a simple correlation heatmap to visualize the \"hot spots\" in which the correlation value is high. def plot_heatmap ( df : pd . DataFrame , predictors : List [ str ], cmap : str ) -> pd . DataFrame : \"\"\"This function takes in a dataframe and a list of predictors, and output the correlation matrix, as well as a plot of heatmap. 1. Note that annot_kws attempts to make the size of the font visible and contained in the heatmap. 2. Note that the CMAP is reversed and darker color indicates higher correlation as I find this more intuitive. Args: df (pd.DataFrame): The dataframe to be plotted. predictors (List[str]): The list of predictors to be plotted. Returns: pd.DataFrame: [description] \"\"\" corr = df [ predictors ] . corr () annot_kws = { \"size\" : 35 / np . sqrt ( len ( corr ))} fig , _ = plt . subplots ( figsize = ( 16 , 12 )) sns . heatmap ( corr , annot = True , cmap = cmap , annot_kws = annot_kws ) return corr corr_matrix = plot_heatmap ( df = df , predictors = predictor_cols , cmap = config . cmap_reversed ) From the correlation plot above, we discovered quite a lot of features being correlated, indicating multi-collinearity. We can further strengthen our stance by using a clusterplot to check. Cluster Plot We can use a Hierarchical Clustering to visualize the correlated clusters, the Cluster Map outputs a Dendrogram and in our Seaborn plot, we used Ward's Linkage as our method and the distance metric is Euclidean's Distance. Hence, in the diagram below, the dendrogram implies A and B are more \"correlated\" than A and C. Courtesy of What is a Dendrogram? corr = df [ predictor_cols ] . corr () g = sns . clustermap ( corr , method = \"ward\" , metric = 'euclidean' , cmap = config . cmap_reversed ); Bivariate Analysis Clutter Alert! Now we have 30 predictors, if we plot all of them in the pairplots below, we will roughly have \\({30 \\choose 2} = 435\\) such figures, which is not really pleasing to look at. We can zoom in on a few predictors and see if we can find more insights. A good way is to be selective when you do pair plots. From the correlation plot above, we can further choose a set of correlated features and plot them. For our purpose, we will plot 10 features only, all of them are related to the mean of the features. The plot will have its diagonal conveniently displaying its univariate histogram and kde distribution, while the off-diagonal will show bivariate scatter plots. mean_cols = predictor_cols [: 10 ] def corrfunc ( x : np . ndarray , y : np . ndarray , ax = None , ** kws ) -> None : \"\"\"Plot the correlation coefficient in the top left hand corner of a plot. Args: x (np.ndarray): x-axis data. y (np.ndarray): y-axis data. ax ([type], optional): Defaults to None. Axes to plot on. \"\"\" r , _ = pearsonr ( x , y ) ax = ax or plt . gca () ax . annotate ( f \" { r : .1f } \" , xy = ( 0.7 , 0.15 ), xycoords = ax . transAxes ) pp_mean = sns . pairplot ( df , hue = \"diagnosis\" , vars = mean_cols , palette = { 1 : config . colors [ 0 ], 0 : config . colors [ 3 ]}, diag_kind = \"auto\" , corner = True , ) pp_mean = pp_mean . map_offdiag ( sns . scatterplot ) pp_mean = pp_mean . map_diag ( sns . histplot , kde = True ) pp_mean = pp_mean . map_lower ( corrfunc ) From the pairplot, we notice interesting things like the three \"good brothers\", radius, area and perimeter, whom are highly positively correlated. PCA, TSNE We can also plot PCA and TSNE to get a feel of how \"separable\" the data points are in 2 dimensional space. Standardization Alert A disclaimer/assumption here is that we are standardizing the full training set for the purpose of visualizations, and both PCA and TSNE benefit from data on the same scale. Note that preprocessing techniques such as standardization should not be applied to the test/hold-out set, as this will cause data leakage. def plot_dimensional_reduction ( df : pd . DataFrame , predictor_cols : List [ str ], colors : List [ str ] ): \"\"\"Plots PCA and TSNE for visualization of higher dimension to lower dimension. Args: df (pd.DataFrame): Dataframe to be plotted. predictor_cols (List[str]): List of predictor columns. colors (List[str]): List of colors for plotting. \"\"\" X_standardized = preprocessing . StandardScaler () . fit_transform ( df [ predictor_cols ]) # Binary classification: we can set n components to 2 to better visualize all features in 2 dimensions pca = decomposition . PCA ( n_components = 2 ) pca_2d = pca . fit_transform ( X_standardized ) tsne = manifold . TSNE ( n_components = 2 , verbose = 1 , perplexity = 40 , n_iter = 1500 ) tsne_2d = tsne . fit_transform ( X_standardized ) # Plot the TSNE and PCA visuals side-by-side plt . figure ( figsize = ( 16 , 11 )) plt . subplot ( 121 ) plt . scatter ( pca_2d [:, 0 ], pca_2d [:, 1 ], c = df [ \"diagnosis\" ], edgecolor = \"None\" , alpha = 0.35 ) plt . colorbar () plt . title ( \"PCA Scatter Plot\" ) plt . subplot ( 122 ) plt . scatter ( tsne_2d [:, 0 ], tsne_2d [:, 1 ], c = df [ \"diagnosis\" ], edgecolor = \"None\" , alpha = 0.35 , ) plt . colorbar () plt . title ( \"TSNE Scatter Plot\" ) plt . show () _ = plot_dimensional_reduction ( df = df , predictor_cols = predictor_cols , colors = [ config . colors [ 0 ], config . colors [ 1 ]]) [t-SNE] Computing 121 nearest neighbors... [t-SNE] Indexed 569 samples in 0.003s... [t-SNE] Computed neighbors for 569 samples in 0.052s... [t-SNE] Computed conditional probabilities for sample 569 / 569 [t-SNE] Mean sigma: 1.522404 [t-SNE] KL divergence after 250 iterations with early exaggeration: 63.866753 [t-SNE] KL divergence after 1450 iterations: 0.847863 Welp, the purpose of this plot is to show how well separated the data points are in 2d-space (binary classification). It does seem that TSNE can distinguish the clusters clearer than PCA. This may suggest that your data points are non-linear, which is one assumption that PCA takes. We are not restricted to only linear models, so this is fine!","title":"EDA"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#dependencies-and-configuration","text":"import random from dataclasses import dataclass , field from typing import Dict , List import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from scipy.cluster import hierarchy from scipy.spatial.distance import squareform from scipy.stats import pearsonr , spearmanr from sklearn import decomposition , manifold , preprocessing @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed , } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) # set config config = config () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data )","title":"Dependencies and Configuration"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#stage-2-eda-preliminary-stage","text":"Terminology Alert I call this stage the preliminary EDA for the following reasons (but not limited to!) - EDA is an iterative process! Data is never clean! So we scrub -> send in for modelling -> scrub -> get new data -> scrub scrub scrub again! - Typically, I do a basic EDA analysis after Stage 1. As an example, in a classical regression setting, I would check for some assumptions like normality checks. And if some variables are of a \"bad distribution\", we can perform a transformation on it first, and plot them again in the next round of EDA to check if we corrected the issues. The below image summarizes the process. Courtesy of Farcaster In summary, we will do the following EDA here: Target and Predictors Distribution. Univariate Analysis. Bivariate Analysis. Multivariate Analysis. Some fancy looking Dimensionality Reduction Techniques to see how our features can be visualized in lower dimensions.","title":"Stage 2: EDA (Preliminary Stage)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#distribution-of-target-and-predictor","text":"","title":"Distribution of Target and Predictor"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#target","text":"Let us first visualize our distribution of target variable. Since the target is discrete, we will just plot a simple count plot to check. Class Imbalance For Classification problems, we often plot the target count to get a good gauge on how imbalanced the dataset is. From there, we can think of whether the class imbalance is going to be a factor in our modelling process. df [ 'diagnosis' ] . value_counts ( normalize = True ) * 100 Malignant : 37.3 % Benign : 62.7 % There is to certain degree a class imbalance issue, although this is not what I have envisioned in the first place. After all, it is just natural that there should be more negative (benign) than positive (malignant) cases, given that in the general population, there are much more people without cancer than those who do. Nevertheless, despite the slight imbalance issue, we should still take them into account and perform stratification during cross-validation later. def plot_target_distribution ( df : pd . DataFrame , target : str , colors : List [ str ]) -> None : \"\"\"Plot Target Distribution with percentage labels. Args: df (pd.DataFrame): DataFrame to plot. target (str): Target column name. colors (List[str]): List of colors to use for plotting. \"\"\" plt . rcParams [ \"figure.dpi\" ] = 100 plt . rcParams [ \"savefig.dpi\" ] = 300 x_axis = df [ \"diagnosis\" ] . value_counts () . index y_axis = df [ \"diagnosis\" ] . value_counts () figure , target_bar = plt . subplots ( figsize = ( 6 , 4 )) bar = sns . barplot ( x = x_axis , y = y_axis , ax = target_bar , palette = { 1 : colors [ 0 ], 0 : colors [ 3 ]}) target_bar . set_xticklabels ([ \"Benign\" , \"Malignant\" ]) target_bar . set_ylabel ( \"Frequency Count\" ) target_bar . legend ([ \"Benign\" , \"Malignant\" ], loc = \"upper right\" ) target_bar . set_title ( \"Count of Target (Diagnosis)\" , fontsize = 16 ) figure . text ( x = 0.27 , y = 0.8 , s = \" {:.1f} %\" . format ( df [ \"diagnosis\" ] . value_counts ( normalize = True )[ 0 ] * 100 ), ** { \"weight\" : \"bold\" , \"color\" : \"black\" }, ) figure . text ( x = 0.66 , y = 0.5 , s = \" {:.1f} %\" . format ( df [ \"diagnosis\" ] . value_counts ( normalize = True )[ 1 ] * 100 ), ** { \"weight\" : \"bold\" , \"color\" : \"black\" }, ) plt . show () _ = plot_target_distribution ( df = df , target = config . target_col , colors = config . colors )","title":"Target"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#univariate-distributions-of-predictors","text":"Univariate Plot on Continuous Variables The predictors are all continuous variables. We can do a univariate plot to show the histogram/kde distribution of these features, we color the hue so that the distribution is parametrized by the target. In addition, we will also perform a box plot to check for potential outliers. predictor_cols = df . columns . to_list ()[ 1 :]","title":"Univariate Distributions of Predictors"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#histogram-and-kde-distribution","text":"We can plot a histogram with KDE for each of the feature, parametrized by the target. The aim of this visual is to briefly see how skewed the features are, whether the features are gaussian, and also the distribution of each feature with respect to the target. def plot_univariate ( df : pd . DataFrame , predictor : str , colors : List [ str ]) -> None : \"\"\"Take in continuous predictors and plot univariate distribution. Note in this setting, we have kde=True. Args: df (pd.DataFrame): Dataframe. predictor (str): Predictor name. \"\"\" univariate_params = { \"nrows\" : 10 , \"ncols\" : 3 , \"figsize\" : ( 12 , 24 ), \"dpi\" : 80 } fig , axs = plt . subplots ( ** univariate_params ) for i , col in enumerate ( predictor ): sns . histplot ( data = df , x = col , kde = True , hue = \"diagnosis\" , ax = axs [ i % univariate_params [ \"nrows\" ]][ i // univariate_params [ \"nrows\" ]], legend = False , palette = { 1 : colors [ 0 ], 0 : colors [ 3 ]}, ) plt . subplots_adjust ( hspace = 2 ) fig . suptitle ( \"Breast Cancer Predictors Univariate Distribution\" , y = 1.01 , fontsize = \"x-large\" ) fig . legend ( df [ \"diagnosis\" ] . unique ()) fig . tight_layout () plt . show () _ = plot_univariate ( df = df , predictor = predictor_cols , colors = config . colors ) Univariate Insights From the plots above, we can form very basic hypothesis on the features, we select a few features to explain: - radius_mean: seems to have a clear boundary to discriminate between benign and malignant classes; in general, the bigger the radius mean, the higher the likelihood of the tumor being malignant; - smoothness_mean: judging the smoothness mean (jaggedness) alone , we can see there is quite a fair bit of overlaps in between the class distributions, the boundary in between both classes are not so clear when compared to radius_mean . This suggests that the feature alone may not distinguish whether a patient's tumor is malignant or not. However, seemingly \"non-informative\" features may become extremely useful when coupled with other features. We can see most graphs are skewed towards the right.","title":"Histogram and KDE distribution"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#box-plots","text":"Although a good alternative to box plots is the violin plot, but we do have the the distribution of KDE earlier on, so we can just zoom in at the box plots to check for outliers. There are some outliers present in the features. Outliers Alert! Outliers are tricky, without domain knowledge, it is sometimes hard to tell whether or not an outlier should be removed. A rule of thumb is that if you are sure the outliers are caused by a labelling or human error, then you can remove them. Otherwise, we may need to investigate further to check if these outliers should be retained during modelling. def plot_univariate_boxplot ( df : pd . DataFrame , predictor : str ) -> None : \"\"\"Take in continuous predictors and plot univariate boxplot distribution. Note in this setting, we have kde=True. Args: df (pd.DataFrame): DataFrame. predictor (str): Predictor name. \"\"\" univariate_params = { \"nrows\" : 10 , \"ncols\" : 3 , \"figsize\" : ( 12 , 24 ), \"dpi\" : 80 } fig , axs = plt . subplots ( ** univariate_params ) for i , col in enumerate ( predictor ): sns . boxplot ( data = df , x = col , hue = \"diagnosis\" , ax = axs [ i % univariate_params [ \"nrows\" ]][ i // univariate_params [ \"nrows\" ]], ) plt . subplots_adjust ( hspace = 2 ) fig . suptitle ( \"Breast Cancer Predictors Boxplot Distribution\" , y = 1.01 , fontsize = \"x-large\" ) fig . legend ( df [ \"diagnosis\" ] . unique ()) fig . tight_layout () plt . show () predictor_cols = df . columns . to_list ()[ 1 :] _ = plot_univariate_boxplot ( df = df , predictor = predictor_cols )","title":"Box Plots"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#correlation-plots","text":"Through the definitions given on the features of the dataset, we know that there are some features that are correlated to each other. For example, we can even make some hypothesis before we plot. Hypothesis Radius Mean, Area Mean and Perimeter Mean are correlated. This makes sense as if a cell nucleus is approximately circle, then the radius \\(r\\) is related linearly with perimeter \\(2\\pi r\\) and quadratically related with area \\(\\pi r ^2\\) .","title":"Correlation Plots"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#heatmap","text":"We can plot a simple correlation heatmap to visualize the \"hot spots\" in which the correlation value is high. def plot_heatmap ( df : pd . DataFrame , predictors : List [ str ], cmap : str ) -> pd . DataFrame : \"\"\"This function takes in a dataframe and a list of predictors, and output the correlation matrix, as well as a plot of heatmap. 1. Note that annot_kws attempts to make the size of the font visible and contained in the heatmap. 2. Note that the CMAP is reversed and darker color indicates higher correlation as I find this more intuitive. Args: df (pd.DataFrame): The dataframe to be plotted. predictors (List[str]): The list of predictors to be plotted. Returns: pd.DataFrame: [description] \"\"\" corr = df [ predictors ] . corr () annot_kws = { \"size\" : 35 / np . sqrt ( len ( corr ))} fig , _ = plt . subplots ( figsize = ( 16 , 12 )) sns . heatmap ( corr , annot = True , cmap = cmap , annot_kws = annot_kws ) return corr corr_matrix = plot_heatmap ( df = df , predictors = predictor_cols , cmap = config . cmap_reversed ) From the correlation plot above, we discovered quite a lot of features being correlated, indicating multi-collinearity. We can further strengthen our stance by using a clusterplot to check.","title":"Heatmap"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#cluster-plot","text":"We can use a Hierarchical Clustering to visualize the correlated clusters, the Cluster Map outputs a Dendrogram and in our Seaborn plot, we used Ward's Linkage as our method and the distance metric is Euclidean's Distance. Hence, in the diagram below, the dendrogram implies A and B are more \"correlated\" than A and C. Courtesy of What is a Dendrogram? corr = df [ predictor_cols ] . corr () g = sns . clustermap ( corr , method = \"ward\" , metric = 'euclidean' , cmap = config . cmap_reversed );","title":"Cluster Plot"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#bivariate-analysis","text":"Clutter Alert! Now we have 30 predictors, if we plot all of them in the pairplots below, we will roughly have \\({30 \\choose 2} = 435\\) such figures, which is not really pleasing to look at. We can zoom in on a few predictors and see if we can find more insights. A good way is to be selective when you do pair plots. From the correlation plot above, we can further choose a set of correlated features and plot them. For our purpose, we will plot 10 features only, all of them are related to the mean of the features. The plot will have its diagonal conveniently displaying its univariate histogram and kde distribution, while the off-diagonal will show bivariate scatter plots. mean_cols = predictor_cols [: 10 ] def corrfunc ( x : np . ndarray , y : np . ndarray , ax = None , ** kws ) -> None : \"\"\"Plot the correlation coefficient in the top left hand corner of a plot. Args: x (np.ndarray): x-axis data. y (np.ndarray): y-axis data. ax ([type], optional): Defaults to None. Axes to plot on. \"\"\" r , _ = pearsonr ( x , y ) ax = ax or plt . gca () ax . annotate ( f \" { r : .1f } \" , xy = ( 0.7 , 0.15 ), xycoords = ax . transAxes ) pp_mean = sns . pairplot ( df , hue = \"diagnosis\" , vars = mean_cols , palette = { 1 : config . colors [ 0 ], 0 : config . colors [ 3 ]}, diag_kind = \"auto\" , corner = True , ) pp_mean = pp_mean . map_offdiag ( sns . scatterplot ) pp_mean = pp_mean . map_diag ( sns . histplot , kde = True ) pp_mean = pp_mean . map_lower ( corrfunc ) From the pairplot, we notice interesting things like the three \"good brothers\", radius, area and perimeter, whom are highly positively correlated.","title":"Bivariate Analysis"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%202%20-%20Preliminary%20EDA/#pca-tsne","text":"We can also plot PCA and TSNE to get a feel of how \"separable\" the data points are in 2 dimensional space. Standardization Alert A disclaimer/assumption here is that we are standardizing the full training set for the purpose of visualizations, and both PCA and TSNE benefit from data on the same scale. Note that preprocessing techniques such as standardization should not be applied to the test/hold-out set, as this will cause data leakage. def plot_dimensional_reduction ( df : pd . DataFrame , predictor_cols : List [ str ], colors : List [ str ] ): \"\"\"Plots PCA and TSNE for visualization of higher dimension to lower dimension. Args: df (pd.DataFrame): Dataframe to be plotted. predictor_cols (List[str]): List of predictor columns. colors (List[str]): List of colors for plotting. \"\"\" X_standardized = preprocessing . StandardScaler () . fit_transform ( df [ predictor_cols ]) # Binary classification: we can set n components to 2 to better visualize all features in 2 dimensions pca = decomposition . PCA ( n_components = 2 ) pca_2d = pca . fit_transform ( X_standardized ) tsne = manifold . TSNE ( n_components = 2 , verbose = 1 , perplexity = 40 , n_iter = 1500 ) tsne_2d = tsne . fit_transform ( X_standardized ) # Plot the TSNE and PCA visuals side-by-side plt . figure ( figsize = ( 16 , 11 )) plt . subplot ( 121 ) plt . scatter ( pca_2d [:, 0 ], pca_2d [:, 1 ], c = df [ \"diagnosis\" ], edgecolor = \"None\" , alpha = 0.35 ) plt . colorbar () plt . title ( \"PCA Scatter Plot\" ) plt . subplot ( 122 ) plt . scatter ( tsne_2d [:, 0 ], tsne_2d [:, 1 ], c = df [ \"diagnosis\" ], edgecolor = \"None\" , alpha = 0.35 , ) plt . colorbar () plt . title ( \"TSNE Scatter Plot\" ) plt . show () _ = plot_dimensional_reduction ( df = df , predictor_cols = predictor_cols , colors = [ config . colors [ 0 ], config . colors [ 1 ]]) [t-SNE] Computing 121 nearest neighbors... [t-SNE] Indexed 569 samples in 0.003s... [t-SNE] Computed neighbors for 569 samples in 0.052s... [t-SNE] Computed conditional probabilities for sample 569 / 569 [t-SNE] Mean sigma: 1.522404 [t-SNE] KL divergence after 250 iterations with early exaggeration: 63.866753 [t-SNE] KL divergence after 1450 iterations: 0.847863 Welp, the purpose of this plot is to show how well separated the data points are in 2d-space (binary classification). It does seem that TSNE can distinguish the clusters clearer than PCA. This may suggest that your data points are non-linear, which is one assumption that PCA takes. We are not restricted to only linear models, so this is fine!","title":"PCA, TSNE"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/","text":"Stage 3: Feature Engineering by Hongnan Gao Quick Navigation Dependencies and Configuration Stage 3: Feature Engineering/Feature Selection Multicollinearity and Feature Selection Target Distribution Using Statsmodels Variance Inflation Factor Oh Dear, we have a Multicollinearity Problem Save the Data Dependencies and Configuration import random from collections import defaultdict from dataclasses import dataclass , field from typing import Dict , List , Union import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn import ( base , decomposition , linear_model , manifold , metrics , preprocessing ) from statsmodels.stats.outliers_influence import variance_inflation_factor /usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed , } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) config = config () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data ) Stage 3: Feature Engineering/Feature Selection Foreward on Data Leakage We are fully aware that oftentimes practitioner may accidentally cause data leakage during preprocessing, for example, a subtle yet often mistake is to standardize the whole dataset prior to splitting, or performing feature selection prior to modelling using the information of our response/target variable. However, this does not mean we should not do any preprocessing before modelling, instead, in the case of removing multicollinearity features, we can still screen predictors for multicollinearity during EDA phase and have a good intuition on which predictors are highly correlated - subsequently, we will incorporate feature selection techniques in our modelling pipeline. Multicollinearity and Feature Selection Why Feature Selection? We need feature selection in certain problems for the following reasons: - Well, one would definitely have heard of the dreaded Curse of Dimensionality in the journey of learning Machine Learning where having too many predictor/features can lead to overfitting; on the other hand, too many dimensions can cause distance between observations to appear equidistance from one another. The equidistance phenomenon causes observations to become harder to cluster, thereby clogging the model's ability to cluster data points (imagine the horror if you use KNN on 1000 dimensions, all the points will be almost the same distance from each other, poor KNN will not know how to predict now). - In case you have access to Google's GPU clusters, you likely want to train your model faster. Reducing the number predictors can aid this process. - Reducing uninformative features may aid in model's performance, the idea is to remove unnecessary noise from the dataset. Multi-Collinearity Looking back at our dataset, it is clear to me that there are quite a number of features that are correlated with each other, causing multi-collinearity. Multi-Collinearity is an issue in the history of Linear Models, as quoted 1 Consider the simplest case where Y is regressed against X and Z and where X and Z are highly positively correlated. Then the effect of X on Y is hard to distinguish from the effect of Z on Y because any increase in X tends to be associated with an increase in Z. We also note that multi-collinearity is not that big of a problem for non-parametric models such as Decision Tree or Random Forests, however, I will attempt to show that it is still best to avoid in this problem setting. Feature Selection Methods There are many methods to perform feature selection. Scikit-Learn offers some of the following: - Univariate feature selection. - Recursive feature elimination. - Backward Elimination of features using Hypothesis Testing. We need to be careful when selecting features before cross-validation. It is therefore, recommended to include feature selection in cross-validation to avoid any \"bias\" introduced before model selection phase! I decided to use the good old Variance Inflation Factor (VIF) as a way to reduce multicollinearity. Unfortunately, there is no out-of-the-box function to integrate into the Pipeline of scikit-learn. Thus, I heavily modified an existing code in order achieve what I want below. Variance Inflation Factor A classical way to check for multicollinearity amongst predictors is to calculate the Variable Inflation Factor (VIF). It is simply done by regressing each predictor \\(\\mathrm{x}_i\\) against all other predictors \\(\\mathrm{x}_j, j \\neq i\\) . In other words, the VIF for a predictor variable \\(i\\) is given by: \\[\\text{VIF}_i = \\dfrac{1}{1 - R^{2}_{i}}\\] where \\(R^{2}_{i}\\) is, by definition, the proportion of the variation in the \"dependent variable\" \\(\\mathrm{x}_i\\) that is predictable from the indepedent predictors \\(\\mathrm{x}_j, j \\neq i\\) . Consequently, the higher the \\(R^2_i\\) of a predictor, the higher the VIF, and this indicates there is linear dependence among predictors. Using Statsmodels Variance Inflation Factor Note that we need to perform scaling first before fitting our ReduceVIF to get the exact same result as the previous version. In this version, I manually added a hard threshold for the number of features remaining to be 15. This hard coded number can be turned into a parameter (hyperparameter) in our pipeline. import numpy as np import pandas as pd from sklearn import base from statsmodels.regression.linear_model import OLS def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : print ( f \"Droppingggggg { max_vif_col } with vif= { max_vif } \" ) column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names We do a sanity check if this coincides with the previous defined class, and the results are the same. predictor_cols = df . columns [ 1 :] transformer = ReduceVIF () scaler = preprocessing . StandardScaler () X = scaler . fit_transform ( df [ predictor_cols ]) # Only use 10 columns for speed in this example X = transformer . fit_transform ( X ) print ( f \"Remaining Features: { transformer . column_indices_kept_ } \" ) Droppingggggg 0 with vif=3806.1152963979675 Droppingggggg 20 with vif=616.3508614719424 Droppingggggg 2 with vif=325.64131198187516 Droppingggggg 22 with vif=123.25781086343038 Droppingggggg 6 with vif=64.65479584770004 Droppingggggg 10 with vif=35.61751844352034 Droppingggggg 25 with vif=33.96063880508537 Droppingggggg 27 with vif=30.596655364834078 Droppingggggg 3 with vif=25.387829695531458 Droppingggggg 5 with vif=18.843208489973282 Droppingggggg 21 with vif=17.232376192128665 Droppingggggg 13 with vif=16.333806476471736 Droppingggggg 26 with vif=15.510661467365699 Remaining Features: [1, 4, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19, 23, 24, 28, 29] We have the remaining indices, and therefore simply use numpy to subset the column indices to get back the original column names that are kept. vif_df = pd . DataFrame ({ 'Predictors' : predictor_cols [ transformer . column_indices_kept_ ]}) display ( vif_df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Predictors 0 texture_mean 1 smoothness_mean 2 concave points_mean 3 symmetry_mean 4 fractal_dimension_mean 5 texture_se 6 perimeter_se 7 smoothness_se 8 compactness_se 9 concavity_se 10 concave points_se 11 symmetry_se 12 fractal_dimension_se 13 area_worst 14 smoothness_worst 15 symmetry_worst 16 fractal_dimension_worst Oh Dear, we have a Multicollinearity Problem Using VIF in Modelling Pipeline At this step, we are just showing how we can remove multicollinear features using VIF; but we will not remove them at this point in time. We will incorporate this feature selection technique in our Cross-Validation pipeline in order to avoid data leakage. Save the Data In this phase, we did not make any changes to the predictor or target columns. So there is nothing to save. https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r \u21a9","title":"Feature Engineering"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#dependencies-and-configuration","text":"import random from collections import defaultdict from dataclasses import dataclass , field from typing import Dict , List , Union import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn import ( base , decomposition , linear_model , manifold , metrics , preprocessing ) from statsmodels.stats.outliers_influence import variance_inflation_factor /usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed , } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) config = config () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data )","title":"Dependencies and Configuration"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#stage-3-feature-engineeringfeature-selection","text":"Foreward on Data Leakage We are fully aware that oftentimes practitioner may accidentally cause data leakage during preprocessing, for example, a subtle yet often mistake is to standardize the whole dataset prior to splitting, or performing feature selection prior to modelling using the information of our response/target variable. However, this does not mean we should not do any preprocessing before modelling, instead, in the case of removing multicollinearity features, we can still screen predictors for multicollinearity during EDA phase and have a good intuition on which predictors are highly correlated - subsequently, we will incorporate feature selection techniques in our modelling pipeline.","title":"Stage 3: Feature Engineering/Feature Selection"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#multicollinearity-and-feature-selection","text":"Why Feature Selection? We need feature selection in certain problems for the following reasons: - Well, one would definitely have heard of the dreaded Curse of Dimensionality in the journey of learning Machine Learning where having too many predictor/features can lead to overfitting; on the other hand, too many dimensions can cause distance between observations to appear equidistance from one another. The equidistance phenomenon causes observations to become harder to cluster, thereby clogging the model's ability to cluster data points (imagine the horror if you use KNN on 1000 dimensions, all the points will be almost the same distance from each other, poor KNN will not know how to predict now). - In case you have access to Google's GPU clusters, you likely want to train your model faster. Reducing the number predictors can aid this process. - Reducing uninformative features may aid in model's performance, the idea is to remove unnecessary noise from the dataset. Multi-Collinearity Looking back at our dataset, it is clear to me that there are quite a number of features that are correlated with each other, causing multi-collinearity. Multi-Collinearity is an issue in the history of Linear Models, as quoted 1 Consider the simplest case where Y is regressed against X and Z and where X and Z are highly positively correlated. Then the effect of X on Y is hard to distinguish from the effect of Z on Y because any increase in X tends to be associated with an increase in Z. We also note that multi-collinearity is not that big of a problem for non-parametric models such as Decision Tree or Random Forests, however, I will attempt to show that it is still best to avoid in this problem setting. Feature Selection Methods There are many methods to perform feature selection. Scikit-Learn offers some of the following: - Univariate feature selection. - Recursive feature elimination. - Backward Elimination of features using Hypothesis Testing. We need to be careful when selecting features before cross-validation. It is therefore, recommended to include feature selection in cross-validation to avoid any \"bias\" introduced before model selection phase! I decided to use the good old Variance Inflation Factor (VIF) as a way to reduce multicollinearity. Unfortunately, there is no out-of-the-box function to integrate into the Pipeline of scikit-learn. Thus, I heavily modified an existing code in order achieve what I want below.","title":"Multicollinearity and Feature Selection"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#variance-inflation-factor","text":"A classical way to check for multicollinearity amongst predictors is to calculate the Variable Inflation Factor (VIF). It is simply done by regressing each predictor \\(\\mathrm{x}_i\\) against all other predictors \\(\\mathrm{x}_j, j \\neq i\\) . In other words, the VIF for a predictor variable \\(i\\) is given by: \\[\\text{VIF}_i = \\dfrac{1}{1 - R^{2}_{i}}\\] where \\(R^{2}_{i}\\) is, by definition, the proportion of the variation in the \"dependent variable\" \\(\\mathrm{x}_i\\) that is predictable from the indepedent predictors \\(\\mathrm{x}_j, j \\neq i\\) . Consequently, the higher the \\(R^2_i\\) of a predictor, the higher the VIF, and this indicates there is linear dependence among predictors.","title":"Variance Inflation Factor"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#using-statsmodels-variance-inflation-factor","text":"Note that we need to perform scaling first before fitting our ReduceVIF to get the exact same result as the previous version. In this version, I manually added a hard threshold for the number of features remaining to be 15. This hard coded number can be turned into a parameter (hyperparameter) in our pipeline. import numpy as np import pandas as pd from sklearn import base from statsmodels.regression.linear_model import OLS def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : print ( f \"Droppingggggg { max_vif_col } with vif= { max_vif } \" ) column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names We do a sanity check if this coincides with the previous defined class, and the results are the same. predictor_cols = df . columns [ 1 :] transformer = ReduceVIF () scaler = preprocessing . StandardScaler () X = scaler . fit_transform ( df [ predictor_cols ]) # Only use 10 columns for speed in this example X = transformer . fit_transform ( X ) print ( f \"Remaining Features: { transformer . column_indices_kept_ } \" ) Droppingggggg 0 with vif=3806.1152963979675 Droppingggggg 20 with vif=616.3508614719424 Droppingggggg 2 with vif=325.64131198187516 Droppingggggg 22 with vif=123.25781086343038 Droppingggggg 6 with vif=64.65479584770004 Droppingggggg 10 with vif=35.61751844352034 Droppingggggg 25 with vif=33.96063880508537 Droppingggggg 27 with vif=30.596655364834078 Droppingggggg 3 with vif=25.387829695531458 Droppingggggg 5 with vif=18.843208489973282 Droppingggggg 21 with vif=17.232376192128665 Droppingggggg 13 with vif=16.333806476471736 Droppingggggg 26 with vif=15.510661467365699 Remaining Features: [1, 4, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19, 23, 24, 28, 29] We have the remaining indices, and therefore simply use numpy to subset the column indices to get back the original column names that are kept. vif_df = pd . DataFrame ({ 'Predictors' : predictor_cols [ transformer . column_indices_kept_ ]}) display ( vif_df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Predictors 0 texture_mean 1 smoothness_mean 2 concave points_mean 3 symmetry_mean 4 fractal_dimension_mean 5 texture_se 6 perimeter_se 7 smoothness_se 8 compactness_se 9 concavity_se 10 concave points_se 11 symmetry_se 12 fractal_dimension_se 13 area_worst 14 smoothness_worst 15 symmetry_worst 16 fractal_dimension_worst","title":"Using Statsmodels Variance Inflation Factor"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#oh-dear-we-have-a-multicollinearity-problem","text":"Using VIF in Modelling Pipeline At this step, we are just showing how we can remove multicollinear features using VIF; but we will not remove them at this point in time. We will incorporate this feature selection technique in our Cross-Validation pipeline in order to avoid data leakage.","title":"Oh Dear, we have a Multicollinearity Problem"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%203%20-%20Feature%20Engineering/#save-the-data","text":"In this phase, we did not make any changes to the predictor or target columns. So there is nothing to save. https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r \u21a9","title":"Save the Data"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/","text":"Stage 4: Modelling (Metrics) by Hongnan Gao Define Metrics Disclaimer: For a more detailed understanding of different metrics, do navigate to my self-made notes on metrics here . Choosing a metric to measure the classifier's (hypothesis) performance is important, as choosing the wrong one can lead to disastrous interpretations. One prime example is using the accuracy metric for imbalanced datasets; consider 1 mil data points, dichotomized by \\(99\\%\\) benign and \\(1\\%\\) malignant samples, even a baseline model zeroR model which predicts the majority class no matter the process will give a \\(99\\%\\) accuracy, completely missing out any positive samples, which unfortunately, is what we may be more interested in. Say No to Accuracy: Consider an imbalanced set, where the training data set has 100 patients (data points), and the ground truth is 90 patients are of class = 0, which means that these patients do not have cancer, whereas the remaining 10 patients are in class 1, where they do have cancer. This is an example of class imbalance where the ratio of class 1 to class 0 is 1:9. Consider a baseline (almost trivial) classifier : def zeroR ( patient_data ): training ... return benign where we predict the patient's class as the most frequent class. Meaning, the most frequent class in this question is the class = 0, where patients do not have cancer, so we just assign this class to everyone in this set. By doing this, we will inevitably achieve a in-sample accuracy rate of \\(\\frac{90}{100} = 90\\%\\) . But unfortunately, this supposedly high accuracy value is completely useless, because this classifier did not label any of the cancer patients correctly. The consequence can be serious, assuming the test set has the same distribution as our training set, where if we have a test set of 1000 patients, there are 900 negative and 100 positive. Our model just literally predict every one of them as benign, yielding a \\(90\\%\\) out-of-sample accuracy. What did we conclude? Well, for one, our accuracy can be 90% high and looks good to the laymen, but it failed to predict the most important class of people - yes, misclassifying true cancer patients as healthy people is very bad! For the reasons mentioned above, we will use metric that can help us reduce False Negatives, and at the same time, outputs meaningful predictions. In order to achieve for both, we will use Receiver operating characteristic (ROC) as the primary metric for the model to maximize (which is our \\(\\mathcal{M}\\) , and Brier Score , a proper scoring rule to measure the performance of our probabilistic predictions. We will go into some details in the next two subsections to justify our choice. Proper Scoring Rule The math behind the idea of Proper Scoring Rule is non-trivial. Here, we try to understand why a proper scoring rule is desired in the context of binary classification. Strictly Proper Scoring Rule: Brier Score Loss, for example, tells us that the best possible score, 0 (lowest loss), is obtained if and only if, the probability prediction we get for a sample, is the true probability itself. In other words, if a selected sample is of class 1, our prediction for this must be 1, with 100% probability, in order to get a score loss of 0. Proper Scoring Rule: Read [here](https://stats.stackexchange.com/questions/339919/what-does-it-mean-that-auc-is-a-semi-proper-scoring-rule) for this. Semi Proper Scoring Rule: AUROC, as mentioned, does not help out in telling whether a prediction by a classifier is close to the true probability or not. In our example, we even see that we can obtain a full score of 1, even if the probabilities all lie within 0.51 and 0.52. Improper Scoring Rule: Accuracy is a prime example, the accuracy score does not, whatsoever, tells us about how close our predicted probabilities are, to the true probability distribution of our samples. Receiver operating characteristic (ROC) Definition: The basic (non-probablistic intepretation) of ROC is graph that plots the True Positive Rate on the y-axis and False Positive Rate on the x-axis parametrized by a threshold vector $\\vec{t}$. We then look at the area under the ROC curve (AUROC) to get an overall performance measure. The choice of ROC over other metrics such as Accuracy is detailed initially. We also established we want to reduce False Negative (FN), since misclassifying a positive patient as benign is way more costly than the other way round. One can choose to minimize Recall in order to reduce FN, but this is less than ideal during training because it is a thresholded metric, and does not provide at which threshold the recall is at minimum. This leads us to choose ROC for the following two main reasons: Threshold Invariant By definition, ROC computes the pair \\(TPR \\times FPR\\) over all thresholds \\(t\\) , consequently, the AUROC is threshold invariant, allowing us to look at the model's performance over all thresholds. We note that ROC may not be that reliable in the case of very imbalanced datasets where majority is in the negative class, as \\(FPR = \\dfrac{FP}{FP+TN}\\) may seem deceptively low as denominator may be made small by the sheer amount of TN, in this case, we may also look at the Precision-Recall curve. Scale Invariant Technically, this is not the desired property that we need, as this means that the ROC is non-proper in scoring, it can take in non-calibrated scores and still perform relatively well. A classic example I always use is the following: y1 = [ 1 , 0 , 1 , 0 ] y2 = [ 0.52 , 0.51 , 0.52 , 0.51 ] y3 = [ 52 , 51 , 52 , 51 ] uncalibrated_roc = roc ( y1 , y2 ) == roc ( y1 , y3 ) print ( f \" { uncalibrated_roc } \" ) -> 1.0 The example tells us two things, as long as the ranking of predictions is preserved, the final AUROC score is the same, regardless of scale. We also notice that even though the model gives very unconfident predictions, the AUROC score is 1, which can be misleadingly over-optimistic. With that, we introduce Brier Score. Common Pitfalls Careful when using ROC function! We also note that when passing arguments to scikit-learn's roc_auc_score function, we should be careful not to pass y_score=model.predict(X) inside as we have to understand that we are passing in non-thresholded probabilities into y_score . If you pass the predicted values (full of 0 and 1s), then you are thresholding on 0 and 1 only, which is incorrect by definition. Brier Score Definition: Brier Score computes the squared difference between the probability of a prediction and its actual outcome. Brier Score is a strictly proper scoring rule while ROC is not ; the lower the Brier Score, the better the predictions are calibrated. We can first compute the AUROC score of the model, and compute Brier Score to give us how well calibrated (confident) the predictions are. Well Calibrated A intuitive way of understanding well calibrated probabilities is as follows, extracted from cambridge's probability calibration : In very simple terms, these are probabilities which can be interpreted as a confidence interval. Furthermore, a classifier is said to produce well calibrated probabilities if for the instances (data points) receiving probability 0.5, 50% of those instances belongs to the positive class. In my own words, if a classifier is well calibrated, say in our context where we predict binary target, and pretend that out of our test set, 100 of the samples have a probability of around 0.1, then this means 10% of these 100 samples actually belong to the positive class. The generic steps are as follows to calculate a calibrated plot: Sort all the samples by the classifier's predicted probabilities, in either ascending or descending order. Bin your diagram into N bins, usually we take 10, which means on the X-axis, note this does not mean we have 0-0.1, 0.1-0.2, ..., 0.9-1 as the 10 bins. What step 2 means is let's say you have 100 predictions, if you bin by 10 bins, and since the predictions are sorted , we can easily divide the 100 predictions into 10 intervals: for illustration, assume the 100 predictions are as follows, where we sort by ascending order and the prediction 0.1 has 10 of them, 0.2 have 10 of them, so on and so forth. y_pred = [ 0.1 , 0.1 , ..... , 0.2 , 0.2 , ... , 0.9 , 0.9 , ... , 1 , 1 , .. .1 ] Since we can divide the above into 10 bins, bin 1 will have 10 samples of predictions 0.1, bin 2 will have 10 samples of predictions 0.2, etc. We then take the mean of the predictions of each bin , that is for the first bin, we calculate \\(\\dfrac{1}{10}\\sum_{i=1}^{10}0.1 = 0.1\\) , and second bin, \\(\\dfrac{1}{10}\\sum_{i=1}^{10}0.2 = 0.2\\) . Note that this may not be such a nice number in reality, I made this example for the ease of illustration! Now, we have our X-axis from step 4, that is, we turned 10 bins, into 10 numbers, 0.1, 0.2, 0.3, ..., 1, and then we need to find the corresponding points for each of the 10 numbers! This is easy, for 0.1, the corresponding y-axis is just the fraction of positives , which means, out of the 10 samples in the first bin, how many of these 10 samples were actually positive? We do this for all 10 bins (points), and plot a line graph as seen in scikit-learn. Now this should be apparent now that a well calibrated model should lie close to the \\(y = x\\) line. That is, if the mean predicted probability is 0.1, then the y-axis should also be 0.1, meaning to say that out of all the samples that were predicted as 0.1, we should really only have about 10% of them being positive. The same logic applies to the rest! Brier Score Loss Brier Score Loss is a handy metric to measure whether a classifier is well calibrated, as quoted from scikit-learn : Brier Score Loss may be used to assess how well a classifier is calibrated. However, this metric should be used with care because a lower Brier score does not always mean a better calibrated model. This is because the Brier score metric is a combination of calibration loss and refinement loss. Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve. As refinement loss can change independently from calibration loss, a lower Brier score does not necessarily mean a better calibrated model. Common Pitfalls Class Imbalance: The good ol' class imbalance issue almost always pop up anywhere and everywhere. Intuitively, if we have a super rare positive/negative class, then if the model is very confident in its predictions for the majority class, but not so confident on the rare class, the overall Brier Score Loss may not be sufficient in discriminating the classifier's inability in correctly classifying the minority class.","title":"Modelling (Metrics)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#define-metrics","text":"Disclaimer: For a more detailed understanding of different metrics, do navigate to my self-made notes on metrics here . Choosing a metric to measure the classifier's (hypothesis) performance is important, as choosing the wrong one can lead to disastrous interpretations. One prime example is using the accuracy metric for imbalanced datasets; consider 1 mil data points, dichotomized by \\(99\\%\\) benign and \\(1\\%\\) malignant samples, even a baseline model zeroR model which predicts the majority class no matter the process will give a \\(99\\%\\) accuracy, completely missing out any positive samples, which unfortunately, is what we may be more interested in. Say No to Accuracy: Consider an imbalanced set, where the training data set has 100 patients (data points), and the ground truth is 90 patients are of class = 0, which means that these patients do not have cancer, whereas the remaining 10 patients are in class 1, where they do have cancer. This is an example of class imbalance where the ratio of class 1 to class 0 is 1:9. Consider a baseline (almost trivial) classifier : def zeroR ( patient_data ): training ... return benign where we predict the patient's class as the most frequent class. Meaning, the most frequent class in this question is the class = 0, where patients do not have cancer, so we just assign this class to everyone in this set. By doing this, we will inevitably achieve a in-sample accuracy rate of \\(\\frac{90}{100} = 90\\%\\) . But unfortunately, this supposedly high accuracy value is completely useless, because this classifier did not label any of the cancer patients correctly. The consequence can be serious, assuming the test set has the same distribution as our training set, where if we have a test set of 1000 patients, there are 900 negative and 100 positive. Our model just literally predict every one of them as benign, yielding a \\(90\\%\\) out-of-sample accuracy. What did we conclude? Well, for one, our accuracy can be 90% high and looks good to the laymen, but it failed to predict the most important class of people - yes, misclassifying true cancer patients as healthy people is very bad! For the reasons mentioned above, we will use metric that can help us reduce False Negatives, and at the same time, outputs meaningful predictions. In order to achieve for both, we will use Receiver operating characteristic (ROC) as the primary metric for the model to maximize (which is our \\(\\mathcal{M}\\) , and Brier Score , a proper scoring rule to measure the performance of our probabilistic predictions. We will go into some details in the next two subsections to justify our choice.","title":"Define Metrics"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#proper-scoring-rule","text":"The math behind the idea of Proper Scoring Rule is non-trivial. Here, we try to understand why a proper scoring rule is desired in the context of binary classification. Strictly Proper Scoring Rule: Brier Score Loss, for example, tells us that the best possible score, 0 (lowest loss), is obtained if and only if, the probability prediction we get for a sample, is the true probability itself. In other words, if a selected sample is of class 1, our prediction for this must be 1, with 100% probability, in order to get a score loss of 0. Proper Scoring Rule: Read [here](https://stats.stackexchange.com/questions/339919/what-does-it-mean-that-auc-is-a-semi-proper-scoring-rule) for this. Semi Proper Scoring Rule: AUROC, as mentioned, does not help out in telling whether a prediction by a classifier is close to the true probability or not. In our example, we even see that we can obtain a full score of 1, even if the probabilities all lie within 0.51 and 0.52. Improper Scoring Rule: Accuracy is a prime example, the accuracy score does not, whatsoever, tells us about how close our predicted probabilities are, to the true probability distribution of our samples.","title":"Proper Scoring Rule"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#receiver-operating-characteristic-roc","text":"Definition: The basic (non-probablistic intepretation) of ROC is graph that plots the True Positive Rate on the y-axis and False Positive Rate on the x-axis parametrized by a threshold vector $\\vec{t}$. We then look at the area under the ROC curve (AUROC) to get an overall performance measure. The choice of ROC over other metrics such as Accuracy is detailed initially. We also established we want to reduce False Negative (FN), since misclassifying a positive patient as benign is way more costly than the other way round. One can choose to minimize Recall in order to reduce FN, but this is less than ideal during training because it is a thresholded metric, and does not provide at which threshold the recall is at minimum. This leads us to choose ROC for the following two main reasons:","title":"Receiver operating characteristic (ROC)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#threshold-invariant","text":"By definition, ROC computes the pair \\(TPR \\times FPR\\) over all thresholds \\(t\\) , consequently, the AUROC is threshold invariant, allowing us to look at the model's performance over all thresholds. We note that ROC may not be that reliable in the case of very imbalanced datasets where majority is in the negative class, as \\(FPR = \\dfrac{FP}{FP+TN}\\) may seem deceptively low as denominator may be made small by the sheer amount of TN, in this case, we may also look at the Precision-Recall curve.","title":"Threshold Invariant"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#scale-invariant","text":"Technically, this is not the desired property that we need, as this means that the ROC is non-proper in scoring, it can take in non-calibrated scores and still perform relatively well. A classic example I always use is the following: y1 = [ 1 , 0 , 1 , 0 ] y2 = [ 0.52 , 0.51 , 0.52 , 0.51 ] y3 = [ 52 , 51 , 52 , 51 ] uncalibrated_roc = roc ( y1 , y2 ) == roc ( y1 , y3 ) print ( f \" { uncalibrated_roc } \" ) -> 1.0 The example tells us two things, as long as the ranking of predictions is preserved, the final AUROC score is the same, regardless of scale. We also notice that even though the model gives very unconfident predictions, the AUROC score is 1, which can be misleadingly over-optimistic. With that, we introduce Brier Score.","title":"Scale Invariant"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#common-pitfalls","text":"Careful when using ROC function! We also note that when passing arguments to scikit-learn's roc_auc_score function, we should be careful not to pass y_score=model.predict(X) inside as we have to understand that we are passing in non-thresholded probabilities into y_score . If you pass the predicted values (full of 0 and 1s), then you are thresholding on 0 and 1 only, which is incorrect by definition.","title":"Common Pitfalls"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#brier-score","text":"Definition: Brier Score computes the squared difference between the probability of a prediction and its actual outcome. Brier Score is a strictly proper scoring rule while ROC is not ; the lower the Brier Score, the better the predictions are calibrated. We can first compute the AUROC score of the model, and compute Brier Score to give us how well calibrated (confident) the predictions are.","title":"Brier Score"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#well-calibrated","text":"A intuitive way of understanding well calibrated probabilities is as follows, extracted from cambridge's probability calibration : In very simple terms, these are probabilities which can be interpreted as a confidence interval. Furthermore, a classifier is said to produce well calibrated probabilities if for the instances (data points) receiving probability 0.5, 50% of those instances belongs to the positive class. In my own words, if a classifier is well calibrated, say in our context where we predict binary target, and pretend that out of our test set, 100 of the samples have a probability of around 0.1, then this means 10% of these 100 samples actually belong to the positive class. The generic steps are as follows to calculate a calibrated plot: Sort all the samples by the classifier's predicted probabilities, in either ascending or descending order. Bin your diagram into N bins, usually we take 10, which means on the X-axis, note this does not mean we have 0-0.1, 0.1-0.2, ..., 0.9-1 as the 10 bins. What step 2 means is let's say you have 100 predictions, if you bin by 10 bins, and since the predictions are sorted , we can easily divide the 100 predictions into 10 intervals: for illustration, assume the 100 predictions are as follows, where we sort by ascending order and the prediction 0.1 has 10 of them, 0.2 have 10 of them, so on and so forth. y_pred = [ 0.1 , 0.1 , ..... , 0.2 , 0.2 , ... , 0.9 , 0.9 , ... , 1 , 1 , .. .1 ] Since we can divide the above into 10 bins, bin 1 will have 10 samples of predictions 0.1, bin 2 will have 10 samples of predictions 0.2, etc. We then take the mean of the predictions of each bin , that is for the first bin, we calculate \\(\\dfrac{1}{10}\\sum_{i=1}^{10}0.1 = 0.1\\) , and second bin, \\(\\dfrac{1}{10}\\sum_{i=1}^{10}0.2 = 0.2\\) . Note that this may not be such a nice number in reality, I made this example for the ease of illustration! Now, we have our X-axis from step 4, that is, we turned 10 bins, into 10 numbers, 0.1, 0.2, 0.3, ..., 1, and then we need to find the corresponding points for each of the 10 numbers! This is easy, for 0.1, the corresponding y-axis is just the fraction of positives , which means, out of the 10 samples in the first bin, how many of these 10 samples were actually positive? We do this for all 10 bins (points), and plot a line graph as seen in scikit-learn. Now this should be apparent now that a well calibrated model should lie close to the \\(y = x\\) line. That is, if the mean predicted probability is 0.1, then the y-axis should also be 0.1, meaning to say that out of all the samples that were predicted as 0.1, we should really only have about 10% of them being positive. The same logic applies to the rest!","title":"Well Calibrated"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#brier-score-loss","text":"Brier Score Loss is a handy metric to measure whether a classifier is well calibrated, as quoted from scikit-learn : Brier Score Loss may be used to assess how well a classifier is calibrated. However, this metric should be used with care because a lower Brier score does not always mean a better calibrated model. This is because the Brier score metric is a combination of calibration loss and refinement loss. Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve. As refinement loss can change independently from calibration loss, a lower Brier score does not necessarily mean a better calibrated model.","title":"Brier Score Loss"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%204%20-%20Modelling%20%28Metric%20to%20Optimize%29/#common-pitfalls_1","text":"Class Imbalance: The good ol' class imbalance issue almost always pop up anywhere and everywhere. Intuitively, if we have a super rare positive/negative class, then if the model is very confident in its predictions for the majority class, but not so confident on the rare class, the overall Brier Score Loss may not be sufficient in discriminating the classifier's inability in correctly classifying the minority class.","title":"Common Pitfalls"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/","text":"Stage 5: Modelling (Cross-Validation) by Hongnan Gao Dependencies and Configuration import logging import random from dataclasses import dataclass , field from time import time from typing import Any , Callable , Dict , List , Optional , Union import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn import model_selection @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ]) cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger config = config () logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data ) Cross-Validation Strategy Generalization Ultimately, we are interested in the Generalization Error made by the model, that is, how well the model perform on unseen data that is not taken from our sample set \\(\\mathcal{D}\\) . In general, we use validation set for Model Selection and the test set for an estimate of generalization error on new data. - Refactored from Elements of Statistical Learning, Chapter 7.2 Step 1: Train-Test-Split Since this dataset is relatively small, we will not use the train-validation-test split and only split into train and test in a ratio of 9:1, whereby the split is stratified on our target, using stratify=y parameter in train_test_split() to ensure that our target has equal representation in both train and test. We note that this is a relatively small dataset and in practice, we need a large sample size to get a reliable/stable split, it is also recommended to retrain the whole dataset (without the \"unseen\" test set) after we have done the model selection process (eg. finding best hyperparameters). Step 2: Resampling Strategy Note that we will be performing StratifiedKFold as our resampling strategy. After our split in Step 1, we have a training set \\(X_{\\text{train}}\\) , we will then perform our resampling strategy on this \\(X_{\\text{train}}\\) . We will choose our choice of \\(K = 5\\) . The choice of \\(K\\) is somewhat arbitrary, and is derived empirically . Cross-Validation Workflow To recap, we have the following: Training Set ( \\(X_{\\text{train}}\\) ) : This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis \\(h \\in \\mathcal{H}\\) . Validation Set ( \\(X_{\\text{val}}\\) ) : This is split from our \\(X_{\\text{train}}\\) during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis \\(g \\in \\mathcal{H}\\) ). Test Set ( \\(X_{\\text{test}}\\) ) : This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model \\(g\\) , we will use \\(g\\) to predict on the test set to get an estimate of the generalization error (also called out-of-sample error). Courtesy of scikit-learn on a typical Cross-Validation workflow. # Make a copy of df and assign it to X X = df . copy () # Pop diagnosis, the target column from X and assign the target column data to y y = X . pop ( \"diagnosis\" ) # Assign predictors and target accordingly predictor_cols = X . columns . to_list () target_col = config . target_col # Split train - test X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , y , train_size = config . train_size , shuffle = True , stratify = y , random_state = config . seed , ) We confirm that we have stratified properly. We do observe that the distribution of targets in both y_train and y_test are similar. # Log and send a class proportion plot to wandb for both train and test set logger . info ( f \"Y Train Distribution is : { y_train . value_counts ( normalize = True ) . to_dict () } \" ) logger . info ( f \"Y Test Distribution is : { y_test . value_counts ( normalize = True ) . to_dict () } \" ) # wandb.sklearn.plot_class_proportions(y_train, y_test, labels=[0, 1]) 2021-11-13,09:53:22 - Y Train Distribution is : {0: 0.626953125, 1: 0.373046875} 2021-11-13,09:53:22 - Y Test Distribution is : {0: 0.631578947368421, 1: 0.3684210526315789} def make_folds ( df : pd . DataFrame , num_folds : int , cv_schema : str , seed : int , predictor_col : List , target_col : List , ) -> pd . DataFrame : \"\"\"Split the given dataframe into training folds. Args: df (pd.DataFrame): The dataframe to be split. num_folds (int): The number of folds to be created. cv_schema (str): The type of cross validation to be used. seed (int): The seed number to be used. Returns: df_folds (pd.DataFrame): The dataframe containing the folds. \"\"\" if cv_schema == \"KFold\" : df_folds = df . copy () kf = model_selection . KFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( kf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) elif cv_schema == \"StratifiedKFold\" : df_folds = df . copy () skf = model_selection . StratifiedKFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( skf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) print ( df_folds . groupby ([ \"fold\" , \"diagnosis\" ]) . size ()) return df_folds # Concat X_train and y_train to apply make_folds on it and return a new dataframe df_folds with # an additional column fold to indicate each sample's fold X_y_train = pd . concat ([ X_train , y_train ], axis = 1 ) . reset_index ( drop = True ) df_folds = make_folds ( X_y_train , num_folds = config . num_folds , cv_schema = config . cv_schema , seed = config . seed , predictor_col = predictor_cols , target_col = config . target_col , ) # TODO: write directly to GCP df_folds . to_csv ( \"df_folds.csv\" , index = False ) fold diagnosis 1 0 64 1 39 2 0 65 1 38 3 0 64 1 38 4 0 64 1 38 5 0 64 1 38 dtype: int64 Looks good! All our five folds are stratified!","title":"Modelling (Cross-Validation Schema)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/#dependencies-and-configuration","text":"import logging import random from dataclasses import dataclass , field from time import time from typing import Any , Callable , Dict , List , Optional , Union import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn import model_selection @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ]) cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger config = config () logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data )","title":"Dependencies and Configuration"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/#cross-validation-strategy","text":"Generalization Ultimately, we are interested in the Generalization Error made by the model, that is, how well the model perform on unseen data that is not taken from our sample set \\(\\mathcal{D}\\) . In general, we use validation set for Model Selection and the test set for an estimate of generalization error on new data. - Refactored from Elements of Statistical Learning, Chapter 7.2","title":"Cross-Validation Strategy"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/#step-1-train-test-split","text":"Since this dataset is relatively small, we will not use the train-validation-test split and only split into train and test in a ratio of 9:1, whereby the split is stratified on our target, using stratify=y parameter in train_test_split() to ensure that our target has equal representation in both train and test. We note that this is a relatively small dataset and in practice, we need a large sample size to get a reliable/stable split, it is also recommended to retrain the whole dataset (without the \"unseen\" test set) after we have done the model selection process (eg. finding best hyperparameters).","title":"Step 1: Train-Test-Split"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/#step-2-resampling-strategy","text":"Note that we will be performing StratifiedKFold as our resampling strategy. After our split in Step 1, we have a training set \\(X_{\\text{train}}\\) , we will then perform our resampling strategy on this \\(X_{\\text{train}}\\) . We will choose our choice of \\(K = 5\\) . The choice of \\(K\\) is somewhat arbitrary, and is derived empirically .","title":"Step 2: Resampling Strategy"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%205%20-%20Modelling%20%28Cross-Validation%20Methodology%29/#cross-validation-workflow","text":"To recap, we have the following: Training Set ( \\(X_{\\text{train}}\\) ) : This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis \\(h \\in \\mathcal{H}\\) . Validation Set ( \\(X_{\\text{val}}\\) ) : This is split from our \\(X_{\\text{train}}\\) during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis \\(g \\in \\mathcal{H}\\) ). Test Set ( \\(X_{\\text{test}}\\) ) : This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model \\(g\\) , we will use \\(g\\) to predict on the test set to get an estimate of the generalization error (also called out-of-sample error). Courtesy of scikit-learn on a typical Cross-Validation workflow. # Make a copy of df and assign it to X X = df . copy () # Pop diagnosis, the target column from X and assign the target column data to y y = X . pop ( \"diagnosis\" ) # Assign predictors and target accordingly predictor_cols = X . columns . to_list () target_col = config . target_col # Split train - test X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , y , train_size = config . train_size , shuffle = True , stratify = y , random_state = config . seed , ) We confirm that we have stratified properly. We do observe that the distribution of targets in both y_train and y_test are similar. # Log and send a class proportion plot to wandb for both train and test set logger . info ( f \"Y Train Distribution is : { y_train . value_counts ( normalize = True ) . to_dict () } \" ) logger . info ( f \"Y Test Distribution is : { y_test . value_counts ( normalize = True ) . to_dict () } \" ) # wandb.sklearn.plot_class_proportions(y_train, y_test, labels=[0, 1]) 2021-11-13,09:53:22 - Y Train Distribution is : {0: 0.626953125, 1: 0.373046875} 2021-11-13,09:53:22 - Y Test Distribution is : {0: 0.631578947368421, 1: 0.3684210526315789} def make_folds ( df : pd . DataFrame , num_folds : int , cv_schema : str , seed : int , predictor_col : List , target_col : List , ) -> pd . DataFrame : \"\"\"Split the given dataframe into training folds. Args: df (pd.DataFrame): The dataframe to be split. num_folds (int): The number of folds to be created. cv_schema (str): The type of cross validation to be used. seed (int): The seed number to be used. Returns: df_folds (pd.DataFrame): The dataframe containing the folds. \"\"\" if cv_schema == \"KFold\" : df_folds = df . copy () kf = model_selection . KFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( kf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) elif cv_schema == \"StratifiedKFold\" : df_folds = df . copy () skf = model_selection . StratifiedKFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( skf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) print ( df_folds . groupby ([ \"fold\" , \"diagnosis\" ]) . size ()) return df_folds # Concat X_train and y_train to apply make_folds on it and return a new dataframe df_folds with # an additional column fold to indicate each sample's fold X_y_train = pd . concat ([ X_train , y_train ], axis = 1 ) . reset_index ( drop = True ) df_folds = make_folds ( X_y_train , num_folds = config . num_folds , cv_schema = config . cv_schema , seed = config . seed , predictor_col = predictor_cols , target_col = config . target_col , ) # TODO: write directly to GCP df_folds . to_csv ( \"df_folds.csv\" , index = False ) fold diagnosis 1 0 64 1 39 2 0 65 1 38 3 0 64 1 38 4 0 64 1 38 5 0 64 1 38 dtype: int64 Looks good! All our five folds are stratified!","title":"Cross-Validation Workflow"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/","text":"Stage 6: Spot Checking Algorithms by Hongnan Gao Dependencies and Configuration # !pip install gcloud == 0.18.3 ! pip install - q mlxtend == 0.19.0 ! pip install - q statsmodels == 0.13.1 import copy import csv import logging import random from dataclasses import dataclass , field from functools import wraps from time import time from typing import Any , Callable , Dict , List , Optional , Union , Tuple import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from mlxtend.evaluate import bias_variance_decomp , paired_ttest_5x2cv from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 5.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.8 MB 1.3 MB/s \u001b[?25h @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" df_folds : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ]) cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed } # spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" # oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" # final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" # precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" # roc_plot = \"../data/images/roc_plot.png\" # feature_importance = \"../data/images/feature_importance.png\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # Utils functions that we need def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : # print(f\"Dropping {max_vif_col} with vif={max_vif}\") column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names config = config () logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df_folds = pd . read_csv ( config . df_folds ) # Assign predictors and target accordingly predictor_cols = df_folds . columns . to_list ()[: - 2 ] target_col = config . target_col Spot Checking Algorithms Terminology Alert! This method is advocated by Jason Brownlee PhD and this serves as the first stage of my modelling process. We will rapidly test (spot check) different classifier algorithms, from DummyClassifier , to LinearModel to more sophisticated ensemble trees like RandomForest . I also note to the readers that we need to think of a few things when choosing the \"optimal\" machine learning algorithm: No Lunch Free Theorem intuitively says that no single optimization algorithm can work best in all situations. Therefore, spot checking can help us form a basis of which algorithm might work better in this particular scenario. Occam's Razor often appears in many Machine Learning textbook, and the narrative is that a simpler model more often times generalizes better than a complex model. This is not unfamiliar when we think of the bias-variance tradeoff, and that is why there is always a tradeoff that we must make. Say No to Data Leakage! Say No to Data Leakage: This has been emphasized throughout and we must be careful as we should never touch the test set when fitting the model. In fact, we should try our best to not contaminate our validation set as well. This means that preprocessing steps such as StandardScaling() should only be fitted on the training data, and then apply the same transformation (mean and std) on the test data. In other words, do not apply scaling on the whole dataset before splitting. However, it is also equally important to take note not to contaminate our validation set, which is often overlooked, resulting in over optimistic results from model selection phase, but perform badly on unseen test set. As a result, when we use a 5 fold cross validation, we should be careful during fitting that the preprocessing steps are only applied on the training folds, and not on all 5 folds. The same idea is also applied to our ReduceVIF() preprocessing step. We should also include this in our pipeline and not select the features outside the cross-validation loop. Quoting from scikit-learn : Data leakage occurs when information that would not be available at prediction time is used when building the model. This results in overly optimistic performance estimates, for example from cross-validation , and thus poorer performance when the model is used on actually novel data, for example during production. A common cause is not keeping the test and train data subsets separate. Test data should never be used to make choices about the model. The general rule is to never call fit on the test data. While this may sound obvious, this is easy to miss in some cases, for example when applying certain pre-processing steps. Although both train and test data subsets should receive the same preprocessing transformation (as described in the previous section), it is important that these transformations are only learnt from the training data. For example, if you have a normalization step where you divide by the average value, the average should be the average of the train subset, not the average of all the data. If the test subset is included in the average calculation, information from the test subset is influencing the model. How to avoid Data Leakage? We know the pitfalls of fitting on validation/test data, the natural question is how can we avoid it completely? You can code it up yourself, but as a starter, we can use scikit-learn's Pipeline object. My tips are as follows: Any preprocessing step must be done after splitting the whole dataset into train and test. If you are also using cross-validation, then we should only apply the preprocessing steps on the train set, and then use the metrics obtained from the train set to transform the validation set. You can see my pseudo-code below for a rough outline. The Pipeline object of Scikit-Learn can help prevent data leakage. Pseudo-Code of Cross-Validation and Pipeline The below outlines a pseudo code of the cross-validation scheme using Pipeline object. Note that I included the most outer loop, which is searching for hyperparameters. Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\) Training Pipeline Make Pipeline # @ TODO: https://www.kaggle.com/kabure/predicting-house-prices-xgb-rf-bagging-reg-pipe # Different models can potentially have different pre-processing steps, consider putting steps as a # passable list. def make_pipeline ( model : Callable ) -> Callable : \"\"\"Create a feature preparation pipeline for a model. Args: model (Callable): The model to be used. Returns: _pipeline (Callable): pipeline object \"\"\" # Create a list of steps, note that some models may not need certain steps, and hence # may need an if-else here steps = list () # standardization steps . append (( \"standardize\" , preprocessing . StandardScaler ())) # reduce VIF steps . append (( \"remove_multicollinearity\" , ReduceVIF ( thresh = 10 ))) # the model to be appended at the last step steps . append (( \"model\" , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline classifiers = [ # baseline model dummy . DummyClassifier ( random_state = config . seed , strategy = \"stratified\" ), # linear model linear_model . LogisticRegression ( random_state = config . seed , solver = \"liblinear\" ), # nearest neighbours neighbors . KNeighborsClassifier ( n_neighbors = 8 ), # SVM svm . SVC ( probability = True , random_state = config . seed ), # tree tree . DecisionTreeClassifier ( random_state = config . seed ), # ensemble ensemble . RandomForestClassifier ( n_estimators = 10 , random_state = config . seed ), ] classifiers = [ make_pipeline ( model ) for model in classifiers ] Results Class The Results class will help us store model's results. Careful when using ROC function! We also note that when passing arguments to scikit-learn's roc_auc_score function, we should be careful not to pass y_score=model.predict(X) inside as we have to understand that we are passing in non-thresholded probabilities into y_score . If you pass the predicted values (full of 0 and 1s), then you are thresholding on 0 and 1 only, which is incorrect by definition. default_result_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" ] default_logit_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" ] default_score_names = [ \"accuracy_score\" , \"precision_recall_fscore_support\" , \"confusion_matrix\" , # \"average_precision_score\", \"multiclass_roc_auc_score\" , \"brier_score_loss\" , ] custom_score_names = [ \"multiclass_roc_auc_score\" , \"brier_score_loss\" ] use_preds = [ \"accuracy_score\" , \"precision_recall_fscore_support\" , \"confusion_matrix\" , ] use_probs = [ \"average_precision_score\" ] class Results : \"\"\"Stores results for model training in columnwise format.\"\"\" _result_dict : Dict logit_names : List [ str ] score_names : List [ str ] def __init__ ( self , logit_names : List [ str ] = default_logit_names , score_names : List [ str ] = default_score_names , existing_dict : Optional [ Dict ] = None , ): \"\"\"Construct a new results store.\"\"\" self . logit_names = logit_names self . score_names = score_names if existing_dict is not None : self . _result_dict = copy . deepcopy ( existing_dict ) return dict_keys = [ \"identifier\" , * logit_names , * score_names ] self . _result_dict = { key : [] for key in dict_keys } def add ( self , identifier : str , results : Dict , in_place = False ): \"\"\"Add a new results row.\"\"\" if not in_place : return Results ( self . logit_names , self . score_names , self . _result_dict ) . add ( identifier , results , in_place = True ) self . _result_dict [ \"identifier\" ] . append ( identifier ) for result_name in set ([ * results . keys (), * self . logit_names , * self . score_names ]): result_value = results . get ( result_name , np . nan ) self . _result_dict [ result_name ] . append ( result_value ) return self def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } def get_result_values ( self , result_name : str ) -> List [ Any ]: \"\"\"Get a list of values for a result.\"\"\" return self . _result_dict [ result_name ] def to_dataframe ( self ) -> pd . DataFrame : \"\"\"Get a Data Frame containing the results.\"\"\" return pd . DataFrame . from_dict ( self . _result_dict ) def to_dict ( self ) -> Dict : \"\"\"Get a dictionary containing the results. Returns: Dict[str, List[Any]]: Dictionary of result columns \"\"\" return self . _result_dict def multiclass_label_binarize ( y : np . ndarray , class_labels : List [ int ], pos_label = 1 , neg_label = 0 ): \"\"\"Binarize labels in one-vs-all fashion. # TODO: to replace with the above vstack method. Args: y (np.ndarray) Sequence of integer labels to encode class_labels (array-like) Labels for each class pos_label (int) Value for positive labels neg_label (int) Value for negative labels Returns: np.ndarray of shape (n_samples, n_classes) Encoded dataset \"\"\" if isinstance ( y , list ): y = np . asarray ( y ) columns = [ np . where ( y == label , pos_label , neg_label ) for label in class_labels ] return np . column_stack ( columns ) def multiclass_roc_auc_score ( y_true , y_score , classes = None ): \"\"\"Compute ROC-AUC score for each class in a multiclass dataset. Args: y_true (np.ndarray of shape (n_samples, n_classes)) True labels y_score (np.ndarray of shape (n_samples, n_classes)) Target scores classes (array-like of shape (n_classes,)) List of dataset classes. If `None`, the lexicographical order of the labels in `y_true` is used. Returns: array-like: ROC-AUC score for each class, in the same order as `classes` \"\"\" classes = ( np . unique ( y_true ) if classes is None else classes ) y_true_multiclass = multiclass_label_binarize ( y_true , class_labels = classes ) def oneclass_roc_auc_score ( class_id ): y_true_class = y_true_multiclass [:, class_id ] y_score_class = y_score [:, class_id ] fpr , tpr , _ = metrics . roc_curve ( y_true = y_true_class , y_score = y_score_class , pos_label = 1 ) return metrics . auc ( fpr , tpr ) return [ oneclass_roc_auc_score ( class_id ) for class_id in range ( len ( classes )) ] Utilities Some utility functions to prepare data and post-process data. def prepare_y ( y : np . ndarray ) -> np . ndarray : \"\"\"Prepare the target variable for the model. If Binary Classification, we need to ravel the array to 1d. Args: y (np.ndarray): Target variable. Returns: np.ndarray: Transformed Target variable. \"\"\" return y . ravel () if config . classification_type == \"binary\" else y def mean_score ( score_values ) -> Union [ float , np . ndarray ]: \"\"\"Compute the mean score.\"\"\" score_values = np . array ( score_values ) shape = score_values . shape if len ( shape ) == 1 : return score_values . mean () return score_values . mean ( axis = 0 ) def mean_cv_results ( model_results : Results ) -> Dict : \"\"\"Add mean cross-validation results. This method computes the mean value for all score types in the model_results, including for scores (e.g., confusion matrix) where the mean value may contain decimal places. \"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result )) for y_result in model_results . logit_names } cv_scores = { score : mean_score ( model_results . get_result_values ( score ) ) for score in model_results . score_names } return { ** cv_logits , ** cv_scores , } def oof_cv_results ( model_results : Results ) -> Dict : \"\"\"Add OOF cross-validation results.\"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result ) ) for y_result in model_results . logit_names } cv_scores = compute_metrics ( cv_logits ) return { ** cv_logits , ** cv_scores , } def add_cv_results ( model_results : Results ): \"\"\"Add cross-validation results. This method returns a copy of the given model results with summary columns for mean and CV cross-validation. \"\"\" mean_cv = mean_cv_results ( model_results ) oof_cv = oof_cv_results ( model_results ) return ( model_results . add ( \"mean_cv\" , mean_cv ) . add ( \"oof_cv\" , oof_cv ) ) def compute_metrics ( logits : Dict [ str , np . ndarray ]) -> Dict [ str , Any ]: \"\"\"Compute metrics from logits. use_probs: all metrics that use probabilities. use_preds: all metrics that use thresholded predictions. # TODO add this precision, recall, fbeta_score, _ = metrics.precision_recall_fscore_support( y_true=y_val, y_pred = y_val_pred, labels=np.unique(y_val), average=None ) \"\"\" y_true , y_pred , y_prob = ( logits [ \"y_true\" ], logits [ \"y_pred\" ], logits [ \"y_prob\" ], ) use_preds = [ \"accuracy_score\" , \"precision_recall_fscore_support\" , \"confusion_matrix\" , ] use_probs = [ \"average_precision_score\" ] default_metrics_dict : Dict [ str , float ] = {} custom_metrics_dict : Dict [ str , float ] = {} for metric_name in default_score_names : if hasattr ( metrics , metric_name ): # TODO: get metric score with default parameters, consider adding kwargs if you want to configure parameters if metric_name in use_preds : metric_score = getattr ( metrics , metric_name )( y_true , y_pred ) elif metric_name in use_probs : # logger.info(\"TODO: write custom scores for precision-recall as here is hardcoded\") pass # metric_score = getattr(metrics, metric_name)( # y_true, y_prob # ) else : # add custom metrics here multiclass_roc_auc = multiclass_roc_auc_score ( y_true , y_prob ) brier_score_loss = ( metrics . brier_score_loss ( y_true = y_true , y_prob = y_prob [:, 1 ]) if config . classification_type == \"binary\" else np . nan ) custom_metrics_dict [ \"multiclass_roc_auc_score\" ] = multiclass_roc_auc custom_metrics_dict [ \"brier_score_loss\" ] = brier_score_loss if metric_name not in default_metrics_dict : default_metrics_dict [ metric_name ] = metric_score metrics_dict = { ** default_metrics_dict , ** custom_metrics_dict } return metrics_dict def train_on_fold ( df_folds : pd . DataFrame , models : List [ Callable ], num_folds : int , predictor_col : List [ str ], target_col : List [ str ], ) -> Dict [ str , List ]: \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results. Args: df_folds (pd.DataFrame): Dataframe with fold number as column. model (Callable): A callable model. num_folds (int): Number of folds. predictor_col (List[str]): List of predictor columns. target_col (List[str]): List of target columns. Returns: model_dict (Dict[str, Results]: Dictionary of model results with model name as key. \"\"\" y_true = df_folds [ target_col ] . values . flatten () # test_pred_arr: np.ndarray = np.zeros(len(X_test)) model_dict = {} for model in models : model_results = Results () if isinstance ( model , pipeline . Pipeline ): model_name = model [ \"model\" ] . __class__ . __name__ else : model_name = model . __class__ . __name__ # out-of-fold validation predictions oof_pred_arr : np . ndarray = np . zeros ( len ( df_folds )) for fold in range ( 1 , num_folds + 1 ): train_df = df_folds [ df_folds [ \"fold\" ] != fold ] . reset_index ( drop = True ) val_df = df_folds [ df_folds [ \"fold\" ] == fold ] . reset_index ( drop = True ) val_idx = df_folds [ df_folds [ \"fold\" ] == fold ] . index . values X_train , y_train = train_df [ predictor_col ] . values , prepare_y ( train_df [ target_col ] . values ) X_val , y_val = val_df [ predictor_col ] . values , prepare_y ( val_df [ target_col ] . values ) model . fit ( X_train , y_train ) y_val_pred = model . predict ( X_val ) y_val_prob = model . predict_proba ( X_val ) logits = { \"y_true\" : y_val , \"y_pred\" : y_val_pred , \"y_prob\" : y_val_prob , } metrics = compute_metrics ( logits ) model_results . add ( f \"fold { fold } \" , { ** logits , ** metrics }, in_place = True ) if model_name not in model_dict : model_dict [ model_name ] = model_results return model_dict # Returns a dict in the format of # {'LogisticRegression': <__main__.Results at 0x7f3bca575e90>} model_dict = train_on_fold ( df_folds , models = classifiers , num_folds = 5 , predictor_col = predictor_cols , target_col = config . target_col ) # Takes in a model_dict and add cv results to the dict model_dict_with_summary = { model : add_cv_results ( model_results ) for model , model_results in model_dict . items () } # Transforms model_dict_with_summary to a Dict of dataframes # model_results_df['LogisticRegression'] -> df model_results_df = { name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () } results_df = pd . concat ( model_results_df , axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv RandomForestClassifier accuracy_score 0.932039 0.961165 0.95098 0.960784 0.95098 0.95119 0.951172 precision_recall_fscore_support ([0.9672131147540983, 0.8809523809523809], [0.... ([0.9552238805970149, 0.9722222222222222], [0.... ([0.927536231884058, 1.0], [1.0, 0.86842105263... ([0.9545454545454546, 0.9722222222222222], [0.... ([0.9402985074626866, 0.9714285714285714], [0.... [[0.9489634378486625, 0.9593650793650793], [0.... ([0.9484848484848485, 0.9560439560439561], [0.... confusion_matrix [[59, 5], [2, 37]] [[64, 1], [3, 35]] [[64, 0], [5, 33]] [[63, 1], [3, 35]] [[63, 1], [4, 34]] [[62.6, 1.6], [3.4, 34.8]] [[313, 8], [17, 174]] multiclass_roc_auc_score [0.9709535256410255, 0.9709535256410255] [0.9896761133603239, 0.9896761133603239] [0.9967105263157895, 0.9967105263157895] [0.9930098684210527, 0.9930098684210527] [0.9802631578947368, 0.980263157894737] [0.9861226383265856, 0.9861226383265856] [0.9849374500497463, 0.9849374500497464] brier_score_loss 0.0594175 0.0415534 0.0376471 0.0347059 0.0413725 0.0429393 0.0429688 Comparison of Cross-Validated Models (CV + OOF) The point of the following comparison is to check how different models are performing across folds. More specifically, if we have 5 folds, we will have a metric score for each fold, subsequently, we can find the standard error of model's performance. We need to be aware of models that have high variance across folds in terms of the metrics performance. This can indicate that the model is highly unstable, and may be a sign of overfitting. def summarize_metrics ( model_dict : Dict [ str , Results ], metric_name : str = \"roc\" , pos_label : int = 1 ): \"\"\" Summarize metrics of each fold with its standard error. We also plot a boxplot to show the results. \"\"\" results = [] for model_name , model_results in model_dict . items (): result_dict = model_results . get_result ( result_name = metric_name ) tmp_score = [] for fold , metric in result_dict . items (): pos_class_score = metric [ pos_label ] results . append (( model_name , fold , pos_class_score )) tmp_score . append ( pos_class_score ) # append the Standard Error of K folds results . append ( ( model_name , \"SE\" , np . std ( tmp_score , ddof = 1 ) / len ( tmp_score ) ** 0.5 ) ) summary_df = pd . DataFrame ( results , columns = [ \"model\" , \"fold\" , metric_name ]) fig , ax = plt . subplots ( figsize = ( 15 , 8 )) _ = sns . boxplot ( x = \"model\" , y = metric_name , data = summary_df [ ( summary_df [ \"model\" ] != \"DummyClassifier\" ) & ( summary_df [ \"fold\" ] != \"SE\" ) ], ax = ax , ) # fig.savefig(config.spot_checking_boxplot, format='png', dpi=300) return summary_df summary_df = summarize_metrics ( model_dict = model_dict , metric_name = \"multiclass_roc_auc_score\" ) display ( summary_df . tail ( 12 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model fold multiclass_roc_auc_score 24 DecisionTreeClassifier fold 1 0.865585 25 DecisionTreeClassifier fold 2 0.924291 26 DecisionTreeClassifier fold 3 0.923931 27 DecisionTreeClassifier fold 4 0.923931 28 DecisionTreeClassifier fold 5 0.895148 29 DecisionTreeClassifier SE 0.011677 30 RandomForestClassifier fold 1 0.970954 31 RandomForestClassifier fold 2 0.989676 32 RandomForestClassifier fold 3 0.996711 33 RandomForestClassifier fold 4 0.993010 34 RandomForestClassifier fold 5 0.980263 35 RandomForestClassifier SE 0.004672 Out-of-Fold Confusion Matrix We do have information on the performance of each folds, we now look at the performance of all 5 folds together. Typicall there are two ways to do it, one is to simply take the average of the score of five folds, the other is to take a look at out of folds predictions. From the confusion matrix of the out of fold performance, Logistic Regression does seem to be a model we can explore on, although slightly lower in terms of overall AUROC score than SVC, it seems to have the quite low False Negatives amongst all. With further hyperparameter tuning and threshold optimization, we can make it better. model_names = [ model for model in model_dict . keys ()] def plot_binary_confusion_matrix ( results_df : pd . DataFrame , model_names : List [ str ] ) -> None : n_models = len ( model_names ) # if 7 models, then 3 rows, 2 columns, and 7 subplots # always fix column to be 3 n_cols = 3 n_rows = int ( np . ceil ( n_models / n_cols )) fig , ax = plt . subplots ( n_rows , n_cols , figsize = ( 10 , 10 )) for axes , algo in zip ( ax . ravel (), model_names ): # Unravel into tn, fp, fn and tp tn , fp , fn , tp = results_df . oof_cv [ algo ] . confusion_matrix . ravel () # reshape into tp, fp, fn, tn - this is personal preference reshaped_cm = np . asarray ([[ tp , fp ], [ fn , tn ]]) # Get positive ROC score - hardcoded here. positive_class_auroc = results_df . oof_cv [ algo ] . multiclass_roc_auc_score [ 1 ] # annotations labels = [ \"True Pos\" , \"False Pos\" , \"False Neg\" , \"True Neg\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in reshaped_cm . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in reshaped_cm . flatten () / np . sum ( reshaped_cm ) ] # final annotations label = ( np . array ( [ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )] ) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = reshaped_cm , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = axes , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) axes . text ( 0 , - 0 , \" {} \" . format ( algo ), { \"size\" : 12 , \"color\" : \"black\" , \"weight\" : \"bold\" } ) axes . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) axes . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( np . round ( positive_class_auroc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels axes . set_xticklabels ( \"\" ) axes . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Out Of Fold Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"This Visualization show the results of various classifiers and there respective results.\"\"\" , { \"size\" : 14 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) # fig.savefig(config.oof_confusion_matrix, format='png', dpi=300) plot_binary_confusion_matrix ( results_df , model_names ) Hypothesis Testing Across Models I am slightly shocked at the performance of plain LogisticRegression, I decide to use an idea from Hypothesis Testing Across Models to check if the difference is really by chance or not. Note that I will be modifying his code as his code does not split using StratifiedKFold. The basic idea is to test if two model's difference in scores (in this case roc), is statistically significant or not. However, we note that this method may violate an assumption of Student's t test. Null Hypothesis \\(H_0\\) : The difference in the performance score of two classifiers is Statistically Significant. Alternate Hypothesis \\(H_1\\) : The difference in the performance score of two classifiers is not Statistically Significant. def paired_ttest_skfold_cv ( estimator1 : Callable , estimator2 : Callable , X : np . ndarray , y : np . ndarray , cv : int = 10 , scoring : str = None , shuffle : bool = False , random_seed : int = None , ) -> float : \"\"\"Modified from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/ttest.py to accomodate StratifiedKFold. Args: estimator1 (Callable): [description] estimator2 (Callable): [description] X (np.ndarray): [description] y (np.ndarray): [description] cv (int, optional): [description]. Defaults to 10. scoring (str, optional): [description]. Defaults to None. shuffle (bool, optional): [description]. Defaults to False. random_seed (int, optional): [description]. Defaults to None. Raises: AttributeError: [description] Returns: float: [description] \"\"\" if not shuffle : skf = model_selection . StratifiedKFold ( n_splits = cv , shuffle = shuffle ) else : skf = model_selection . StratifiedKFold ( n_splits = cv , random_state = random_seed , shuffle = shuffle ) if scoring is None : if estimator1 . _estimator_type == \"classifier\" : scoring = \"accuracy\" elif estimator1 . _estimator_type == \"regressor\" : scoring = \"r2\" else : raise AttributeError ( \"Estimator must \" \"be a Classifier or Regressor.\" ) if isinstance ( scoring , str ): scorer = metrics . get_scorer ( scoring ) else : scorer = scoring score_diff = [] for train_index , test_index in skf . split ( X = X , y = y ): X_train , X_test = X [ train_index ], X [ test_index ] y_train , y_test = y [ train_index ], y [ test_index ] estimator1 . fit ( X_train , y_train ) estimator2 . fit ( X_train , y_train ) est1_score = scorer ( estimator1 , X_test , y_test ) est2_score = scorer ( estimator2 , X_test , y_test ) score_diff . append ( est1_score - est2_score ) avg_diff = np . mean ( score_diff ) numerator = avg_diff * np . sqrt ( cv ) denominator = np . sqrt ( sum ([( diff - avg_diff ) ** 2 for diff in score_diff ]) / ( cv - 1 ) ) t_stat = numerator / denominator pvalue = stats . t . sf ( np . abs ( t_stat ), cv - 1 ) * 2.0 return float ( t_stat ), float ( pvalue ) # check if difference between algorithms is real X_tmp = df_folds [ predictor_cols ] . values y_tmp = df_folds [ 'diagnosis' ] . values t , p = paired_ttest_skfold_cv ( estimator1 = classifiers [ 1 ], estimator2 = classifiers [ - 1 ], shuffle = True , cv = 5 , X = X_tmp , y = y_tmp , scoring = 'roc_auc' , random_seed = config . seed ) logger . info ( 'P-value: %.3f , t-Statistic: %.3f ' % ( p , t )) 2021-11-13,14:04:35 - P-value: 0.171, t-Statistic: 1.667 2021-11-13,14:04:35 - P-value: 0.171, t-Statistic: 1.667 Since \\(p\\) -value is quite high, and more the basic threshold of 0.05 or 0.1, we fail to reject the null hypothesis, and say that there is no significant difference between these two models.","title":"Modelling (Preprocessing and Spot Checking)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#dependencies-and-configuration","text":"# !pip install gcloud == 0.18.3 ! pip install - q mlxtend == 0.19.0 ! pip install - q statsmodels == 0.13.1 import copy import csv import logging import random from dataclasses import dataclass , field from functools import wraps from time import time from typing import Any , Callable , Dict , List , Optional , Union , Tuple import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from mlxtend.evaluate import bias_variance_decomp , paired_ttest_5x2cv from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 5.2 MB/s \u001b[K |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.8 MB 1.3 MB/s \u001b[?25h @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" df_folds : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ]) cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) def to_dict ( self ) -> Dict : \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return { \"raw_data\" : self . raw_data , \"processed_data\" : self . processed_data , \"train_size\" : self . train_size , \"seed\" : self . seed , \"num_folds\" : self . num_folds , \"cv_schema\" : self . cv_schema , \"classification_type\" : self . classification_type , \"target_col\" : self . target_col , \"unwanted_cols\" : self . unwanted_cols , \"colors\" : self . colors , \"cmap_reversed\" : self . cmap_reversed } # spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" # oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" # final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" # precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" # roc_plot = \"../data/images/roc_plot.png\" # feature_importance = \"../data/images/feature_importance.png\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # Utils functions that we need def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : # print(f\"Dropping {max_vif_col} with vif={max_vif}\") column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names config = config () logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df_folds = pd . read_csv ( config . df_folds ) # Assign predictors and target accordingly predictor_cols = df_folds . columns . to_list ()[: - 2 ] target_col = config . target_col","title":"Dependencies and Configuration"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#spot-checking-algorithms","text":"Terminology Alert! This method is advocated by Jason Brownlee PhD and this serves as the first stage of my modelling process. We will rapidly test (spot check) different classifier algorithms, from DummyClassifier , to LinearModel to more sophisticated ensemble trees like RandomForest . I also note to the readers that we need to think of a few things when choosing the \"optimal\" machine learning algorithm: No Lunch Free Theorem intuitively says that no single optimization algorithm can work best in all situations. Therefore, spot checking can help us form a basis of which algorithm might work better in this particular scenario. Occam's Razor often appears in many Machine Learning textbook, and the narrative is that a simpler model more often times generalizes better than a complex model. This is not unfamiliar when we think of the bias-variance tradeoff, and that is why there is always a tradeoff that we must make.","title":"Spot Checking Algorithms"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#say-no-to-data-leakage","text":"Say No to Data Leakage: This has been emphasized throughout and we must be careful as we should never touch the test set when fitting the model. In fact, we should try our best to not contaminate our validation set as well. This means that preprocessing steps such as StandardScaling() should only be fitted on the training data, and then apply the same transformation (mean and std) on the test data. In other words, do not apply scaling on the whole dataset before splitting. However, it is also equally important to take note not to contaminate our validation set, which is often overlooked, resulting in over optimistic results from model selection phase, but perform badly on unseen test set. As a result, when we use a 5 fold cross validation, we should be careful during fitting that the preprocessing steps are only applied on the training folds, and not on all 5 folds. The same idea is also applied to our ReduceVIF() preprocessing step. We should also include this in our pipeline and not select the features outside the cross-validation loop. Quoting from scikit-learn : Data leakage occurs when information that would not be available at prediction time is used when building the model. This results in overly optimistic performance estimates, for example from cross-validation , and thus poorer performance when the model is used on actually novel data, for example during production. A common cause is not keeping the test and train data subsets separate. Test data should never be used to make choices about the model. The general rule is to never call fit on the test data. While this may sound obvious, this is easy to miss in some cases, for example when applying certain pre-processing steps. Although both train and test data subsets should receive the same preprocessing transformation (as described in the previous section), it is important that these transformations are only learnt from the training data. For example, if you have a normalization step where you divide by the average value, the average should be the average of the train subset, not the average of all the data. If the test subset is included in the average calculation, information from the test subset is influencing the model. How to avoid Data Leakage? We know the pitfalls of fitting on validation/test data, the natural question is how can we avoid it completely? You can code it up yourself, but as a starter, we can use scikit-learn's Pipeline object. My tips are as follows: Any preprocessing step must be done after splitting the whole dataset into train and test. If you are also using cross-validation, then we should only apply the preprocessing steps on the train set, and then use the metrics obtained from the train set to transform the validation set. You can see my pseudo-code below for a rough outline. The Pipeline object of Scikit-Learn can help prevent data leakage.","title":"Say No to Data Leakage!"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#pseudo-code-of-cross-validation-and-pipeline","text":"The below outlines a pseudo code of the cross-validation scheme using Pipeline object. Note that I included the most outer loop, which is searching for hyperparameters. Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\)","title":"Pseudo-Code of Cross-Validation and Pipeline"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#training-pipeline","text":"","title":"Training Pipeline"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#make-pipeline","text":"# @ TODO: https://www.kaggle.com/kabure/predicting-house-prices-xgb-rf-bagging-reg-pipe # Different models can potentially have different pre-processing steps, consider putting steps as a # passable list. def make_pipeline ( model : Callable ) -> Callable : \"\"\"Create a feature preparation pipeline for a model. Args: model (Callable): The model to be used. Returns: _pipeline (Callable): pipeline object \"\"\" # Create a list of steps, note that some models may not need certain steps, and hence # may need an if-else here steps = list () # standardization steps . append (( \"standardize\" , preprocessing . StandardScaler ())) # reduce VIF steps . append (( \"remove_multicollinearity\" , ReduceVIF ( thresh = 10 ))) # the model to be appended at the last step steps . append (( \"model\" , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline classifiers = [ # baseline model dummy . DummyClassifier ( random_state = config . seed , strategy = \"stratified\" ), # linear model linear_model . LogisticRegression ( random_state = config . seed , solver = \"liblinear\" ), # nearest neighbours neighbors . KNeighborsClassifier ( n_neighbors = 8 ), # SVM svm . SVC ( probability = True , random_state = config . seed ), # tree tree . DecisionTreeClassifier ( random_state = config . seed ), # ensemble ensemble . RandomForestClassifier ( n_estimators = 10 , random_state = config . seed ), ] classifiers = [ make_pipeline ( model ) for model in classifiers ]","title":"Make Pipeline"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#results-class","text":"The Results class will help us store model's results. Careful when using ROC function! We also note that when passing arguments to scikit-learn's roc_auc_score function, we should be careful not to pass y_score=model.predict(X) inside as we have to understand that we are passing in non-thresholded probabilities into y_score . If you pass the predicted values (full of 0 and 1s), then you are thresholding on 0 and 1 only, which is incorrect by definition. default_result_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" ] default_logit_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" ] default_score_names = [ \"accuracy_score\" , \"precision_recall_fscore_support\" , \"confusion_matrix\" , # \"average_precision_score\", \"multiclass_roc_auc_score\" , \"brier_score_loss\" , ] custom_score_names = [ \"multiclass_roc_auc_score\" , \"brier_score_loss\" ] use_preds = [ \"accuracy_score\" , \"precision_recall_fscore_support\" , \"confusion_matrix\" , ] use_probs = [ \"average_precision_score\" ] class Results : \"\"\"Stores results for model training in columnwise format.\"\"\" _result_dict : Dict logit_names : List [ str ] score_names : List [ str ] def __init__ ( self , logit_names : List [ str ] = default_logit_names , score_names : List [ str ] = default_score_names , existing_dict : Optional [ Dict ] = None , ): \"\"\"Construct a new results store.\"\"\" self . logit_names = logit_names self . score_names = score_names if existing_dict is not None : self . _result_dict = copy . deepcopy ( existing_dict ) return dict_keys = [ \"identifier\" , * logit_names , * score_names ] self . _result_dict = { key : [] for key in dict_keys } def add ( self , identifier : str , results : Dict , in_place = False ): \"\"\"Add a new results row.\"\"\" if not in_place : return Results ( self . logit_names , self . score_names , self . _result_dict ) . add ( identifier , results , in_place = True ) self . _result_dict [ \"identifier\" ] . append ( identifier ) for result_name in set ([ * results . keys (), * self . logit_names , * self . score_names ]): result_value = results . get ( result_name , np . nan ) self . _result_dict [ result_name ] . append ( result_value ) return self def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } def get_result_values ( self , result_name : str ) -> List [ Any ]: \"\"\"Get a list of values for a result.\"\"\" return self . _result_dict [ result_name ] def to_dataframe ( self ) -> pd . DataFrame : \"\"\"Get a Data Frame containing the results.\"\"\" return pd . DataFrame . from_dict ( self . _result_dict ) def to_dict ( self ) -> Dict : \"\"\"Get a dictionary containing the results. Returns: Dict[str, List[Any]]: Dictionary of result columns \"\"\" return self . _result_dict def multiclass_label_binarize ( y : np . ndarray , class_labels : List [ int ], pos_label = 1 , neg_label = 0 ): \"\"\"Binarize labels in one-vs-all fashion. # TODO: to replace with the above vstack method. Args: y (np.ndarray) Sequence of integer labels to encode class_labels (array-like) Labels for each class pos_label (int) Value for positive labels neg_label (int) Value for negative labels Returns: np.ndarray of shape (n_samples, n_classes) Encoded dataset \"\"\" if isinstance ( y , list ): y = np . asarray ( y ) columns = [ np . where ( y == label , pos_label , neg_label ) for label in class_labels ] return np . column_stack ( columns ) def multiclass_roc_auc_score ( y_true , y_score , classes = None ): \"\"\"Compute ROC-AUC score for each class in a multiclass dataset. Args: y_true (np.ndarray of shape (n_samples, n_classes)) True labels y_score (np.ndarray of shape (n_samples, n_classes)) Target scores classes (array-like of shape (n_classes,)) List of dataset classes. If `None`, the lexicographical order of the labels in `y_true` is used. Returns: array-like: ROC-AUC score for each class, in the same order as `classes` \"\"\" classes = ( np . unique ( y_true ) if classes is None else classes ) y_true_multiclass = multiclass_label_binarize ( y_true , class_labels = classes ) def oneclass_roc_auc_score ( class_id ): y_true_class = y_true_multiclass [:, class_id ] y_score_class = y_score [:, class_id ] fpr , tpr , _ = metrics . roc_curve ( y_true = y_true_class , y_score = y_score_class , pos_label = 1 ) return metrics . auc ( fpr , tpr ) return [ oneclass_roc_auc_score ( class_id ) for class_id in range ( len ( classes )) ]","title":"Results Class"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#utilities","text":"Some utility functions to prepare data and post-process data. def prepare_y ( y : np . ndarray ) -> np . ndarray : \"\"\"Prepare the target variable for the model. If Binary Classification, we need to ravel the array to 1d. Args: y (np.ndarray): Target variable. Returns: np.ndarray: Transformed Target variable. \"\"\" return y . ravel () if config . classification_type == \"binary\" else y def mean_score ( score_values ) -> Union [ float , np . ndarray ]: \"\"\"Compute the mean score.\"\"\" score_values = np . array ( score_values ) shape = score_values . shape if len ( shape ) == 1 : return score_values . mean () return score_values . mean ( axis = 0 ) def mean_cv_results ( model_results : Results ) -> Dict : \"\"\"Add mean cross-validation results. This method computes the mean value for all score types in the model_results, including for scores (e.g., confusion matrix) where the mean value may contain decimal places. \"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result )) for y_result in model_results . logit_names } cv_scores = { score : mean_score ( model_results . get_result_values ( score ) ) for score in model_results . score_names } return { ** cv_logits , ** cv_scores , } def oof_cv_results ( model_results : Results ) -> Dict : \"\"\"Add OOF cross-validation results.\"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result ) ) for y_result in model_results . logit_names } cv_scores = compute_metrics ( cv_logits ) return { ** cv_logits , ** cv_scores , } def add_cv_results ( model_results : Results ): \"\"\"Add cross-validation results. This method returns a copy of the given model results with summary columns for mean and CV cross-validation. \"\"\" mean_cv = mean_cv_results ( model_results ) oof_cv = oof_cv_results ( model_results ) return ( model_results . add ( \"mean_cv\" , mean_cv ) . add ( \"oof_cv\" , oof_cv ) ) def compute_metrics ( logits : Dict [ str , np . ndarray ]) -> Dict [ str , Any ]: \"\"\"Compute metrics from logits. use_probs: all metrics that use probabilities. use_preds: all metrics that use thresholded predictions. # TODO add this precision, recall, fbeta_score, _ = metrics.precision_recall_fscore_support( y_true=y_val, y_pred = y_val_pred, labels=np.unique(y_val), average=None ) \"\"\" y_true , y_pred , y_prob = ( logits [ \"y_true\" ], logits [ \"y_pred\" ], logits [ \"y_prob\" ], ) use_preds = [ \"accuracy_score\" , \"precision_recall_fscore_support\" , \"confusion_matrix\" , ] use_probs = [ \"average_precision_score\" ] default_metrics_dict : Dict [ str , float ] = {} custom_metrics_dict : Dict [ str , float ] = {} for metric_name in default_score_names : if hasattr ( metrics , metric_name ): # TODO: get metric score with default parameters, consider adding kwargs if you want to configure parameters if metric_name in use_preds : metric_score = getattr ( metrics , metric_name )( y_true , y_pred ) elif metric_name in use_probs : # logger.info(\"TODO: write custom scores for precision-recall as here is hardcoded\") pass # metric_score = getattr(metrics, metric_name)( # y_true, y_prob # ) else : # add custom metrics here multiclass_roc_auc = multiclass_roc_auc_score ( y_true , y_prob ) brier_score_loss = ( metrics . brier_score_loss ( y_true = y_true , y_prob = y_prob [:, 1 ]) if config . classification_type == \"binary\" else np . nan ) custom_metrics_dict [ \"multiclass_roc_auc_score\" ] = multiclass_roc_auc custom_metrics_dict [ \"brier_score_loss\" ] = brier_score_loss if metric_name not in default_metrics_dict : default_metrics_dict [ metric_name ] = metric_score metrics_dict = { ** default_metrics_dict , ** custom_metrics_dict } return metrics_dict def train_on_fold ( df_folds : pd . DataFrame , models : List [ Callable ], num_folds : int , predictor_col : List [ str ], target_col : List [ str ], ) -> Dict [ str , List ]: \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results. Args: df_folds (pd.DataFrame): Dataframe with fold number as column. model (Callable): A callable model. num_folds (int): Number of folds. predictor_col (List[str]): List of predictor columns. target_col (List[str]): List of target columns. Returns: model_dict (Dict[str, Results]: Dictionary of model results with model name as key. \"\"\" y_true = df_folds [ target_col ] . values . flatten () # test_pred_arr: np.ndarray = np.zeros(len(X_test)) model_dict = {} for model in models : model_results = Results () if isinstance ( model , pipeline . Pipeline ): model_name = model [ \"model\" ] . __class__ . __name__ else : model_name = model . __class__ . __name__ # out-of-fold validation predictions oof_pred_arr : np . ndarray = np . zeros ( len ( df_folds )) for fold in range ( 1 , num_folds + 1 ): train_df = df_folds [ df_folds [ \"fold\" ] != fold ] . reset_index ( drop = True ) val_df = df_folds [ df_folds [ \"fold\" ] == fold ] . reset_index ( drop = True ) val_idx = df_folds [ df_folds [ \"fold\" ] == fold ] . index . values X_train , y_train = train_df [ predictor_col ] . values , prepare_y ( train_df [ target_col ] . values ) X_val , y_val = val_df [ predictor_col ] . values , prepare_y ( val_df [ target_col ] . values ) model . fit ( X_train , y_train ) y_val_pred = model . predict ( X_val ) y_val_prob = model . predict_proba ( X_val ) logits = { \"y_true\" : y_val , \"y_pred\" : y_val_pred , \"y_prob\" : y_val_prob , } metrics = compute_metrics ( logits ) model_results . add ( f \"fold { fold } \" , { ** logits , ** metrics }, in_place = True ) if model_name not in model_dict : model_dict [ model_name ] = model_results return model_dict # Returns a dict in the format of # {'LogisticRegression': <__main__.Results at 0x7f3bca575e90>} model_dict = train_on_fold ( df_folds , models = classifiers , num_folds = 5 , predictor_col = predictor_cols , target_col = config . target_col ) # Takes in a model_dict and add cv results to the dict model_dict_with_summary = { model : add_cv_results ( model_results ) for model , model_results in model_dict . items () } # Transforms model_dict_with_summary to a Dict of dataframes # model_results_df['LogisticRegression'] -> df model_results_df = { name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () } results_df = pd . concat ( model_results_df , axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv RandomForestClassifier accuracy_score 0.932039 0.961165 0.95098 0.960784 0.95098 0.95119 0.951172 precision_recall_fscore_support ([0.9672131147540983, 0.8809523809523809], [0.... ([0.9552238805970149, 0.9722222222222222], [0.... ([0.927536231884058, 1.0], [1.0, 0.86842105263... ([0.9545454545454546, 0.9722222222222222], [0.... ([0.9402985074626866, 0.9714285714285714], [0.... [[0.9489634378486625, 0.9593650793650793], [0.... ([0.9484848484848485, 0.9560439560439561], [0.... confusion_matrix [[59, 5], [2, 37]] [[64, 1], [3, 35]] [[64, 0], [5, 33]] [[63, 1], [3, 35]] [[63, 1], [4, 34]] [[62.6, 1.6], [3.4, 34.8]] [[313, 8], [17, 174]] multiclass_roc_auc_score [0.9709535256410255, 0.9709535256410255] [0.9896761133603239, 0.9896761133603239] [0.9967105263157895, 0.9967105263157895] [0.9930098684210527, 0.9930098684210527] [0.9802631578947368, 0.980263157894737] [0.9861226383265856, 0.9861226383265856] [0.9849374500497463, 0.9849374500497464] brier_score_loss 0.0594175 0.0415534 0.0376471 0.0347059 0.0413725 0.0429393 0.0429688","title":"Utilities"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#comparison-of-cross-validated-models-cv-oof","text":"The point of the following comparison is to check how different models are performing across folds. More specifically, if we have 5 folds, we will have a metric score for each fold, subsequently, we can find the standard error of model's performance. We need to be aware of models that have high variance across folds in terms of the metrics performance. This can indicate that the model is highly unstable, and may be a sign of overfitting. def summarize_metrics ( model_dict : Dict [ str , Results ], metric_name : str = \"roc\" , pos_label : int = 1 ): \"\"\" Summarize metrics of each fold with its standard error. We also plot a boxplot to show the results. \"\"\" results = [] for model_name , model_results in model_dict . items (): result_dict = model_results . get_result ( result_name = metric_name ) tmp_score = [] for fold , metric in result_dict . items (): pos_class_score = metric [ pos_label ] results . append (( model_name , fold , pos_class_score )) tmp_score . append ( pos_class_score ) # append the Standard Error of K folds results . append ( ( model_name , \"SE\" , np . std ( tmp_score , ddof = 1 ) / len ( tmp_score ) ** 0.5 ) ) summary_df = pd . DataFrame ( results , columns = [ \"model\" , \"fold\" , metric_name ]) fig , ax = plt . subplots ( figsize = ( 15 , 8 )) _ = sns . boxplot ( x = \"model\" , y = metric_name , data = summary_df [ ( summary_df [ \"model\" ] != \"DummyClassifier\" ) & ( summary_df [ \"fold\" ] != \"SE\" ) ], ax = ax , ) # fig.savefig(config.spot_checking_boxplot, format='png', dpi=300) return summary_df summary_df = summarize_metrics ( model_dict = model_dict , metric_name = \"multiclass_roc_auc_score\" ) display ( summary_df . tail ( 12 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model fold multiclass_roc_auc_score 24 DecisionTreeClassifier fold 1 0.865585 25 DecisionTreeClassifier fold 2 0.924291 26 DecisionTreeClassifier fold 3 0.923931 27 DecisionTreeClassifier fold 4 0.923931 28 DecisionTreeClassifier fold 5 0.895148 29 DecisionTreeClassifier SE 0.011677 30 RandomForestClassifier fold 1 0.970954 31 RandomForestClassifier fold 2 0.989676 32 RandomForestClassifier fold 3 0.996711 33 RandomForestClassifier fold 4 0.993010 34 RandomForestClassifier fold 5 0.980263 35 RandomForestClassifier SE 0.004672","title":"Comparison of Cross-Validated Models (CV + OOF)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#out-of-fold-confusion-matrix","text":"We do have information on the performance of each folds, we now look at the performance of all 5 folds together. Typicall there are two ways to do it, one is to simply take the average of the score of five folds, the other is to take a look at out of folds predictions. From the confusion matrix of the out of fold performance, Logistic Regression does seem to be a model we can explore on, although slightly lower in terms of overall AUROC score than SVC, it seems to have the quite low False Negatives amongst all. With further hyperparameter tuning and threshold optimization, we can make it better. model_names = [ model for model in model_dict . keys ()] def plot_binary_confusion_matrix ( results_df : pd . DataFrame , model_names : List [ str ] ) -> None : n_models = len ( model_names ) # if 7 models, then 3 rows, 2 columns, and 7 subplots # always fix column to be 3 n_cols = 3 n_rows = int ( np . ceil ( n_models / n_cols )) fig , ax = plt . subplots ( n_rows , n_cols , figsize = ( 10 , 10 )) for axes , algo in zip ( ax . ravel (), model_names ): # Unravel into tn, fp, fn and tp tn , fp , fn , tp = results_df . oof_cv [ algo ] . confusion_matrix . ravel () # reshape into tp, fp, fn, tn - this is personal preference reshaped_cm = np . asarray ([[ tp , fp ], [ fn , tn ]]) # Get positive ROC score - hardcoded here. positive_class_auroc = results_df . oof_cv [ algo ] . multiclass_roc_auc_score [ 1 ] # annotations labels = [ \"True Pos\" , \"False Pos\" , \"False Neg\" , \"True Neg\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in reshaped_cm . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in reshaped_cm . flatten () / np . sum ( reshaped_cm ) ] # final annotations label = ( np . array ( [ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )] ) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = reshaped_cm , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = axes , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) axes . text ( 0 , - 0 , \" {} \" . format ( algo ), { \"size\" : 12 , \"color\" : \"black\" , \"weight\" : \"bold\" } ) axes . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) axes . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( np . round ( positive_class_auroc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels axes . set_xticklabels ( \"\" ) axes . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Out Of Fold Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"This Visualization show the results of various classifiers and there respective results.\"\"\" , { \"size\" : 14 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) # fig.savefig(config.oof_confusion_matrix, format='png', dpi=300) plot_binary_confusion_matrix ( results_df , model_names )","title":"Out-of-Fold Confusion Matrix"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%206%20-%20Modelling%20%28Preprocessing%20and%20Spot%20Checking%29/#hypothesis-testing-across-models","text":"I am slightly shocked at the performance of plain LogisticRegression, I decide to use an idea from Hypothesis Testing Across Models to check if the difference is really by chance or not. Note that I will be modifying his code as his code does not split using StratifiedKFold. The basic idea is to test if two model's difference in scores (in this case roc), is statistically significant or not. However, we note that this method may violate an assumption of Student's t test. Null Hypothesis \\(H_0\\) : The difference in the performance score of two classifiers is Statistically Significant. Alternate Hypothesis \\(H_1\\) : The difference in the performance score of two classifiers is not Statistically Significant. def paired_ttest_skfold_cv ( estimator1 : Callable , estimator2 : Callable , X : np . ndarray , y : np . ndarray , cv : int = 10 , scoring : str = None , shuffle : bool = False , random_seed : int = None , ) -> float : \"\"\"Modified from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/ttest.py to accomodate StratifiedKFold. Args: estimator1 (Callable): [description] estimator2 (Callable): [description] X (np.ndarray): [description] y (np.ndarray): [description] cv (int, optional): [description]. Defaults to 10. scoring (str, optional): [description]. Defaults to None. shuffle (bool, optional): [description]. Defaults to False. random_seed (int, optional): [description]. Defaults to None. Raises: AttributeError: [description] Returns: float: [description] \"\"\" if not shuffle : skf = model_selection . StratifiedKFold ( n_splits = cv , shuffle = shuffle ) else : skf = model_selection . StratifiedKFold ( n_splits = cv , random_state = random_seed , shuffle = shuffle ) if scoring is None : if estimator1 . _estimator_type == \"classifier\" : scoring = \"accuracy\" elif estimator1 . _estimator_type == \"regressor\" : scoring = \"r2\" else : raise AttributeError ( \"Estimator must \" \"be a Classifier or Regressor.\" ) if isinstance ( scoring , str ): scorer = metrics . get_scorer ( scoring ) else : scorer = scoring score_diff = [] for train_index , test_index in skf . split ( X = X , y = y ): X_train , X_test = X [ train_index ], X [ test_index ] y_train , y_test = y [ train_index ], y [ test_index ] estimator1 . fit ( X_train , y_train ) estimator2 . fit ( X_train , y_train ) est1_score = scorer ( estimator1 , X_test , y_test ) est2_score = scorer ( estimator2 , X_test , y_test ) score_diff . append ( est1_score - est2_score ) avg_diff = np . mean ( score_diff ) numerator = avg_diff * np . sqrt ( cv ) denominator = np . sqrt ( sum ([( diff - avg_diff ) ** 2 for diff in score_diff ]) / ( cv - 1 ) ) t_stat = numerator / denominator pvalue = stats . t . sf ( np . abs ( t_stat ), cv - 1 ) * 2.0 return float ( t_stat ), float ( pvalue ) # check if difference between algorithms is real X_tmp = df_folds [ predictor_cols ] . values y_tmp = df_folds [ 'diagnosis' ] . values t , p = paired_ttest_skfold_cv ( estimator1 = classifiers [ 1 ], estimator2 = classifiers [ - 1 ], shuffle = True , cv = 5 , X = X_tmp , y = y_tmp , scoring = 'roc_auc' , random_seed = config . seed ) logger . info ( 'P-value: %.3f , t-Statistic: %.3f ' % ( p , t )) 2021-11-13,14:04:35 - P-value: 0.171, t-Statistic: 1.667 2021-11-13,14:04:35 - P-value: 0.171, t-Statistic: 1.667 Since \\(p\\) -value is quite high, and more the basic threshold of 0.05 or 0.1, we fail to reject the null hypothesis, and say that there is no significant difference between these two models.","title":"Hypothesis Testing Across Models"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/","text":"Stage 7: Hyperparameter Tuning by Hongnan Gao Dependencies and Configuration %% capture ! pip install - q wandb # !pip install -q shap ! pip install - q mlxtend == 0.19.0 ! pip install - q statsmodels == 0.13.1 # !pip install gcloud == 0.18.3 import wandb wandb . login () \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect. True import copy import csv import logging import os import random from dataclasses import asdict , dataclass , field from functools import wraps from pathlib import Path from time import time from typing import Any , Callable , Dict , List , Optional , Tuple , Union import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from joblib import dump , load from mlxtend.evaluate import bias_variance_decomp , paired_ttest_5x2cv from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS Utils and Configurations @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" df_folds : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ]) cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return asdict ( self ) # spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" # oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" # final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" # precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" # roc_plot = \"../data/images/roc_plot.png\" # feature_importance = \"../data/images/feature_importance.png\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # Utils functions that we need def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : # print(f\"Dropping {max_vif_col} with vif={max_vif}\") column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names def prepare_y ( y : np . ndarray ) -> np . ndarray : \"\"\"Prepare the target variable for the model. If Binary Classification, we need to ravel the array to 1d. Args: y (np.ndarray): Target variable. Returns: np.ndarray: Transformed Target variable. \"\"\" return y . ravel () if config . classification_type == \"binary\" else y config = config () basic_config : Dict = config . to_dict () # train_config: Dict = Train().to_dict() global_config : Dict = dict ( basic = basic_config ) # We can log multiple dict under global_config - in wandb UI, it will show as basic. and train. to show which dict it is referring to. run = wandb . init ( project = \"bcw\" , name = \"classification\" , config = global_config ) # set logger logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df_folds = pd . read_csv ( config . df_folds ) # Assign predictors and target accordingly predictor_cols = df_folds . columns . to_list ()[: - 2 ] target_col = config . target_col Model Selection: Hyperparameter Tuning with GridSearchCV Hyperparameter Tuning We have done a quick spot checking on algorithms and realized that LogisticRegression is doing well for this task. For this purpose, I will just perform hyperparameter tuning on this single algorithm. However, in practice and if resources are allowed, I will also tune other models such as RandomForest() , or gradient boosting algorithms such as XGBoost , as I believe they will perform no worse than our Logistic Regression model given the right hyperparameters. Grid Search is the Gwei? Meh! We will use an old-fashioned way to search for hyperparameters, which is brute force method. The time complexity of Grid Search is high and if you have many hyperparameters to tune, I recommend trying out Random Grid Search or libraries like Optuna that uses Bayesian Optimization. TODO Try to code up your own GridSearchCV to have maximum flexibility. Make Finetuning Pipeline The following make_finetuning_pipeline does exactly the same thing is as make_pipeline earlier. The only difference is we can pass in flexible list of steps to the pipeline from outside. def make_finetuning_pipeline ( model : Callable , steps : List [ Tuple [ str , Callable ]] ) -> pipeline . Pipeline : \"\"\"Return a pipeline that can be used for finetuning. Args: model (Callable): A model with default parameters. steps (List[Tuple[str, Callable]]): A list of preprocessing steps to pass in Pipeline object. Returns: Pipeline: Returns a pipeline that can be used for finetuning. \"\"\" return pipeline . Pipeline ([ * steps , ( \"model\" , model )]) # TODO: Make a class to hold pipelines? # class MakePipeline: # def __init__(self, estimator: Callable, steps: List[Callable]): # pass # def spot_checking_pipeline(): # pass # def fine_tuning_pipeline(): # pass finetuning_pipeline_steps = [ # standardization ( \"standardize\" , preprocessing . StandardScaler ()), # reduce VIF ( \"remove_multicollinearity\" , ReduceVIF ( thresh = 10 )) ] Search Space Run our hyperparameter search with cross-validation. For example, our param_grid has \\(2 \\times 10 = 20\\) combinations, and our cross validation has 5 folds, then there will be a total of 100 fits. Below details the pseudo code of what happens under the hood: Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\) @dataclass class ModelForTuning : model : Callable param_grid : Dict Define our search space for the hyperparameters: logistic_r_param_grid = { model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 )} We conveniently use dataclass to act as a medium so we can pass in model and param_grid independently for each model. We then collate them into a list of ModelForTuning object. models_list = [ ModelForTuning ( model = linear_model . LogisticRegression ( solver = \"saga\" , random_state = config . seed , max_iter = 10000 , n_jobs =- 1 , fit_intercept = True , ), param_grid = dict ( model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 ), ), ), ModelForTuning ( model = tree . DecisionTreeClassifier ( random_state = config . seed ), param_grid = dict ( model__max_depth = [ 2 , 3 , 5 , 10 , 20 ], model__min_samples_leaf = [ 5 , 10 , 20 , 50 , 100 ], model__criterion = [ \"gini\" , \"entropy\" ], ), ), ModelForTuning ( model = ensemble . GradientBoostingClassifier ( n_estimators = 100 ), param_grid = dict ( model__max_depth = [ 3 , 6 ], model__learning_rate = [ 0.1 , 0.05 ], model__subsample = [ 1 , 0.5 , ], ), ), ] def optimize_models ( models_list : List [ ModelForTuning ], X_train : np . ndarray , y_train : np . ndarray , scorer : Union [ str , Callable ], steps : List [ Tuple [ str , Callable ]], ) -> List [ Callable ]: \"\"\"Optimize models in models_list using X_train and y_train. We are using GridSearchCV to find the best parameters for each model. Consider using Optuna for hyperparameter optimization (or wandb for hyperparameter optimization). Args: models_list (List[ModelForTuning]): List of models to optimize. X_train (np.ndarray): X_train data. y_train (np.ndarray): y_train data. Returns: grids (List[Callable]): List of optimized models. \"\"\" # @ TODO: make a scoring list to pass in so we can evaluate multiple metrics. grids = [ model_selection . GridSearchCV ( make_finetuning_pipeline ( model . model , steps ), param_grid = model . param_grid , cv = 5 , refit = True , verbose = 1 , scoring = scorer , n_jobs =- 1 , ) for model in models_list ] for grid in grids : grid . fit ( X_train , y_train ) return grids roc_auc_scorer = \"roc_auc_ovr\" # Unsure why this gives much lower score - to investigate # metrics.make_scorer(metrics.roc_auc_score, average=\"macro\", multi_class='ovr') X_train , y_train = df_folds [ predictor_cols ] . values , df_folds [ target_col ] . values y_train = prepare_y ( y_train ) grids = optimize_models ( models_list , X_train , y_train , scorer = roc_auc_scorer , steps = finetuning_pipeline_steps ) Fitting 5 folds for each of 20 candidates, totalling 100 fits Fitting 5 folds for each of 50 candidates, totalling 250 fits Fitting 5 folds for each of 8 candidates, totalling 40 fits # The above optimize code is equivalent to the below, for better readability # pipeline_logistic = make_finetuning_pipeline( # linear_model.LogisticRegression( # solver=\"saga\", random_state=config.seed, max_iter=10000, n_jobs=None, fit_intercept=True # ), steps=steps # ) # param_grid = dict( # model__penalty=[\"l1\", \"l2\"], # model__C=np.logspace(-4, 4, 10), # ) # grid = model_selection.GridSearchCV(pipeline_logistic, param_grid=param_grid, cv=5, refit=True, verbose=3, scoring = \"roc_auc\") # _ = grid.fit(X_train, y_train) We can save our results in a dataframe, we will also look at the top performing hyperparameter by querying the below: grid_cv_df = pd . DataFrame ( grid . cv_results_ ) grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ] # For example, we can see Logistic Regression's GridSearchCV # results like this. grid_cv_df = pd . DataFrame ( grids [ 0 ] . cv_results_ ) display ( grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_fit_time std_fit_time mean_score_time std_score_time param_model__C param_model__penalty params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score 8 0.931891 0.058794 0.002457 0.000387 0.359381 l1 {'model__C': 0.3593813663804626, 'model__penal... 0.997997 0.995547 0.997944 0.990132 0.995477 0.995419 0.002863 1 def return_grid_df ( grids : List [ model_selection . GridSearchCV ], ) -> Union [ pd . DataFrame , List [ model_selection . GridSearchCV ]]: \"\"\"Return a dataframe of the grids with shorted names. Args: grids (List[model_selection.GridSearchCV]): A list of GridSearchCV models that are tuned. Returns: grid_df, grids (Union[pd.DataFrame, List[model_selection.GridSearchCV]]): A dataframe of the grids with shorted names. \"\"\" def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name grid_df = [] for grid in grids : model_name = grid . estimator [ \"model\" ] . __class__ . __name__ cv_results = pd . DataFrame ( grid . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in grid . param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" , ] cv_results = cv_results [ column_results ] cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results [ \"model_name\" ] = model_name grid_df . append ( cv_results ) return grid_df , grids # grid_df and grids should necessarily be in the same sequence. # grid_df[0] == grids[0] in terms of model information, in this # case, the first index of both should be logistic regression. grid_df , grids = return_grid_df ( grids ) for model_df , grid in zip ( grid_df , grids ): best_hyperparams_df = model_df . iloc [[ 0 ]] model_name = best_hyperparams_df . model_name . unique ()[ 0 ] logger . info ( f \"Best hyperparameters found for { model_name } is as follows: \\n { grid . best_params_ } \" ) display ( best_hyperparams_df ) print () 2021-11-16,09:19:36 - Best hyperparameters found for LogisticRegression is as follows: {'model__C': 0.3593813663804626, 'model__penalty': 'l1'} 2021-11-16,09:19:36 - Best hyperparameters found for LogisticRegression is as follows: {'model__C': 0.3593813663804626, 'model__penalty': 'l1'} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } penalty C mean_test_score std_test_score rank_test_score model_name 8 l1 0.359381 0.995419 0.002863 1 LogisticRegression 2021-11-16,09:19:36 - Best hyperparameters found for DecisionTreeClassifier is as follows: {'model__criterion': 'entropy', 'model__max_depth': 10, 'model__min_samples_leaf': 10} 2021-11-16,09:19:36 - Best hyperparameters found for DecisionTreeClassifier is as follows: {'model__criterion': 'entropy', 'model__max_depth': 10, 'model__min_samples_leaf': 10} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } max_depth min_samples_leaf criterion mean_test_score std_test_score rank_test_score model_name 41 10 10 entropy 0.954515 0.015913 1 DecisionTreeClassifier 2021-11-16,09:19:37 - Best hyperparameters found for GradientBoostingClassifier is as follows: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__subsample': 0.5} 2021-11-16,09:19:37 - Best hyperparameters found for GradientBoostingClassifier is as follows: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__subsample': 0.5} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } max_depth learning_rate subsample mean_test_score std_test_score rank_test_score model_name 1 3 0.1 0.5 0.991031 0.005869 1 GradientBoostingClassifier Success Our best performing set of hyperparameters for Logistic Regression {'model__C': 0.3593813663804626, 'model__penalty': 'l1'} gives rise to a mean cross validation score of \\(0.995419\\) , which is higher than the model with default hyperparameter scoring, \\(0.995\\) by a small margin. Not too surprising for Logistic Regression here since there aren't many things to tune, and should not see major improvements, but for Decesion Tree, it has increased from 0.907 to around 0.95, seeing quite a big jump with tuned params. DANGERRRRRRRRRRRRR I am being a bit hand wavy in terms of comparison here, I assumed THAT GridSearchCV used the exact same splitting strategy (yes it uses StratifiedKFold here) with the exact SEED/RANDOM_STATE , which I cannot promise as of now. Thus, a different splitting will, unfortunately, result in different results, although, I don't expect by a huge margin - so I think it is a no-go to compare like this. We can probably pass in a cv function into GridSearchCV to ensure seeding. This also highlights a problem that even K-fold splitting does not guarantee the reduction in variance. Room for Improvement Apart from the other methods to search for the optimal hyperparameters, we can also include preprocessing step as a tunable hyperparameter. More specifically, in our ReduceVIF() step, we hard coded two manual criterion in which the algorithm will stop; if the threshold reaches 10, or if the number of features removed hit 20; we can include them in the search space so we do not need to worry about how many features to remove! Model Persistence (Saving Models) Model Persistence We save our models using joblib and we can load it back any time. Note Save it to wandb or GCP storage to store models for better consistency. model_path = \"/content/\" def save_model ( grids : List [ Callable ], path : str ): \"\"\"Save a model to a file\"\"\" for grid in grids : model_name = grid . best_estimator_ [ \"model\" ] . __class__ . __name__ path_to_save = Path ( path , f \" { model_name } _grid.joblib\" ) # Dump to local path dump ( grid , Path ( path , path_to_save )) # Dump to wandb cloud # \"model.h5\" is saved in wandb.run.dir & will be uploaded at the end of training wandb . save ( os . path . join ( wandb . run . dir , path_to_save )) Save the model! Wandb We first see how we save and load using wandb. save_model ( grids , model_path ) \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\") \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\") \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\") logistic_path = \"LogisticRegression_grid.joblib\" # restore the model file \"model.h5\" from a specific run by user \"lavanyashukla\" # in project \"save_and_restore\" from run \"10pr4joa\" best_model = wandb . restore ( logistic_path ) # use the \"name\" attribute of the returned object # if your framework expects a filename, e.g. as in Keras # model.load_weights(best_model.name) Joblib We see how we use joblib to save and load. Load the model, and we can test it now if our loaded models is predicting correctly! logistic_grid = load ( \"/content/LogisticRegression_grid.joblib\" ) Great it seems to work! Sanity Check Note We just make sure our loaded weight from path is the same as the one we trained. We can easily compare predictions (or coefficients) by the following. load ( best_model . name ) . predict ( X_train ) . all () == logistic_grid . predict ( X_train ) . all () == grids [ 0 ] . predict ( X_train ) . all () True metrics . roc_auc_score ( y_train , logistic_grid . predict_proba ( X_train )[:, 1 ] ) == metrics . roc_auc_score ( y_train , grids [ 0 ] . predict_proba ( X_train )[:, 1 ] ) == metrics . roc_auc_score ( y_train , load ( best_model . name ) . predict_proba ( X_train )[:, 1 ] ) True Seems like the save and load method works perfectly. Warning Do not call this directly. grids [ 0 ] . best_estimator_ [ \"model\" ] . predict ( X_train ) This is because grids[0].best_estimator_[\"model\"] is only referring to the Logistic Regression Model WITHOUT the pipeline (preprocessing) steps. And hence will raise error if the preprocessing steps has feature selection. But the main idea is, be careful when using the above. # grids[0].best_estimator_[\"model\"].predict(X_train) Retrain using Hyperparameters Retraining Methods From the discussion 1 , my doubts are cleared. Quoting verbatim from the discussion, we have: K-folds cross validation was devised as a way to assess model performance using training data. A great paper on this from Sebastian Raschka is a must read https://arxiv.org/abs/1811.12808. You use K-folds cv to tune you model, then retrain on all training data with best hyperparamters found. However, once you have run K-fold cv, you get \\(K\\) trained models. Kagglers quickly found that ensembling these models was giving good results at zero computation cost, rather than having to retrain a model on full data. It soon became a very common practice. Takeway For small-medium datasets, after finding the best hyperparameters \\(G\\) , we use \\(G\\) in our model \\(h\\) to train on the whole dataset \\(\\mathcal{X}\\) again to get the fitted parameters of \\(h\\) . Then you use the newly gained fitted parameters to then evaluate on the Test Set . For large and computationally expensive datasets, when you finished your K-folds, say 5 folds, you get 5 \"different\" models, \\(h_{i}, i \\in {1, 2, 3, 4, 5}\\) , what you can do is to save the weights (or in normal ML, weights refer to the parameters gained), and evaluate on the test set for each of the five models, you then get 5 different test predictions, and a common practice is the do a simple mean of these 5 set of predictions. Retrain on K-Folds TODO: This should be easy for me as I dabbled more in Kaggle comp and are more familiar with this methodology. Retrain on the whole training set A common practice after the hyperparameter tuning phase is to retrain the model on the whole dataset \\(X_{\\text{train}}\\) where we will get the estimator's coefficients obtained from the retraining. This is actually already done as the scikit-learn's GridSearchCV has a parameter refit ; if we select it to be true, then after the model selection process is done (i.e. getting the best hyperparameters after cross validation with grid search), the grid search object will retrain on the whole \\(X_{\\text{train}}\\) with the best hyperparameters internally, and return us back an object in which we can call predict etc. Paranoia Alert However, to be extra careful, we can retrain manually using the best hyperparameters and check if scikit-learn is true to its documentation. We will just reconstruct the pipeline using the grid's best hyper parameters. We will then test if the retrained model's coefficients coincide with the grid's best estimator's coefficients. If there difference is 0, this means they are trained under the same circumstances and we can be sure that the refit parameter is behaving true to its words. grid_best_hyperparams = grid . best_params_ print ( grid_best_hyperparams ) -> { 'model__C' : 0.3593813663804626 , 'model__penalty' : 'l1' } retrain_logistic_pipeline = pipeline . Pipeline ( [ ( \"standardize\" , preprocessing . StandardScaler ()), ( \"remove_multicollinearity\" , ReduceVIF ( thresh = 10 )), ( \"model\" , linear_model . LogisticRegression ( C = 0.3593813663804626 , max_iter = 10000 , random_state = 1992 , solver = \"saga\" , penalty = \"l1\" , ), ), ] ) _ = retrain_logistic_pipeline . fit ( X_train , y_train ) logistic_grid = grids [ 0 ] coef_diff = ( retrain_logistic_pipeline [ \"model\" ] . coef_ - logistic_grid . best_estimator_ [ \"model\" ] . coef_ ) print ( \"...\" ) assert np . all ( coef_diff == 0 ) == True logger . info ( \"Retraining Assertion Passed!\" ) 2021-11-16,09:19:38 - Retraining Assertion Passed! 2021-11-16,09:19:38 - Retraining Assertion Passed! ... https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/275883 \u21a9","title":"Modelling (Model Selection)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#dependencies-and-configuration","text":"%% capture ! pip install - q wandb # !pip install -q shap ! pip install - q mlxtend == 0.19.0 ! pip install - q statsmodels == 0.13.1 # !pip install gcloud == 0.18.3 import wandb wandb . login () \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect. True import copy import csv import logging import os import random from dataclasses import asdict , dataclass , field from functools import wraps from pathlib import Path from time import time from typing import Any , Callable , Dict , List , Optional , Tuple , Union import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from joblib import dump , load from mlxtend.evaluate import bias_variance_decomp , paired_ttest_5x2cv from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS","title":"Dependencies and Configuration"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#utils-and-configurations","text":"@dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" df_folds : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ]) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ]) cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return asdict ( self ) # spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" # oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" # final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" # precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" # roc_plot = \"../data/images/roc_plot.png\" # feature_importance = \"../data/images/feature_importance.png\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # Utils functions that we need def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : # print(f\"Dropping {max_vif_col} with vif={max_vif}\") column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names def prepare_y ( y : np . ndarray ) -> np . ndarray : \"\"\"Prepare the target variable for the model. If Binary Classification, we need to ravel the array to 1d. Args: y (np.ndarray): Target variable. Returns: np.ndarray: Transformed Target variable. \"\"\" return y . ravel () if config . classification_type == \"binary\" else y config = config () basic_config : Dict = config . to_dict () # train_config: Dict = Train().to_dict() global_config : Dict = dict ( basic = basic_config ) # We can log multiple dict under global_config - in wandb UI, it will show as basic. and train. to show which dict it is referring to. run = wandb . init ( project = \"bcw\" , name = \"classification\" , config = global_config ) # set logger logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df_folds = pd . read_csv ( config . df_folds ) # Assign predictors and target accordingly predictor_cols = df_folds . columns . to_list ()[: - 2 ] target_col = config . target_col","title":"Utils and Configurations"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#model-selection-hyperparameter-tuning-with-gridsearchcv","text":"Hyperparameter Tuning We have done a quick spot checking on algorithms and realized that LogisticRegression is doing well for this task. For this purpose, I will just perform hyperparameter tuning on this single algorithm. However, in practice and if resources are allowed, I will also tune other models such as RandomForest() , or gradient boosting algorithms such as XGBoost , as I believe they will perform no worse than our Logistic Regression model given the right hyperparameters. Grid Search is the Gwei? Meh! We will use an old-fashioned way to search for hyperparameters, which is brute force method. The time complexity of Grid Search is high and if you have many hyperparameters to tune, I recommend trying out Random Grid Search or libraries like Optuna that uses Bayesian Optimization. TODO Try to code up your own GridSearchCV to have maximum flexibility.","title":"Model Selection: Hyperparameter Tuning with GridSearchCV"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#make-finetuning-pipeline","text":"The following make_finetuning_pipeline does exactly the same thing is as make_pipeline earlier. The only difference is we can pass in flexible list of steps to the pipeline from outside. def make_finetuning_pipeline ( model : Callable , steps : List [ Tuple [ str , Callable ]] ) -> pipeline . Pipeline : \"\"\"Return a pipeline that can be used for finetuning. Args: model (Callable): A model with default parameters. steps (List[Tuple[str, Callable]]): A list of preprocessing steps to pass in Pipeline object. Returns: Pipeline: Returns a pipeline that can be used for finetuning. \"\"\" return pipeline . Pipeline ([ * steps , ( \"model\" , model )]) # TODO: Make a class to hold pipelines? # class MakePipeline: # def __init__(self, estimator: Callable, steps: List[Callable]): # pass # def spot_checking_pipeline(): # pass # def fine_tuning_pipeline(): # pass finetuning_pipeline_steps = [ # standardization ( \"standardize\" , preprocessing . StandardScaler ()), # reduce VIF ( \"remove_multicollinearity\" , ReduceVIF ( thresh = 10 )) ]","title":"Make Finetuning Pipeline"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#search-space","text":"Run our hyperparameter search with cross-validation. For example, our param_grid has \\(2 \\times 10 = 20\\) combinations, and our cross validation has 5 folds, then there will be a total of 100 fits. Below details the pseudo code of what happens under the hood: Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\) @dataclass class ModelForTuning : model : Callable param_grid : Dict Define our search space for the hyperparameters: logistic_r_param_grid = { model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 )} We conveniently use dataclass to act as a medium so we can pass in model and param_grid independently for each model. We then collate them into a list of ModelForTuning object. models_list = [ ModelForTuning ( model = linear_model . LogisticRegression ( solver = \"saga\" , random_state = config . seed , max_iter = 10000 , n_jobs =- 1 , fit_intercept = True , ), param_grid = dict ( model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 ), ), ), ModelForTuning ( model = tree . DecisionTreeClassifier ( random_state = config . seed ), param_grid = dict ( model__max_depth = [ 2 , 3 , 5 , 10 , 20 ], model__min_samples_leaf = [ 5 , 10 , 20 , 50 , 100 ], model__criterion = [ \"gini\" , \"entropy\" ], ), ), ModelForTuning ( model = ensemble . GradientBoostingClassifier ( n_estimators = 100 ), param_grid = dict ( model__max_depth = [ 3 , 6 ], model__learning_rate = [ 0.1 , 0.05 ], model__subsample = [ 1 , 0.5 , ], ), ), ] def optimize_models ( models_list : List [ ModelForTuning ], X_train : np . ndarray , y_train : np . ndarray , scorer : Union [ str , Callable ], steps : List [ Tuple [ str , Callable ]], ) -> List [ Callable ]: \"\"\"Optimize models in models_list using X_train and y_train. We are using GridSearchCV to find the best parameters for each model. Consider using Optuna for hyperparameter optimization (or wandb for hyperparameter optimization). Args: models_list (List[ModelForTuning]): List of models to optimize. X_train (np.ndarray): X_train data. y_train (np.ndarray): y_train data. Returns: grids (List[Callable]): List of optimized models. \"\"\" # @ TODO: make a scoring list to pass in so we can evaluate multiple metrics. grids = [ model_selection . GridSearchCV ( make_finetuning_pipeline ( model . model , steps ), param_grid = model . param_grid , cv = 5 , refit = True , verbose = 1 , scoring = scorer , n_jobs =- 1 , ) for model in models_list ] for grid in grids : grid . fit ( X_train , y_train ) return grids roc_auc_scorer = \"roc_auc_ovr\" # Unsure why this gives much lower score - to investigate # metrics.make_scorer(metrics.roc_auc_score, average=\"macro\", multi_class='ovr') X_train , y_train = df_folds [ predictor_cols ] . values , df_folds [ target_col ] . values y_train = prepare_y ( y_train ) grids = optimize_models ( models_list , X_train , y_train , scorer = roc_auc_scorer , steps = finetuning_pipeline_steps ) Fitting 5 folds for each of 20 candidates, totalling 100 fits Fitting 5 folds for each of 50 candidates, totalling 250 fits Fitting 5 folds for each of 8 candidates, totalling 40 fits # The above optimize code is equivalent to the below, for better readability # pipeline_logistic = make_finetuning_pipeline( # linear_model.LogisticRegression( # solver=\"saga\", random_state=config.seed, max_iter=10000, n_jobs=None, fit_intercept=True # ), steps=steps # ) # param_grid = dict( # model__penalty=[\"l1\", \"l2\"], # model__C=np.logspace(-4, 4, 10), # ) # grid = model_selection.GridSearchCV(pipeline_logistic, param_grid=param_grid, cv=5, refit=True, verbose=3, scoring = \"roc_auc\") # _ = grid.fit(X_train, y_train) We can save our results in a dataframe, we will also look at the top performing hyperparameter by querying the below: grid_cv_df = pd . DataFrame ( grid . cv_results_ ) grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ] # For example, we can see Logistic Regression's GridSearchCV # results like this. grid_cv_df = pd . DataFrame ( grids [ 0 ] . cv_results_ ) display ( grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_fit_time std_fit_time mean_score_time std_score_time param_model__C param_model__penalty params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score 8 0.931891 0.058794 0.002457 0.000387 0.359381 l1 {'model__C': 0.3593813663804626, 'model__penal... 0.997997 0.995547 0.997944 0.990132 0.995477 0.995419 0.002863 1 def return_grid_df ( grids : List [ model_selection . GridSearchCV ], ) -> Union [ pd . DataFrame , List [ model_selection . GridSearchCV ]]: \"\"\"Return a dataframe of the grids with shorted names. Args: grids (List[model_selection.GridSearchCV]): A list of GridSearchCV models that are tuned. Returns: grid_df, grids (Union[pd.DataFrame, List[model_selection.GridSearchCV]]): A dataframe of the grids with shorted names. \"\"\" def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name grid_df = [] for grid in grids : model_name = grid . estimator [ \"model\" ] . __class__ . __name__ cv_results = pd . DataFrame ( grid . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in grid . param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" , ] cv_results = cv_results [ column_results ] cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results [ \"model_name\" ] = model_name grid_df . append ( cv_results ) return grid_df , grids # grid_df and grids should necessarily be in the same sequence. # grid_df[0] == grids[0] in terms of model information, in this # case, the first index of both should be logistic regression. grid_df , grids = return_grid_df ( grids ) for model_df , grid in zip ( grid_df , grids ): best_hyperparams_df = model_df . iloc [[ 0 ]] model_name = best_hyperparams_df . model_name . unique ()[ 0 ] logger . info ( f \"Best hyperparameters found for { model_name } is as follows: \\n { grid . best_params_ } \" ) display ( best_hyperparams_df ) print () 2021-11-16,09:19:36 - Best hyperparameters found for LogisticRegression is as follows: {'model__C': 0.3593813663804626, 'model__penalty': 'l1'} 2021-11-16,09:19:36 - Best hyperparameters found for LogisticRegression is as follows: {'model__C': 0.3593813663804626, 'model__penalty': 'l1'} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } penalty C mean_test_score std_test_score rank_test_score model_name 8 l1 0.359381 0.995419 0.002863 1 LogisticRegression 2021-11-16,09:19:36 - Best hyperparameters found for DecisionTreeClassifier is as follows: {'model__criterion': 'entropy', 'model__max_depth': 10, 'model__min_samples_leaf': 10} 2021-11-16,09:19:36 - Best hyperparameters found for DecisionTreeClassifier is as follows: {'model__criterion': 'entropy', 'model__max_depth': 10, 'model__min_samples_leaf': 10} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } max_depth min_samples_leaf criterion mean_test_score std_test_score rank_test_score model_name 41 10 10 entropy 0.954515 0.015913 1 DecisionTreeClassifier 2021-11-16,09:19:37 - Best hyperparameters found for GradientBoostingClassifier is as follows: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__subsample': 0.5} 2021-11-16,09:19:37 - Best hyperparameters found for GradientBoostingClassifier is as follows: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__subsample': 0.5} .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } max_depth learning_rate subsample mean_test_score std_test_score rank_test_score model_name 1 3 0.1 0.5 0.991031 0.005869 1 GradientBoostingClassifier Success Our best performing set of hyperparameters for Logistic Regression {'model__C': 0.3593813663804626, 'model__penalty': 'l1'} gives rise to a mean cross validation score of \\(0.995419\\) , which is higher than the model with default hyperparameter scoring, \\(0.995\\) by a small margin. Not too surprising for Logistic Regression here since there aren't many things to tune, and should not see major improvements, but for Decesion Tree, it has increased from 0.907 to around 0.95, seeing quite a big jump with tuned params. DANGERRRRRRRRRRRRR I am being a bit hand wavy in terms of comparison here, I assumed THAT GridSearchCV used the exact same splitting strategy (yes it uses StratifiedKFold here) with the exact SEED/RANDOM_STATE , which I cannot promise as of now. Thus, a different splitting will, unfortunately, result in different results, although, I don't expect by a huge margin - so I think it is a no-go to compare like this. We can probably pass in a cv function into GridSearchCV to ensure seeding. This also highlights a problem that even K-fold splitting does not guarantee the reduction in variance. Room for Improvement Apart from the other methods to search for the optimal hyperparameters, we can also include preprocessing step as a tunable hyperparameter. More specifically, in our ReduceVIF() step, we hard coded two manual criterion in which the algorithm will stop; if the threshold reaches 10, or if the number of features removed hit 20; we can include them in the search space so we do not need to worry about how many features to remove!","title":"Search Space"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#model-persistence-saving-models","text":"Model Persistence We save our models using joblib and we can load it back any time. Note Save it to wandb or GCP storage to store models for better consistency. model_path = \"/content/\" def save_model ( grids : List [ Callable ], path : str ): \"\"\"Save a model to a file\"\"\" for grid in grids : model_name = grid . best_estimator_ [ \"model\" ] . __class__ . __name__ path_to_save = Path ( path , f \" { model_name } _grid.joblib\" ) # Dump to local path dump ( grid , Path ( path , path_to_save )) # Dump to wandb cloud # \"model.h5\" is saved in wandb.run.dir & will be uploaded at the end of training wandb . save ( os . path . join ( wandb . run . dir , path_to_save )) Save the model!","title":"Model Persistence (Saving Models)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#wandb","text":"We first see how we save and load using wandb. save_model ( grids , model_path ) \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\") \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\") \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\") logistic_path = \"LogisticRegression_grid.joblib\" # restore the model file \"model.h5\" from a specific run by user \"lavanyashukla\" # in project \"save_and_restore\" from run \"10pr4joa\" best_model = wandb . restore ( logistic_path ) # use the \"name\" attribute of the returned object # if your framework expects a filename, e.g. as in Keras # model.load_weights(best_model.name)","title":"Wandb"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#joblib","text":"We see how we use joblib to save and load. Load the model, and we can test it now if our loaded models is predicting correctly! logistic_grid = load ( \"/content/LogisticRegression_grid.joblib\" ) Great it seems to work!","title":"Joblib"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#sanity-check","text":"Note We just make sure our loaded weight from path is the same as the one we trained. We can easily compare predictions (or coefficients) by the following. load ( best_model . name ) . predict ( X_train ) . all () == logistic_grid . predict ( X_train ) . all () == grids [ 0 ] . predict ( X_train ) . all () True metrics . roc_auc_score ( y_train , logistic_grid . predict_proba ( X_train )[:, 1 ] ) == metrics . roc_auc_score ( y_train , grids [ 0 ] . predict_proba ( X_train )[:, 1 ] ) == metrics . roc_auc_score ( y_train , load ( best_model . name ) . predict_proba ( X_train )[:, 1 ] ) True Seems like the save and load method works perfectly. Warning Do not call this directly. grids [ 0 ] . best_estimator_ [ \"model\" ] . predict ( X_train ) This is because grids[0].best_estimator_[\"model\"] is only referring to the Logistic Regression Model WITHOUT the pipeline (preprocessing) steps. And hence will raise error if the preprocessing steps has feature selection. But the main idea is, be careful when using the above. # grids[0].best_estimator_[\"model\"].predict(X_train)","title":"Sanity Check"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#retrain-using-hyperparameters","text":"Retraining Methods From the discussion 1 , my doubts are cleared. Quoting verbatim from the discussion, we have: K-folds cross validation was devised as a way to assess model performance using training data. A great paper on this from Sebastian Raschka is a must read https://arxiv.org/abs/1811.12808. You use K-folds cv to tune you model, then retrain on all training data with best hyperparamters found. However, once you have run K-fold cv, you get \\(K\\) trained models. Kagglers quickly found that ensembling these models was giving good results at zero computation cost, rather than having to retrain a model on full data. It soon became a very common practice. Takeway For small-medium datasets, after finding the best hyperparameters \\(G\\) , we use \\(G\\) in our model \\(h\\) to train on the whole dataset \\(\\mathcal{X}\\) again to get the fitted parameters of \\(h\\) . Then you use the newly gained fitted parameters to then evaluate on the Test Set . For large and computationally expensive datasets, when you finished your K-folds, say 5 folds, you get 5 \"different\" models, \\(h_{i}, i \\in {1, 2, 3, 4, 5}\\) , what you can do is to save the weights (or in normal ML, weights refer to the parameters gained), and evaluate on the test set for each of the five models, you then get 5 different test predictions, and a common practice is the do a simple mean of these 5 set of predictions.","title":"Retrain using Hyperparameters"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#retrain-on-k-folds","text":"TODO: This should be easy for me as I dabbled more in Kaggle comp and are more familiar with this methodology.","title":"Retrain on K-Folds"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%207%20-%20Modelling%20%28Model%20Selection%20and%20Hyperparameter%20Tuning%29/#retrain-on-the-whole-training-set","text":"A common practice after the hyperparameter tuning phase is to retrain the model on the whole dataset \\(X_{\\text{train}}\\) where we will get the estimator's coefficients obtained from the retraining. This is actually already done as the scikit-learn's GridSearchCV has a parameter refit ; if we select it to be true, then after the model selection process is done (i.e. getting the best hyperparameters after cross validation with grid search), the grid search object will retrain on the whole \\(X_{\\text{train}}\\) with the best hyperparameters internally, and return us back an object in which we can call predict etc. Paranoia Alert However, to be extra careful, we can retrain manually using the best hyperparameters and check if scikit-learn is true to its documentation. We will just reconstruct the pipeline using the grid's best hyper parameters. We will then test if the retrained model's coefficients coincide with the grid's best estimator's coefficients. If there difference is 0, this means they are trained under the same circumstances and we can be sure that the refit parameter is behaving true to its words. grid_best_hyperparams = grid . best_params_ print ( grid_best_hyperparams ) -> { 'model__C' : 0.3593813663804626 , 'model__penalty' : 'l1' } retrain_logistic_pipeline = pipeline . Pipeline ( [ ( \"standardize\" , preprocessing . StandardScaler ()), ( \"remove_multicollinearity\" , ReduceVIF ( thresh = 10 )), ( \"model\" , linear_model . LogisticRegression ( C = 0.3593813663804626 , max_iter = 10000 , random_state = 1992 , solver = \"saga\" , penalty = \"l1\" , ), ), ] ) _ = retrain_logistic_pipeline . fit ( X_train , y_train ) logistic_grid = grids [ 0 ] coef_diff = ( retrain_logistic_pipeline [ \"model\" ] . coef_ - logistic_grid . best_estimator_ [ \"model\" ] . coef_ ) print ( \"...\" ) assert np . all ( coef_diff == 0 ) == True logger . info ( \"Retraining Assertion Passed!\" ) 2021-11-16,09:19:38 - Retraining Assertion Passed! 2021-11-16,09:19:38 - Retraining Assertion Passed! ... https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/275883 \u21a9","title":"Retrain on the whole training set"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/","text":"Stage 8: Model Evaluation by Hongnan Gao Dependencies and Configuration %% capture ! pip install - q wandb # !pip install -q shap ! pip install - q mlxtend == 0.19.0 ! pip install - q statsmodels == 0.13.1 # !pip install gcloud == 0.18.3 import wandb wandb . login () <IPython.core.display.Javascript object> \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc True import copy import csv import logging import os import random from dataclasses import asdict , dataclass , field from functools import wraps from pathlib import Path from time import time from typing import Any , Callable , Dict , List , Optional , Tuple , Union import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from joblib import dump , load from mlxtend.evaluate import bias_variance_decomp , paired_ttest_5x2cv from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS Utils and Configurations We need import ReduceVIF if not we cannot call our model. @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" df_folds : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ] ) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return asdict ( self ) # spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" # oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" # final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" # precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" # roc_plot = \"../data/images/roc_plot.png\" # feature_importance = \"../data/images/feature_importance.png\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # Utils functions that we need def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : # print(f\"Dropping {max_vif_col} with vif={max_vif}\") column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names def prepare_y ( y : np . ndarray ) -> np . ndarray : \"\"\"Prepare the target variable for the model. If Binary Classification, we need to ravel the array to 1d. Args: y (np.ndarray): Target variable. Returns: np.ndarray: Transformed Target variable. \"\"\" return y . ravel () if config . classification_type == \"binary\" else y config = config () logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df_folds = pd . read_csv ( config . df_folds ) # Assign predictors and target accordingly predictor_cols = df_folds . columns . to_list ()[: - 2 ] target_col = config . target_col Resume Run and Load Weights Here we resume wandb using its run_id and then load the model's weights. # Resume run by getting run_id # TODO: return id as an artifact so we can get it easily. run = wandb . init ( project = \"bcw\" , name = \"classification\" , resume = True , id = '3qh37hoo' ) \u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreighns\u001b[0m (use `wandb login --relogin` to force relogin) Resuming run classification to Weights & Biases ( docs ). logistic_path = \"LogisticRegression_grid.joblib\" dt_path = \"DecisionTreeClassifier_grid.joblib\" gdb_path = \"GradientBoostingClassifier_grid.joblib\" logistic_best_weight = wandb . restore ( logistic_path ) logistic_best_model = load ( logistic_best_weight . name ) \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout. \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout. Interpretation of Results Interpretation of Coefficients As shown in the figure below, all else being equal, for every square unit increase in mean cell area, the odds of the tumor being malignant increases by a factor of \\(e^{1.43} = 4.19\\) . The variation (standard error) of the characteristics of cells also are deemed important by the model, for example, area se played an important role in determining whether a cell is malignant; intuitively, if some cells are noticably larger than the rest, then it is also a good indicator of malignancy. selected_features_by_vif_index = logistic_best_model . best_estimator_ [ 'remove_multicollinearity' ] . column_indices_kept_ selected_feature_names = np . asarray ( predictor_cols )[ selected_features_by_vif_index ] selected_features_coefficients = logistic_best_model . best_estimator_ [ 'model' ] . coef_ . flatten () # assertion #assert grid.best_estimator_['remove_multicollinearity'].feature_names_ == retrain_pipeline['remove_multicollinearity'].feature_names_ fig , ax = plt . subplots ( figsize = ( 15 , 15 )) # .abs() _ = pd . Series ( selected_features_coefficients , index = selected_feature_names ) . sort_values () . plot ( ax = ax , kind = 'barh' ) # fig.savefig(config.feature_importance, format=\"png\", dpi=300) Interpretation of Metric Scores on Train Set We are also interested in choosing an optimal threshold for the model such that it gives the lowest recall, or False Negatives. We note that the default threshold when calling predict() from a model is \\(0.5\\) . In this section, we will explore one way to get the best tradeoff we can when choosing a high recall, while maintaining a reasonable score for precision. def evaluate_train_test_set ( estimator : Callable , X : Union [ pd . DataFrame , np . ndarray ], y : Union [ pd . DataFrame , np . ndarray ] ) -> Dict [ str , Union [ float , np . ndarray ]]: \"\"\"This function takes in X and y and returns a dictionary of scores. Args: estimator (Callable): [description] X (Union[pd.DataFrame, np.ndarray]): [description] y (Union[pd.DataFrame, np.ndarray]): [description] Returns: Dict[str, Union[float, np.ndarray]]: [description] \"\"\" test_results = {} y_pred = estimator . predict ( X ) # This is the probability array of class 1 (malignant) y_prob = estimator . predict_proba ( X )[:, 1 ] test_brier = metrics . brier_score_loss ( y , y_prob ) test_roc = metrics . roc_auc_score ( y , y_prob ) test_results [ \"brier\" ] = test_brier test_results [ \"roc\" ] = test_roc test_results [ \"y\" ] = np . asarray ( y ) . flatten () test_results [ \"y_pred\" ] = y_pred . flatten () test_results [ \"y_prob\" ] = y_prob . flatten () return test_results def plot_precision_recall_vs_threshold ( precisions , recalls , thresholds ): \"\"\" Modified from: Hands-On Machine learning with Scikit-Learn and TensorFlow; p.89 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( \"Precision and Recall Scores as a function of the decision threshold\" ) plt . plot ( thresholds , precisions [: - 1 ], \"b--\" , label = \"Precision\" ) plt . plot ( thresholds , recalls [: - 1 ], \"g-\" , label = \"Recall\" ) plt . ylabel ( \"Score\" ) plt . xlabel ( \"Decision Threshold\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . precision_recall_threshold_plot , format = \"png\" , dpi = 300 ) def plot_roc_curve ( fpr , tpr , label = None ): \"\"\" The ROC curve, modified from Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( 'ROC Curve' ) plt . plot ( fpr , tpr , linewidth = 2 , label = label ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . axis ([ - 0.005 , 1 , 0 , 1.005 ]) plt . xticks ( np . arange ( 0 , 1 , 0.05 ), rotation = 90 ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate (Recall)\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . roc_plot , format = \"png\" , dpi = 300 ) def adjusted_classes ( y_scores , t ): \"\"\" This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" return [ 1 if y >= t else 0 for y in y_scores ] The plots below show the tradeoffs between precision and recall, recall and false positive rate. The confusion matrix on the train set tells us that there is still more false negatives than false positives. We can choose a particular threshold in order to minimize false negatives, at some expense of false positive. X_train , y_train = df_folds [ predictor_cols ] . values , df_folds [ target_col ] . values y_train = prepare_y ( y_train ) train_results = evaluate_train_test_set ( logistic_best_model , X_train , y_train ) def plot_confusion_matrix ( y_true : np . ndarray , y_pred : np . ndarray , title : str , labels : List [ str ], tick_labels : List [ str ], ) -> None : \"\"\"Plots a Binary Confusion Matrix. Args: y_true (np.ndarray): the actual labels. y_pred (np.ndarray): the predicted labels. title (str): the title of the plot. tick_labels (List[str]): The labels for the ticks. \"\"\" # Unravel into tn, fp, fn and tp tn , fp , fn , tp = metrics . confusion_matrix ( y_true , y_pred , labels = labels ) . ravel () # reshape into tp, fp, fn, tn - this is personal preference reshaped_cm = np . asarray ([[ tp , fp ], [ fn , tn ]]) # flatten this 2d array cm_flattened = reshaped_cm . flatten () labels = [ \"True Positive\" , \"False Positive\" , \"False Negative\" , \"True Negative\" , ] annot = ( np . asarray ( [ f \" { label } \\n { cm_count } \" for label , cm_count in zip ( labels , cm_flattened ) ] ) ) . reshape ( 2 , 2 ) ax = plt . subplot () heatmap = sns . heatmap ( reshaped_cm , annot = annot , fmt = \"\" , cmap = \"Greens\" , ax = ax , xticklabels = tick_labels , yticklabels = tick_labels , ) ax . set_title ( title ) ax . set_xlabel ( \"Predicted labels\" ) ax . set_ylabel ( \"True labels\" ) plt . show () # CM y_true , y_pred = train_results [ 'y' ], train_results [ 'y_pred' ] plot_confusion_matrix ( y_true , y_pred , title = \"Confusion Matrix (Malignant as +)\" , labels = [ 0 , 1 ], tick_labels = [ \"benign\" , \"malignant\" ], ) # fig, ax = plt.subplots(figsize=(10, 10)) # # CM # cm_train = metrics.confusion_matrix(train_results['y'], train_results['y_pred']) # #### scores # auc = metrics.roc_auc_score(train_results['y'], train_results['y_prob']) # #### annotations # labels = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"] # counts = [\"{0:0.0f}\".format(value) for value in cm_train.flatten()] # percentages = [\"{0:.2%}\".format(value) for value in cm_train.flatten() / np.sum(cm_train)] # #### final annotations # label = ( # np.array([f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(labels, counts, percentages)]) # ).reshape(2, 2) # # heatmap # sns.heatmap( # data=cm_train, # vmin=0, # vmax=330, # cmap=[\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"], # linewidth=2, # linecolor=\"white\", # square=True, # ax=ax, # annot=label, # fmt=\"\", # cbar=False, # annot_kws={\"size\": 10, \"color\": \"black\", \"weight\": \"bold\", \"alpha\": 0.8}, # alpha=1, # ) # ax.scatter(1, 1, s=3500, c=\"white\") # ax.text( # 0.72, # 1.0, # \"AUC: {}\".format(round(auc, 3)), # {\"size\": 10, \"color\": \"black\", \"weight\": \"bold\"}, # ) # ## ticks and labels # ax.set_xticklabels(\"\") # ax.set_yticklabels(\"\") # ## titles and text # fig.text(0, 1.05, \"Train Set Confusion Matrix\", {\"size\": 22, \"weight\": \"bold\"}, alpha=1) # fig.text( # 0, # 1, # \"\"\"Training Set Confusion Matrix.\"\"\", # {\"size\": 12, \"weight\": \"normal\"}, # alpha=0.98, # ) # fig.tight_layout(pad=2.5, w_pad=2.5, h_pad=2.5) # # fig.savefig(config.final_train_confusion_matrix, format='png', dpi=300) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-26-3f454a3182d6> in <module>() 61 62 fig.tight_layout(pad=2.5, w_pad=2.5, h_pad=2.5) ---> 63 fig.savefig(config.final_train_confusion_matrix, format='png', dpi=300) AttributeError: 'config' object has no attribute 'final_train_confusion_matrix' # generate the precision recall curve precision , recall , pr_thresholds = metrics . precision_recall_curve ( train_results [ 'y' ], train_results [ 'y_prob' ]) fpr , tpr , roc_thresholds = metrics . roc_curve ( train_results [ 'y' ], train_results [ 'y_prob' ], pos_label = 1 ) # use the same p, r, thresholds that were previously calculated plot_precision_recall_vs_threshold ( precision , recall , pr_thresholds ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-28-1a8ed36985bb> in <module>() 1 # use the same p, r, thresholds that were previously calculated ----> 2 plot_precision_recall_vs_threshold(precision, recall, pr_thresholds) <ipython-input-18-2dad3ff8a911> in plot_precision_recall_vs_threshold(precisions, recalls, thresholds) 12 plt.xlabel(\"Decision Threshold\") 13 plt.legend(loc='best') ---> 14 plt.savefig(config.precision_recall_threshold_plot, format=\"png\", dpi=300) 15 16 def plot_roc_curve(fpr, tpr, label=None): AttributeError: 'config' object has no attribute 'precision_recall_threshold_plot' Based on the tradeoff plot above, a good threshold can be set at \\(t = 0.35\\) , let us see how it performs with this threshold. y_pred_adj = adjusted_classes ( train_results [ \"y_prob\" ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( train_results [ \"y\" ], y_pred_adj ), columns = [ \"pred_neg\" , \"pred_pos\" ], index = [ \"neg\" , \"pos\" ], ) ) pred_neg pred_pos neg 313 8 pos 5 186 print ( metrics . classification_report ( y_true = train_results [ \"y\" ], y_pred = y_pred_adj )) train_brier = train_results [ 'brier' ] print ( f \"train brier: { train_brier } \" ) precision recall f1-score support 0 0.98 0.98 0.98 321 1 0.96 0.97 0.97 191 accuracy 0.97 512 macro avg 0.97 0.97 0.97 512 weighted avg 0.97 0.97 0.97 512 train brier: 0.022402196649862854 The False Negatives reduced from 15 to 9, at the expense of increase False Positives from 6 to 14. We should take comfort that less patients are falsely classified as benign when in fact they need treatment. This is a tradeoff that we have to choose. The ROC curve below also paints a similar story, in order for you to have high recall, one must accept that there will more False Positives. plot_roc_curve ( fpr , tpr , 'recall_optimized' ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-31-893ec679a4c4> in <module>() ----> 1 plot_roc_curve(fpr, tpr, 'recall_optimized') <ipython-input-18-2dad3ff8a911> in plot_roc_curve(fpr, tpr, label) 29 plt.ylabel(\"True Positive Rate (Recall)\") 30 plt.legend(loc='best') ---> 31 plt.savefig(config.roc_plot, format=\"png\", dpi=300) 32 33 def adjusted_classes(y_scores, t): AttributeError: 'config' object has no attribute 'roc_plot' Evaluation on Test Set Ultimately, we are interested in finding the estimate of the generalization error of the model, and thus it is time for us to evaluate our model's performance on the \"unseen\" test set \\(X_{\\text{test}}\\) to get a good gauge on how well the model generalizes to unseen data. Take note that now everything has settled, we will use the exact hyperparameters to predict on test set, with the pre-defined threshold of 0.35. test_results = evaluate_train_test_set ( grid , X_test , y_test ) y_test_pred_adj = adjusted_classes ( test_results [ 'y_prob' ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( test_results [ 'y' ], y_test_pred_adj ), columns = [ 'pred_neg' , 'pred_pos' ], index = [ 'neg' , 'pos' ])) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-32-f47e1573a96d> in <module>() ----> 1 test_results = evaluate_train_test_set(grid, X_test, y_test) 2 y_test_pred_adj = adjusted_classes(test_results['y_prob'], t=0.35) 3 4 print(pd.DataFrame(metrics.confusion_matrix(test_results['y'], y_test_pred_adj), 5 columns=['pred_neg', 'pred_pos'], NameError: name 'grid' is not defined test_roc = test_results [ 'roc' ] test_brier = test_results [ 'brier' ] print ( test_roc ) print ( test_brier ) print ( metrics . classification_report ( y_true = test_results [ \"y\" ], y_pred = y_test_pred_adj )) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-33-0cc7fe660860> in <module>() ----> 1 test_roc = test_results['roc'] 2 test_brier = test_results['brier'] 3 print(test_roc) 4 print(test_brier) 5 print(metrics.classification_report(y_true=test_results[\"y\"], y_pred=y_test_pred_adj)) NameError: name 'test_results' is not defined Using the same threshold we used on training set, we see that the False Negative is quite low. The overall ROC score is 0.9828, and the corresponding Brier Score is 0.04136, both seem reasonably well performing. Benefit Structure Refer to health insurance project! https://ghnreigns.github.io/reighns-ml-website/metrics/classification_metrics/confusion_matrix/#benefit-structure # threshold_list : List[float] = [0.01, 0.1, 0.2, 0.5] # benefit_dict : Dict[str, int] = {\"old_structure\": {\"tp\":10, \"fn\":-10, \"fp\": -2, \"tp+fp\":-1}, # \"new_structure\": {\"tp\":100, \"fn\": -100, \"fp\": -2, \"tn+fp\":-1}} # columns = [\"threshold\", \"tp\", \"fn\", \"fp\", \"tn\", \"benefit_cost_old\", \"benefit_cost_new\"] # benefit_cost_list = [] # for t in threshold_list: # y_pred_adj = adjusted_classes(y_test_dt_prob, t=t) # cm = metrics.confusion_matrix(y_true=y_test_gt, y_pred = y_pred_adj) # tn, fp, fn, tp = metrics.confusion_matrix(y_true=y_test_gt, y_pred = y_pred_adj).ravel() # # this one check if it is correct formula # benefit_cost_old = tp*10 - fn*10 - fp*2 - (tp+fp)*1 # benefit_cost_new = tp*100 - fn*100 - fp*2 - (tp+fp)*1 # benefit_cost_list.append([t, tn, fn, fp, tn, benefit_cost_old, benefit_cost_new]) benefit_df = pd . DataFrame ( benefit_cost_list , columns = columns ) benefit_df Bias-Variance Tradeoff avg_expected_loss , avg_bias , avg_var = bias_variance_decomp ( grid . best_estimator_ [ 'model' ], X_train . values , y_train . values , X_test . values , y_test . values , loss = '0-1_loss' , random_seed = 123 ) print ( 'Average expected loss: %.3f ' % avg_expected_loss ) print ( 'Average bias: %.3f ' % avg_bias ) print ( 'Average variance: %.3f ' % avg_var ) We use the mlxtend library to estimate the Bias-Variance Tradeoff in our Logistic Regression model. The core idea behind this function is to use bagging and repeatedly sample from our training set so as to simulate that we are actually drawing samples from the \"true\" population over a distribution \\(\\mathcal{P}\\) . As expected, Logistic Regression being a linear model, its simplicity contributes to its high bias and low variance.","title":"Modelling (Model Evaluation)"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#dependencies-and-configuration","text":"%% capture ! pip install - q wandb # !pip install -q shap ! pip install - q mlxtend == 0.19.0 ! pip install - q statsmodels == 0.13.1 # !pip install gcloud == 0.18.3 import wandb wandb . login () <IPython.core.display.Javascript object> \u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc True import copy import csv import logging import os import random from dataclasses import asdict , dataclass , field from functools import wraps from pathlib import Path from time import time from typing import Any , Callable , Dict , List , Optional , Tuple , Union import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from joblib import dump , load from mlxtend.evaluate import bias_variance_decomp , paired_ttest_5x2cv from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS","title":"Dependencies and Configuration"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#utils-and-configurations","text":"We need import ReduceVIF if not we cannot call our model. @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\" processed_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\" df_folds : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\" train_size : float = 0.9 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"StratifiedKFold\" classification_type : str = \"binary\" target_col : List [ str ] = field ( default_factory = lambda : [ \"diagnosis\" ]) unwanted_cols : List [ str ] = field ( default_factory = lambda : [ \"id\" , \"Unnamed: 32\" ] ) # Plotting colors : List [ str ] = field ( default_factory = lambda : [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] ) cmap_reversed = plt . cm . get_cmap ( \"mako_r\" ) def to_dict ( self ) -> Dict [ str , Any ]: \"\"\"Convert the config object to a dictionary. Returns: Dict: The config object as a dictionary. \"\"\" return asdict ( self ) # spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" # oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" # final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" # precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" # roc_plot = \"../data/images/roc_plot.png\" # feature_importance = \"../data/images/feature_importance.png\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # Utils functions that we need def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : # print(f\"Dropping {max_vif_col} with vif={max_vif}\") column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names def prepare_y ( y : np . ndarray ) -> np . ndarray : \"\"\"Prepare the target variable for the model. If Binary Classification, we need to ravel the array to 1d. Args: y (np.ndarray): Target variable. Returns: np.ndarray: Transformed Target variable. \"\"\" return y . ravel () if config . classification_type == \"binary\" else y config = config () logger = init_logger () # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df_folds = pd . read_csv ( config . df_folds ) # Assign predictors and target accordingly predictor_cols = df_folds . columns . to_list ()[: - 2 ] target_col = config . target_col","title":"Utils and Configurations"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#resume-run-and-load-weights","text":"Here we resume wandb using its run_id and then load the model's weights. # Resume run by getting run_id # TODO: return id as an artifact so we can get it easily. run = wandb . init ( project = \"bcw\" , name = \"classification\" , resume = True , id = '3qh37hoo' ) \u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreighns\u001b[0m (use `wandb login --relogin` to force relogin) Resuming run classification to Weights & Biases ( docs ). logistic_path = \"LogisticRegression_grid.joblib\" dt_path = \"DecisionTreeClassifier_grid.joblib\" gdb_path = \"GradientBoostingClassifier_grid.joblib\" logistic_best_weight = wandb . restore ( logistic_path ) logistic_best_model = load ( logistic_best_weight . name ) \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout. \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.","title":"Resume Run and Load Weights"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#interpretation-of-results","text":"","title":"Interpretation of Results"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#interpretation-of-coefficients","text":"As shown in the figure below, all else being equal, for every square unit increase in mean cell area, the odds of the tumor being malignant increases by a factor of \\(e^{1.43} = 4.19\\) . The variation (standard error) of the characteristics of cells also are deemed important by the model, for example, area se played an important role in determining whether a cell is malignant; intuitively, if some cells are noticably larger than the rest, then it is also a good indicator of malignancy. selected_features_by_vif_index = logistic_best_model . best_estimator_ [ 'remove_multicollinearity' ] . column_indices_kept_ selected_feature_names = np . asarray ( predictor_cols )[ selected_features_by_vif_index ] selected_features_coefficients = logistic_best_model . best_estimator_ [ 'model' ] . coef_ . flatten () # assertion #assert grid.best_estimator_['remove_multicollinearity'].feature_names_ == retrain_pipeline['remove_multicollinearity'].feature_names_ fig , ax = plt . subplots ( figsize = ( 15 , 15 )) # .abs() _ = pd . Series ( selected_features_coefficients , index = selected_feature_names ) . sort_values () . plot ( ax = ax , kind = 'barh' ) # fig.savefig(config.feature_importance, format=\"png\", dpi=300)","title":"Interpretation of Coefficients"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#interpretation-of-metric-scores-on-train-set","text":"We are also interested in choosing an optimal threshold for the model such that it gives the lowest recall, or False Negatives. We note that the default threshold when calling predict() from a model is \\(0.5\\) . In this section, we will explore one way to get the best tradeoff we can when choosing a high recall, while maintaining a reasonable score for precision. def evaluate_train_test_set ( estimator : Callable , X : Union [ pd . DataFrame , np . ndarray ], y : Union [ pd . DataFrame , np . ndarray ] ) -> Dict [ str , Union [ float , np . ndarray ]]: \"\"\"This function takes in X and y and returns a dictionary of scores. Args: estimator (Callable): [description] X (Union[pd.DataFrame, np.ndarray]): [description] y (Union[pd.DataFrame, np.ndarray]): [description] Returns: Dict[str, Union[float, np.ndarray]]: [description] \"\"\" test_results = {} y_pred = estimator . predict ( X ) # This is the probability array of class 1 (malignant) y_prob = estimator . predict_proba ( X )[:, 1 ] test_brier = metrics . brier_score_loss ( y , y_prob ) test_roc = metrics . roc_auc_score ( y , y_prob ) test_results [ \"brier\" ] = test_brier test_results [ \"roc\" ] = test_roc test_results [ \"y\" ] = np . asarray ( y ) . flatten () test_results [ \"y_pred\" ] = y_pred . flatten () test_results [ \"y_prob\" ] = y_prob . flatten () return test_results def plot_precision_recall_vs_threshold ( precisions , recalls , thresholds ): \"\"\" Modified from: Hands-On Machine learning with Scikit-Learn and TensorFlow; p.89 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( \"Precision and Recall Scores as a function of the decision threshold\" ) plt . plot ( thresholds , precisions [: - 1 ], \"b--\" , label = \"Precision\" ) plt . plot ( thresholds , recalls [: - 1 ], \"g-\" , label = \"Recall\" ) plt . ylabel ( \"Score\" ) plt . xlabel ( \"Decision Threshold\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . precision_recall_threshold_plot , format = \"png\" , dpi = 300 ) def plot_roc_curve ( fpr , tpr , label = None ): \"\"\" The ROC curve, modified from Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( 'ROC Curve' ) plt . plot ( fpr , tpr , linewidth = 2 , label = label ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . axis ([ - 0.005 , 1 , 0 , 1.005 ]) plt . xticks ( np . arange ( 0 , 1 , 0.05 ), rotation = 90 ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate (Recall)\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . roc_plot , format = \"png\" , dpi = 300 ) def adjusted_classes ( y_scores , t ): \"\"\" This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" return [ 1 if y >= t else 0 for y in y_scores ] The plots below show the tradeoffs between precision and recall, recall and false positive rate. The confusion matrix on the train set tells us that there is still more false negatives than false positives. We can choose a particular threshold in order to minimize false negatives, at some expense of false positive. X_train , y_train = df_folds [ predictor_cols ] . values , df_folds [ target_col ] . values y_train = prepare_y ( y_train ) train_results = evaluate_train_test_set ( logistic_best_model , X_train , y_train ) def plot_confusion_matrix ( y_true : np . ndarray , y_pred : np . ndarray , title : str , labels : List [ str ], tick_labels : List [ str ], ) -> None : \"\"\"Plots a Binary Confusion Matrix. Args: y_true (np.ndarray): the actual labels. y_pred (np.ndarray): the predicted labels. title (str): the title of the plot. tick_labels (List[str]): The labels for the ticks. \"\"\" # Unravel into tn, fp, fn and tp tn , fp , fn , tp = metrics . confusion_matrix ( y_true , y_pred , labels = labels ) . ravel () # reshape into tp, fp, fn, tn - this is personal preference reshaped_cm = np . asarray ([[ tp , fp ], [ fn , tn ]]) # flatten this 2d array cm_flattened = reshaped_cm . flatten () labels = [ \"True Positive\" , \"False Positive\" , \"False Negative\" , \"True Negative\" , ] annot = ( np . asarray ( [ f \" { label } \\n { cm_count } \" for label , cm_count in zip ( labels , cm_flattened ) ] ) ) . reshape ( 2 , 2 ) ax = plt . subplot () heatmap = sns . heatmap ( reshaped_cm , annot = annot , fmt = \"\" , cmap = \"Greens\" , ax = ax , xticklabels = tick_labels , yticklabels = tick_labels , ) ax . set_title ( title ) ax . set_xlabel ( \"Predicted labels\" ) ax . set_ylabel ( \"True labels\" ) plt . show () # CM y_true , y_pred = train_results [ 'y' ], train_results [ 'y_pred' ] plot_confusion_matrix ( y_true , y_pred , title = \"Confusion Matrix (Malignant as +)\" , labels = [ 0 , 1 ], tick_labels = [ \"benign\" , \"malignant\" ], ) # fig, ax = plt.subplots(figsize=(10, 10)) # # CM # cm_train = metrics.confusion_matrix(train_results['y'], train_results['y_pred']) # #### scores # auc = metrics.roc_auc_score(train_results['y'], train_results['y_prob']) # #### annotations # labels = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"] # counts = [\"{0:0.0f}\".format(value) for value in cm_train.flatten()] # percentages = [\"{0:.2%}\".format(value) for value in cm_train.flatten() / np.sum(cm_train)] # #### final annotations # label = ( # np.array([f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(labels, counts, percentages)]) # ).reshape(2, 2) # # heatmap # sns.heatmap( # data=cm_train, # vmin=0, # vmax=330, # cmap=[\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"], # linewidth=2, # linecolor=\"white\", # square=True, # ax=ax, # annot=label, # fmt=\"\", # cbar=False, # annot_kws={\"size\": 10, \"color\": \"black\", \"weight\": \"bold\", \"alpha\": 0.8}, # alpha=1, # ) # ax.scatter(1, 1, s=3500, c=\"white\") # ax.text( # 0.72, # 1.0, # \"AUC: {}\".format(round(auc, 3)), # {\"size\": 10, \"color\": \"black\", \"weight\": \"bold\"}, # ) # ## ticks and labels # ax.set_xticklabels(\"\") # ax.set_yticklabels(\"\") # ## titles and text # fig.text(0, 1.05, \"Train Set Confusion Matrix\", {\"size\": 22, \"weight\": \"bold\"}, alpha=1) # fig.text( # 0, # 1, # \"\"\"Training Set Confusion Matrix.\"\"\", # {\"size\": 12, \"weight\": \"normal\"}, # alpha=0.98, # ) # fig.tight_layout(pad=2.5, w_pad=2.5, h_pad=2.5) # # fig.savefig(config.final_train_confusion_matrix, format='png', dpi=300) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-26-3f454a3182d6> in <module>() 61 62 fig.tight_layout(pad=2.5, w_pad=2.5, h_pad=2.5) ---> 63 fig.savefig(config.final_train_confusion_matrix, format='png', dpi=300) AttributeError: 'config' object has no attribute 'final_train_confusion_matrix' # generate the precision recall curve precision , recall , pr_thresholds = metrics . precision_recall_curve ( train_results [ 'y' ], train_results [ 'y_prob' ]) fpr , tpr , roc_thresholds = metrics . roc_curve ( train_results [ 'y' ], train_results [ 'y_prob' ], pos_label = 1 ) # use the same p, r, thresholds that were previously calculated plot_precision_recall_vs_threshold ( precision , recall , pr_thresholds ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-28-1a8ed36985bb> in <module>() 1 # use the same p, r, thresholds that were previously calculated ----> 2 plot_precision_recall_vs_threshold(precision, recall, pr_thresholds) <ipython-input-18-2dad3ff8a911> in plot_precision_recall_vs_threshold(precisions, recalls, thresholds) 12 plt.xlabel(\"Decision Threshold\") 13 plt.legend(loc='best') ---> 14 plt.savefig(config.precision_recall_threshold_plot, format=\"png\", dpi=300) 15 16 def plot_roc_curve(fpr, tpr, label=None): AttributeError: 'config' object has no attribute 'precision_recall_threshold_plot' Based on the tradeoff plot above, a good threshold can be set at \\(t = 0.35\\) , let us see how it performs with this threshold. y_pred_adj = adjusted_classes ( train_results [ \"y_prob\" ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( train_results [ \"y\" ], y_pred_adj ), columns = [ \"pred_neg\" , \"pred_pos\" ], index = [ \"neg\" , \"pos\" ], ) ) pred_neg pred_pos neg 313 8 pos 5 186 print ( metrics . classification_report ( y_true = train_results [ \"y\" ], y_pred = y_pred_adj )) train_brier = train_results [ 'brier' ] print ( f \"train brier: { train_brier } \" ) precision recall f1-score support 0 0.98 0.98 0.98 321 1 0.96 0.97 0.97 191 accuracy 0.97 512 macro avg 0.97 0.97 0.97 512 weighted avg 0.97 0.97 0.97 512 train brier: 0.022402196649862854 The False Negatives reduced from 15 to 9, at the expense of increase False Positives from 6 to 14. We should take comfort that less patients are falsely classified as benign when in fact they need treatment. This is a tradeoff that we have to choose. The ROC curve below also paints a similar story, in order for you to have high recall, one must accept that there will more False Positives. plot_roc_curve ( fpr , tpr , 'recall_optimized' ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-31-893ec679a4c4> in <module>() ----> 1 plot_roc_curve(fpr, tpr, 'recall_optimized') <ipython-input-18-2dad3ff8a911> in plot_roc_curve(fpr, tpr, label) 29 plt.ylabel(\"True Positive Rate (Recall)\") 30 plt.legend(loc='best') ---> 31 plt.savefig(config.roc_plot, format=\"png\", dpi=300) 32 33 def adjusted_classes(y_scores, t): AttributeError: 'config' object has no attribute 'roc_plot'","title":"Interpretation of Metric Scores on Train Set"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#evaluation-on-test-set","text":"Ultimately, we are interested in finding the estimate of the generalization error of the model, and thus it is time for us to evaluate our model's performance on the \"unseen\" test set \\(X_{\\text{test}}\\) to get a good gauge on how well the model generalizes to unseen data. Take note that now everything has settled, we will use the exact hyperparameters to predict on test set, with the pre-defined threshold of 0.35. test_results = evaluate_train_test_set ( grid , X_test , y_test ) y_test_pred_adj = adjusted_classes ( test_results [ 'y_prob' ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( test_results [ 'y' ], y_test_pred_adj ), columns = [ 'pred_neg' , 'pred_pos' ], index = [ 'neg' , 'pos' ])) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-32-f47e1573a96d> in <module>() ----> 1 test_results = evaluate_train_test_set(grid, X_test, y_test) 2 y_test_pred_adj = adjusted_classes(test_results['y_prob'], t=0.35) 3 4 print(pd.DataFrame(metrics.confusion_matrix(test_results['y'], y_test_pred_adj), 5 columns=['pred_neg', 'pred_pos'], NameError: name 'grid' is not defined test_roc = test_results [ 'roc' ] test_brier = test_results [ 'brier' ] print ( test_roc ) print ( test_brier ) print ( metrics . classification_report ( y_true = test_results [ \"y\" ], y_pred = y_test_pred_adj )) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-33-0cc7fe660860> in <module>() ----> 1 test_roc = test_results['roc'] 2 test_brier = test_results['brier'] 3 print(test_roc) 4 print(test_brier) 5 print(metrics.classification_report(y_true=test_results[\"y\"], y_pred=y_test_pred_adj)) NameError: name 'test_results' is not defined Using the same threshold we used on training set, we see that the False Negative is quite low. The overall ROC score is 0.9828, and the corresponding Brier Score is 0.04136, both seem reasonably well performing.","title":"Evaluation on Test Set"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#benefit-structure","text":"Refer to health insurance project! https://ghnreigns.github.io/reighns-ml-website/metrics/classification_metrics/confusion_matrix/#benefit-structure # threshold_list : List[float] = [0.01, 0.1, 0.2, 0.5] # benefit_dict : Dict[str, int] = {\"old_structure\": {\"tp\":10, \"fn\":-10, \"fp\": -2, \"tp+fp\":-1}, # \"new_structure\": {\"tp\":100, \"fn\": -100, \"fp\": -2, \"tn+fp\":-1}} # columns = [\"threshold\", \"tp\", \"fn\", \"fp\", \"tn\", \"benefit_cost_old\", \"benefit_cost_new\"] # benefit_cost_list = [] # for t in threshold_list: # y_pred_adj = adjusted_classes(y_test_dt_prob, t=t) # cm = metrics.confusion_matrix(y_true=y_test_gt, y_pred = y_pred_adj) # tn, fp, fn, tp = metrics.confusion_matrix(y_true=y_test_gt, y_pred = y_pred_adj).ravel() # # this one check if it is correct formula # benefit_cost_old = tp*10 - fn*10 - fp*2 - (tp+fp)*1 # benefit_cost_new = tp*100 - fn*100 - fp*2 - (tp+fp)*1 # benefit_cost_list.append([t, tn, fn, fp, tn, benefit_cost_old, benefit_cost_new]) benefit_df = pd . DataFrame ( benefit_cost_list , columns = columns ) benefit_df","title":"Benefit Structure"},{"location":"supervised_learning/classification/breast_cancer_wisconsin/Stage%208%20-%20Modelling%20%28Model%20Evaluation%20and%20Interpretation%29/#bias-variance-tradeoff","text":"avg_expected_loss , avg_bias , avg_var = bias_variance_decomp ( grid . best_estimator_ [ 'model' ], X_train . values , y_train . values , X_test . values , y_test . values , loss = '0-1_loss' , random_seed = 123 ) print ( 'Average expected loss: %.3f ' % avg_expected_loss ) print ( 'Average bias: %.3f ' % avg_bias ) print ( 'Average variance: %.3f ' % avg_var ) We use the mlxtend library to estimate the Bias-Variance Tradeoff in our Logistic Regression model. The core idea behind this function is to use bagging and repeatedly sample from our training set so as to simulate that we are actually drawing samples from the \"true\" population over a distribution \\(\\mathcal{P}\\) . As expected, Logistic Regression being a linear model, its simplicity contributes to its high bias and low variance.","title":"Bias-Variance Tradeoff"},{"location":"supervised_learning/regression/DBA%203803%20Project%20-%20Hongnan%20%281%29/","text":"import csv import random from functools import wraps from time import time from typing import Callable , Dict , List , Union , Any , Optional import copy import matplotlib.pyplot as plt import mlxtend import numpy as np import pandas as pd import seaborn as sns from mlxtend.evaluate import paired_ttest_5x2cv , bias_variance_decomp from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) # Created a config class to write global parameters. class global_config : # File Path raw_data = \"../data/raw/data.csv\" processed_data = \"Project2_Train.csv\" test_data = \"Project2_Test.csv\" spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\" oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\" final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\" precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\" roc_plot = \"../data/images/roc_plot.png\" feature_importance = \"../data/images/feature_importance.png\" # Data Information target = [ \"Response\" ] unwanted_cols = [ \"id\" , \"Unnamed: 32\" ] classification_type = \"binary\" # Plotting colors = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ] cmap_reversed = plt . cm . get_cmap ( 'mako_r' ) # Seed Number seed = 1992 # Cross Validation num_folds = 5 cv_schema = \"StratifiedKFold\" split_size = { \"train_size\" : 1 , \"test_size\" : 0 } def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) # Read the csv file to a pandas dataframe. config = global_config # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # read data df = pd . read_csv ( config . processed_data ) df_test = pd . read_csv ( config . test_data ) # Drop the redundant column id. df . drop ( columns = [ \"id\" ], inplace = True ) df_test . drop ( columns = [ \"id\" ], inplace = True ) We first manually map the categorical features, Note the manual mapping we mapped letters to numbers, but for now, we will keep the 0, 1, 2 etc as strings, as function calls such as pd.get_dummies and OneHotEncoder work better when they see strings (which is a major signal for them to convert those columns). categorical_feat = [ 'Gender' , 'Vehicle_Age' , 'Vehicle_Damage' , 'Region_Code' ] continuous_feat = [ 'Age' , 'Annual_Premium' , 'Vintage' ] class_feat = [ 'Response' ] # assert that these columns are of the correct dtype! df = df . astype ( { \"Age\" : \"int64\" , \"Region_Code\" : \"int64\" , \"Annual_Premium\" : \"int64\" , \"Vintage\" : \"int64\" , \"Gender\" : \"str\" , \"Vehicle_Age\" : \"str\" , \"Vehicle_Damage\" : \"str\" , \"Response\" : \"int64\" } ) df_test = df_test . astype ( { \"Age\" : \"int64\" , \"Region_Code\" : \"int64\" , \"Annual_Premium\" : \"int64\" , \"Vintage\" : \"int64\" , \"Gender\" : \"str\" , \"Vehicle_Age\" : \"str\" , \"Vehicle_Damage\" : \"str\" , \"Response\" : \"int64\" } ) gender_map = { \"Female\" : \"0\" , \"Male\" : \"1\" } vehicle_age_map = { \"< 1 Year\" : \"0\" , \"1-2 Year\" : \"1\" , \"> 2 Years\" : \"2\" } vehicle_dmg_map = { \"No\" : \"0\" , \"Yes\" : \"1\" } region_code_map = { code : code for code in df . Region_Code . unique ()} cat_feature_map = { \"Gender\" : gender_map , \"Vehicle_Age\" : vehicle_age_map , \"Vehicle_Damage\" : vehicle_dmg_map , \"Region_Code\" : region_code_map } for cat_col in categorical_feat : df [ cat_col ] = df [ cat_col ] . map ( cat_feature_map [ cat_col ]) df_test [ cat_col ] = df_test [ cat_col ] . map ( cat_feature_map [ cat_col ]) X = df . copy () y = X . pop ( \"Response\" ) X_test = df_test . copy () y_test = X_test . pop ( \"Response\" ) # The below 3 code cells are just applying one hot encoding to gender and vehicle damage columns import logging log = logging . getLogger ( __name__ ) logging . basicConfig ( level = logging . INFO ) class CategoricalDummyCoder ( base . TransformerMixin ): \"\"\"https://stackoverflow.com/questions/39923927/pandas-sklearn-one-hot-encoding-dataframe-or-numpy Identifies categorical columns by dtype of object and dummy codes them. Optionally a pandas.DataFrame can be returned where categories are of pandas.Category dtype and not binarized for better coding strategies than dummy coding.\"\"\" def __init__ ( self , only_categoricals = False , selected_columns = None , ** get_dummies_kwargs ): self . categorical_variables = [] self . categories_per_column = {} self . only_categoricals = only_categoricals self . selected_columns = selected_columns self . get_dummies_kwargs = get_dummies_kwargs [ 'get_dummies_kwargs' ] def fit ( self , X , y = None ): if self . selected_columns is None : logging . info ( \"No selected columns by user, we will use all columns that is string\" ) self . categorical_variables = list ( X . select_dtypes ( include = [ 'object' ]) . columns ) else : logging . info ( \"Users have chosen selected columns to fit\" ) self . categorical_variables = list ( self . selected_columns ) logging . debug ( f 'identified the following categorical variables: { self . categorical_variables } ' ) for col in self . categorical_variables : self . categories_per_column [ col ] = X [ col ] . astype ( 'category' ) . cat . categories logging . debug ( 'fitted categories' ) return self def transform ( self , X ): for col in self . categorical_variables : logging . debug ( f 'transforming cat col: { col } ' ) X [ col ] = pd . Categorical ( X [ col ], categories = self . categories_per_column [ col ]) if self . only_categoricals : X [ col ] = X [ col ] . cat . codes if not self . only_categoricals : selected_cat_feat_df = pd . get_dummies ( X [ self . categorical_variables ], ** self . get_dummies_kwargs ) concat_df = pd . concat ([ X , selected_cat_feat_df ], axis = 1 ) . drop ( columns = self . categorical_variables , axis = 1 ) return concat_df else : return X one_hot_encoder = CategoricalDummyCoder ( only_categoricals = False , selected_columns = [ \"Vehicle_Age\" , 'Vehicle_Damage' , \"Gender\" , \"Region_Code\" ], get_dummies_kwargs = { \"sparse\" : True , \"drop_first\" : False }) X = one_hot_encoder . fit_transform ( X ) X_test = one_hot_encoder . transform ( X_test ) INFO:root:Users have chosen selected columns to fit predictor_cols = X . columns . to_list () # Split train - test X_train , y_train = X . copy (), y . copy () print ( f \"Y Train Distribution is : { y_train . value_counts ( normalize = True ) . to_dict () } \" ) print ( f \"Y Test Distribution is : { y_test . value_counts ( normalize = True ) . to_dict () } \" ) Y Train Distribution is : {0: 0.8674, 1: 0.1326} Y Test Distribution is : {0: 0.8636, 1: 0.1364} # The make fold function will further split the x training set into 5 different folds, and we stratified them accordingly. def make_folds ( df : pd . DataFrame , num_folds : int , cv_schema : str , seed : int , predictor_col : List , target_col : List , ) -> pd . DataFrame : \"\"\"Split the given dataframe into training folds. Args: df (pd.DataFrame): [description] num_folds (int): [description] cv_schema (str): [description] seed (int): [description] Returns: pd.DataFrame: [description] \"\"\" if cv_schema == \"KFold\" : df_folds = df . copy () kf = model_selection . KFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( kf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) elif cv_schema == \"StratifiedKFold\" : df_folds = df . copy () skf = model_selection . StratifiedKFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( skf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) print ( df_folds . groupby ([ \"fold\" , target_col [ 0 ]]) . size ()) return df_folds X_y_train = pd . concat ([ X_train , y_train ], axis = 1 ) . reset_index ( drop = True ) df_folds = make_folds ( X_y_train , num_folds = config . num_folds , cv_schema = config . cv_schema , seed = config . seed , predictor_col = predictor_cols , target_col = config . target ) fold Response 1 0 1735 1 265 2 0 1735 1 265 3 0 1735 1 265 4 0 1735 1 265 5 0 1734 1 266 dtype: int64 # The make_pipeline function will first standardize our training data, then fit the model. # create a feature preparation pipeline for a model def make_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline # Now we define a list of classifier, which is a few models of our choice. classifiers = [ # linear model linear_model . LogisticRegression ( random_state = config . seed , solver = \"liblinear\" ), # # tree tree . DecisionTreeClassifier ( random_state = config . seed ), # # ensemble ensemble . RandomForestClassifier ( n_estimators = 100 , random_state = config . seed ), ] classifiers = [ make_pipeline ( model ) for model in classifiers ] Utils Results Class This class is an abstract data structure that takes in default result names and store the results in columnwise format. This is abstract also because if you call model_results which is a dictionary, it returns {'DummyClassifier': <__main__.Results at 0x1fd24436ca0>} . default_result_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" , \"brier_loss\" , \"roc\" , \"precision\" , \"recall\" , \"f1\" , \"confusion_matrix\" , ] default_logit_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" ] default_score_names = [ \"brier_loss\" , \"roc\" , \"precision\" , \"recall\" , \"f1\" , \"confusion_matrix\" ] class Results : \"\"\"Stores results for model training in columnwise format.\"\"\" _result_dict : Dict logit_names : List [ str ] score_names : List [ str ] def __init__ ( self , logit_names : List [ str ] = default_logit_names , score_names : List [ str ] = default_score_names , existing_dict : Optional [ Dict ] = None , ): \"\"\"Construct a new results store.\"\"\" self . logit_names = logit_names self . score_names = score_names if existing_dict is not None : self . _result_dict = copy . deepcopy ( existing_dict ) return dict_keys = [ \"identifier\" , * logit_names , * score_names ] self . _result_dict = { key : [] for key in dict_keys } def add ( self , identifier : str , results : Dict , in_place = False ): \"\"\"Add a new results row.\"\"\" if not in_place : return Results ( self . logit_names , self . score_names , self . _result_dict ) . add ( identifier , results , in_place = True ) self . _result_dict [ \"identifier\" ] . append ( identifier ) for result_name in set ([ * results . keys (), * self . logit_names , * self . score_names ]): result_value = results . get ( result_name , np . nan ) self . _result_dict [ result_name ] . append ( result_value ) return self def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } def get_result_values ( self , result_name : str ) -> List [ Any ]: \"\"\"Get a list of values for a result.\"\"\" return self . _result_dict [ result_name ] def to_dataframe ( self ) -> pd . DataFrame : \"\"\"Get a Data Frame containing the results.\"\"\" return pd . DataFrame . from_dict ( self . _result_dict ) def to_dict ( self ) -> Dict : \"\"\"Get a dictionary containing the results. Returns: Dict[str, List[Any]]: Dictionary of result columns \"\"\" return self . _result_dict Metrics def multiclass_label_binarize ( y , classes , pos_label = 1 , neg_label = 0 ): \"\"\"Binarize labels in one-vs-all fashion. Args: y (np.ndarray) Sequence of integer labels to encode classes (array-like) Labels for each class pos_label (int) Value for positive labels neg_label (int) Value for negative labels Returns: np.ndarray of shape (n_samples, n_classes) Encoded dataset \"\"\" columns = [ np . where ( y == label , pos_label , neg_label ) for label in classes ] return np . column_stack ( columns ) def multiclass_roc_auc_score ( y_true , y_score , classes = None ): \"\"\"Compute ROC-AUC score for each class in a multiclass dataset. Args: y_true (np.ndarray of shape (n_samples, n_classes)) True labels y_score (np.ndarray of shape (n_samples, n_classes)) Target scores classes (array-like of shape (n_classes,)) List of dataset classes. If `None`, the lexicographical order of the labels in `y_true` is used. Returns: array-like: ROC-AUC score for each class, in the same order as `classes` \"\"\" classes = ( np . unique ( y_true ) if classes is None else classes ) y_true_multiclass = multiclass_label_binarize ( y_true , classes = classes ) def oneclass_roc_auc_score ( class_id ): y_true_class = y_true_multiclass [:, class_id ] y_score_class = y_score [:, class_id ] fpr , tpr , _ = metrics . roc_curve ( y_true = y_true_class , y_score = y_score_class , pos_label = 1 ) return metrics . auc ( fpr , tpr ) return [ oneclass_roc_auc_score ( class_id ) for class_id in range ( len ( classes )) ] def compute_metrics ( logits : Dict [ str , np . ndarray ]) -> Dict [ str , Any ]: \"\"\"Compute metrics from logits.\"\"\" y_val , y_val_pred , y_val_prob = logits [ \"y_true\" ], logits [ \"y_pred\" ], logits [ \"y_prob\" ] #val_score = metrics.roc_auc_score( # y_true=y_val, # y_score=y_val_prob #) val_score = multiclass_roc_auc_score ( y_true = y_val , y_score = y_val_prob ) precision , recall , fbeta_score , _ = metrics . precision_recall_fscore_support ( y_true = y_val , y_pred = y_val_pred , labels = np . unique ( y_val ), average = None ) brier_loss = ( metrics . brier_score_loss ( y_true = y_val , y_prob = y_val_prob [:, 1 ] ) if config . classification_type == \"binary\" else np . nan ) confusion_matrix = metrics . confusion_matrix ( y_val , y_val_pred ) return { \"roc\" : val_score , \"precision\" : precision , \"recall\" : recall , \"f1\" : fbeta_score , \"brier_loss\" : brier_loss , \"confusion_matrix\" : confusion_matrix } def prepare_y ( y ): return ( y . ravel () if config . classification_type == \"binary\" else y ) def mean_score ( score_values ) -> Union [ float , np . ndarray ]: \"\"\"Compute the mean score.\"\"\" score_values = np . array ( score_values ) shape = score_values . shape if len ( shape ) == 1 : return score_values . mean () return score_values . mean ( axis = 0 ) def mean_cv_results ( model_results : Results ) -> Dict : \"\"\"Add mean cross-validation results. This method computes the mean value for all score types in the model_results, including for scores (e.g., confusion matrix) where the mean value may contain decimal places. \"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result )) for y_result in model_results . logit_names } cv_scores = { score : mean_score ( model_results . get_result_values ( score ) ) for score in model_results . score_names } return { ** cv_logits , ** cv_scores , } def oof_cv_results ( model_results : Results ) -> Dict : \"\"\"Add OOF cross-validation results.\"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result ) ) for y_result in model_results . logit_names } cv_scores = compute_metrics ( cv_logits ) return { ** cv_logits , ** cv_scores , } def add_cv_results ( model_results : Results ): \"\"\"Add cross-validation results. This method returns a copy of the given model results with summary columns for mean and CV cross-validation. \"\"\" mean_cv = mean_cv_results ( model_results ) oof_cv = oof_cv_results ( model_results ) return ( model_results . add ( \"mean_cv\" , mean_cv ) . add ( \"oof_cv\" , oof_cv ) ) def train_on_fold ( df_folds : pd . DataFrame , models : List [ Callable ], num_folds : int , predictor_col : List , target_col : List , ) -> Dict [ str , List ]: \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results. Args: df_folds (pd.DataFrame): [description] model (Callable): [description] num_folds (int): [description] predictor_col (List): [description] target_col (List): [description] Returns: Dict[str, List]: [description] \"\"\" y_true = df_folds [ target_col ] . values . flatten () # test_pred_arr: np.ndarray = np.zeros(len(X_test)) model_dict = {} for model in models : model_results = Results () if isinstance ( model , pipeline . Pipeline ): model_name = model [ \"model\" ] . __class__ . __name__ else : model_name = model . __class__ . __name__ # out-of-fold validation predictions oof_pred_arr : np . ndarray = np . zeros ( len ( df_folds )) for fold in range ( 1 , num_folds + 1 ): train_df = df_folds [ df_folds [ \"fold\" ] != fold ] . reset_index ( drop = True ) val_df = df_folds [ df_folds [ \"fold\" ] == fold ] . reset_index ( drop = True ) val_idx = df_folds [ df_folds [ \"fold\" ] == fold ] . index . values X_train , y_train = train_df [ predictor_col ] . values , prepare_y ( train_df [ target_col ] . values ) X_val , y_val = val_df [ predictor_col ] . values , prepare_y ( val_df [ target_col ] . values ) model . fit ( X_train , y_train ) y_val_pred = model . predict ( X_val ) y_val_prob = model . predict_proba ( X_val ) logits = { \"y_true\" : y_val , \"y_pred\" : y_val_pred , \"y_prob\" : y_val_prob , } metrics = compute_metrics ( logits ) model_results . add ( f \"fold { fold } \" , { ** logits , ** metrics }, in_place = True ) if model_name not in model_dict : model_dict [ model_name ] = model_results return model_dict model_dict = train_on_fold ( df_folds , models = classifiers , num_folds = 5 , predictor_col = predictor_cols , target_col = config . target ) C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) model_dict_with_summary = { model : add_cv_results ( model_results ) for model , model_results in model_dict . items () } results_df = pd . concat ({ name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () }, axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv LogisticRegression identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[0.9913007976279888, 0.00869920237201111], [0... [[0.6945486216835197, 0.30545137831648034], [0... [[0.9958858145499713, 0.004114185450028691], [... [[0.8859877852400798, 0.11401221475992017], [0... [[0.9977204955737388, 0.0022795044262612914], ... [[0.9913007976279888, 0.00869920237201111], [0... [[0.9913007976279888, 0.00869920237201111], [0... brier_loss 0.095722 0.097785 0.096882 0.096771 0.09971 0.097374 0.097374 roc [0.829466586917514, 0.8294665869175141] [0.811457778261106, 0.811457778261106] [0.82251101082051, 0.82251101082051] [0.8232026534718069, 0.8232026534718069] [0.7994488817198706, 0.7994488817198706] [0.8172173822381614, 0.8172173822381614] [0.816942573130776, 0.8169425731307758] precision [0.8674337168584292, 0.0] [0.8675, 0.0] [0.8675, 0.0] [0.8678017025538307, 0.3333333333333333] [0.867, 0.0] [0.8674470838824521, 0.06666666666666667] [0.8674469787915166, 0.25] recall [0.9994236311239193, 0.0] [1.0, 0.0] [1.0, 0.0] [0.9988472622478386, 0.0037735849056603774] [1.0, 0.0] [0.9996541786743517, 0.0007547169811320754] [0.999654138805626, 0.0007541478129713424] f1 [0.9287627209426889, 0.0] [0.92904953145917, 0.0] [0.92904953145917, 0.0] [0.9287245444801714, 0.007462686567164179] [0.9287627209426887, 0.0] [0.9288698098567778, 0.0014925373134328358] [0.9288698446705945, 0.0015037593984962407] confusion_matrix [[1734, 1], [265, 0]] [[1735, 0], [265, 0]] [[1735, 0], [265, 0]] [[1733, 2], [264, 1]] [[1734, 0], [266, 0]] [[1734.2, 0.6], [265.0, 0.2]] [[8671, 3], [1325, 1]] DecisionTreeClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... brier_loss 0.194 0.188 0.2045 0.186 0.1895 0.1924 0.1924 roc [0.6164210755260726, 0.6164210755260725] [0.6198792887825567, 0.6198792887825567] [0.5656081779131097, 0.5656081779131097] [0.5938556902832908, 0.5938556902832907] [0.6090464049396849, 0.6090464049396849] [0.600962127488943, 0.600962127488943] [0.6009676462415547, 0.6009676462415547] precision [0.8992294013040901, 0.3035143769968051] [0.899941141848146, 0.31561461794019935] [0.8850174216027874, 0.24100719424460432] [0.8923431203223949, 0.2965779467680608] [0.8964306612053833, 0.30584192439862545] [0.8945923492565603, 0.292511212069659] [0.894552256254384, 0.29322268326417705] recall [0.8743515850144092, 0.3584905660377358] [0.8812680115273775, 0.3584905660377358] [0.878386167146974, 0.2528301886792453] [0.8933717579250721, 0.2943396226415094] [0.8835063437139562, 0.33458646616541354] [0.8821767730655579, 0.319747481912328] [0.8821766197832603, 0.31975867269984914] f1 [0.886616014026885, 0.328719723183391] [0.8905066977285965, 0.33568904593639576] [0.8816893260052068, 0.24677716390423574] [0.8928571428571428, 0.2954545454545454] [0.8899215800174267, 0.31956912028725315] [0.8883181521270516, 0.3052419197531642] [0.8883213373577897, 0.3059163059163059] confusion_matrix [[1517, 218], [170, 95]] [[1529, 206], [170, 95]] [[1524, 211], [198, 67]] [[1550, 185], [187, 78]] [[1532, 202], [177, 89]] [[1530.4, 204.4], [180.4, 84.8]] [[7652, 1022], [902, 424]] RandomForestClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[0.81, 0.19], [0.88, 0.12], [1.0, 0.0], [1.0,... [[1.0, 0.0], [1.0, 0.0], [0.64, 0.36], [0.97, ... [[0.98, 0.02], [0.49, 0.51], [0.99, 0.01], [0.... [[1.0, 0.0], [0.54, 0.46], [1.0, 0.0], [0.69, ... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... brier_loss 0.107495 0.106624 0.106167 0.103634 0.10682 0.106148 0.106148 roc [0.7979500842803545, 0.7979500842803545] [0.7956076341688869, 0.7956076341688869] [0.798345930074493, 0.798345930074493] [0.8149660159860801, 0.8149660159860802] [0.7988808526506578, 0.7988808526506577] [0.8011501034320945, 0.8011501034320944] [0.8012694444763238, 0.8012694444763238] precision [0.8798511430090378, 0.3277310924369748] [0.8757861635220126, 0.30434782608695654] [0.8771097046413502, 0.3076923076923077] [0.8760460251046025, 0.3181818181818182] [0.8754598003152917, 0.29896907216494845] [0.876850567318459, 0.31138442331260113] [0.8768421052631579, 0.312] recall [0.9538904899135446, 0.1471698113207547] [0.9631123919308358, 0.10566037735849057] [0.9585014409221903, 0.12075471698113208] [0.9654178674351584, 0.10566037735849057] [0.9607843137254902, 0.10902255639097744] [0.9603413007854439, 0.11765356788196908] [0.9603412497117824, 0.11764705882352941] f1 [0.9153761061946902, 0.203125] [0.9173757891847379, 0.1568627450980392] [0.9160011016248968, 0.1734417344173442] [0.9185632026323005, 0.15864022662889515] [0.9161396755567776, 0.15977961432506887] [0.9166911750386806, 0.1703698640938695] [0.9166941784967537, 0.17086527929901427] confusion_matrix [[1655, 80], [226, 39]] [[1671, 64], [237, 28]] [[1663, 72], [233, 32]] [[1675, 60], [237, 28]] [[1666, 68], [237, 29]] [[1666.0, 68.8], [234.0, 31.2]] [[8330, 344], [1170, 156]] def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } results_df = pd . concat ({ name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () }, axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv LogisticRegression identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[0.9913007976279888, 0.00869920237201111], [0... [[0.6945486216835197, 0.30545137831648034], [0... [[0.9958858145499713, 0.004114185450028691], [... [[0.8859877852400798, 0.11401221475992017], [0... [[0.9977204955737388, 0.0022795044262612914], ... [[0.9913007976279888, 0.00869920237201111], [0... [[0.9913007976279888, 0.00869920237201111], [0... brier_loss 0.095722 0.097785 0.096882 0.096771 0.09971 0.097374 0.097374 roc [0.829466586917514, 0.8294665869175141] [0.811457778261106, 0.811457778261106] [0.82251101082051, 0.82251101082051] [0.8232026534718069, 0.8232026534718069] [0.7994488817198706, 0.7994488817198706] [0.8172173822381614, 0.8172173822381614] [0.816942573130776, 0.8169425731307758] precision [0.8674337168584292, 0.0] [0.8675, 0.0] [0.8675, 0.0] [0.8678017025538307, 0.3333333333333333] [0.867, 0.0] [0.8674470838824521, 0.06666666666666667] [0.8674469787915166, 0.25] recall [0.9994236311239193, 0.0] [1.0, 0.0] [1.0, 0.0] [0.9988472622478386, 0.0037735849056603774] [1.0, 0.0] [0.9996541786743517, 0.0007547169811320754] [0.999654138805626, 0.0007541478129713424] f1 [0.9287627209426889, 0.0] [0.92904953145917, 0.0] [0.92904953145917, 0.0] [0.9287245444801714, 0.007462686567164179] [0.9287627209426887, 0.0] [0.9288698098567778, 0.0014925373134328358] [0.9288698446705945, 0.0015037593984962407] confusion_matrix [[1734, 1], [265, 0]] [[1735, 0], [265, 0]] [[1735, 0], [265, 0]] [[1733, 2], [264, 1]] [[1734, 0], [266, 0]] [[1734.2, 0.6], [265.0, 0.2]] [[8671, 3], [1325, 1]] DecisionTreeClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... brier_loss 0.194 0.188 0.2045 0.186 0.1895 0.1924 0.1924 roc [0.6164210755260726, 0.6164210755260725] [0.6198792887825567, 0.6198792887825567] [0.5656081779131097, 0.5656081779131097] [0.5938556902832908, 0.5938556902832907] [0.6090464049396849, 0.6090464049396849] [0.600962127488943, 0.600962127488943] [0.6009676462415547, 0.6009676462415547] precision [0.8992294013040901, 0.3035143769968051] [0.899941141848146, 0.31561461794019935] [0.8850174216027874, 0.24100719424460432] [0.8923431203223949, 0.2965779467680608] [0.8964306612053833, 0.30584192439862545] [0.8945923492565603, 0.292511212069659] [0.894552256254384, 0.29322268326417705] recall [0.8743515850144092, 0.3584905660377358] [0.8812680115273775, 0.3584905660377358] [0.878386167146974, 0.2528301886792453] [0.8933717579250721, 0.2943396226415094] [0.8835063437139562, 0.33458646616541354] [0.8821767730655579, 0.319747481912328] [0.8821766197832603, 0.31975867269984914] f1 [0.886616014026885, 0.328719723183391] [0.8905066977285965, 0.33568904593639576] [0.8816893260052068, 0.24677716390423574] [0.8928571428571428, 0.2954545454545454] [0.8899215800174267, 0.31956912028725315] [0.8883181521270516, 0.3052419197531642] [0.8883213373577897, 0.3059163059163059] confusion_matrix [[1517, 218], [170, 95]] [[1529, 206], [170, 95]] [[1524, 211], [198, 67]] [[1550, 185], [187, 78]] [[1532, 202], [177, 89]] [[1530.4, 204.4], [180.4, 84.8]] [[7652, 1022], [902, 424]] RandomForestClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[0.81, 0.19], [0.88, 0.12], [1.0, 0.0], [1.0,... [[1.0, 0.0], [1.0, 0.0], [0.64, 0.36], [0.97, ... [[0.98, 0.02], [0.49, 0.51], [0.99, 0.01], [0.... [[1.0, 0.0], [0.54, 0.46], [1.0, 0.0], [0.69, ... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... brier_loss 0.107495 0.106624 0.106167 0.103634 0.10682 0.106148 0.106148 roc [0.7979500842803545, 0.7979500842803545] [0.7956076341688869, 0.7956076341688869] [0.798345930074493, 0.798345930074493] [0.8149660159860801, 0.8149660159860802] [0.7988808526506578, 0.7988808526506577] [0.8011501034320945, 0.8011501034320944] [0.8012694444763238, 0.8012694444763238] precision [0.8798511430090378, 0.3277310924369748] [0.8757861635220126, 0.30434782608695654] [0.8771097046413502, 0.3076923076923077] [0.8760460251046025, 0.3181818181818182] [0.8754598003152917, 0.29896907216494845] [0.876850567318459, 0.31138442331260113] [0.8768421052631579, 0.312] recall [0.9538904899135446, 0.1471698113207547] [0.9631123919308358, 0.10566037735849057] [0.9585014409221903, 0.12075471698113208] [0.9654178674351584, 0.10566037735849057] [0.9607843137254902, 0.10902255639097744] [0.9603413007854439, 0.11765356788196908] [0.9603412497117824, 0.11764705882352941] f1 [0.9153761061946902, 0.203125] [0.9173757891847379, 0.1568627450980392] [0.9160011016248968, 0.1734417344173442] [0.9185632026323005, 0.15864022662889515] [0.9161396755567776, 0.15977961432506887] [0.9166911750386806, 0.1703698640938695] [0.9166941784967537, 0.17086527929901427] confusion_matrix [[1655, 80], [226, 39]] [[1671, 64], [237, 28]] [[1663, 72], [233, 32]] [[1675, 60], [237, 28]] [[1666, 68], [237, 29]] [[1666.0, 68.8], [234.0, 31.2]] [[8330, 344], [1170, 156]] def summarize_metrics ( model_dict : Dict , metric_name : str = \"roc\" , pos_label : int = 1 ): \"\"\" Summarize metrics of each fold with its standard error. We also plot a boxplot to show the results. \"\"\" results = [] for model_name , model_results in model_dict . items (): result_dict = model_results . get_result ( result_name = metric_name ) tmp_score = [] for fold , metric in result_dict . items (): pos_class_score = metric [ pos_label ] results . append (( model_name , fold , pos_class_score )) tmp_score . append ( pos_class_score ) # append the Standard Error of K folds results . append (( model_name , \"SE\" , np . std ( tmp_score , ddof = 1 ) / len ( tmp_score ) ** 0.5 )) summary_df = pd . DataFrame ( results , columns = [ \"model\" , \"fold\" , metric_name ]) fig , ax = plt . subplots ( figsize = ( 15 , 8 )) _ = sns . boxplot ( x = \"model\" , y = metric_name , data = summary_df [( summary_df [ 'model' ] != 'DummyClassifier' ) & ( summary_df [ 'fold' ] != 'SE' )], ax = ax ) # fig.savefig(config.spot_checking_boxplot, format='png', dpi=300) return summary_df summary_df = summarize_metrics ( model_dict = model_dict , metric_name = \"roc\" ) display ( summary_df . tail ( 12 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model fold roc 6 DecisionTreeClassifier fold 1 0.616421 7 DecisionTreeClassifier fold 2 0.619879 8 DecisionTreeClassifier fold 3 0.565608 9 DecisionTreeClassifier fold 4 0.593856 10 DecisionTreeClassifier fold 5 0.609046 11 DecisionTreeClassifier SE 0.009906 12 RandomForestClassifier fold 1 0.797950 13 RandomForestClassifier fold 2 0.795608 14 RandomForestClassifier fold 3 0.798346 15 RandomForestClassifier fold 4 0.814966 16 RandomForestClassifier fold 5 0.798881 17 RandomForestClassifier SE 0.003499 model_names = [ model for model in model_dict . keys ()] fig , ax = plt . subplots ( 1 , 3 , figsize = ( 10 , 10 )) for axes , algo in zip ( ax . ravel (), model_names ): cf_mat = results_df . oof_cv [ algo ] . confusion_matrix #### scores positive_class_auroc = results_df . oof_cv [ algo ] . roc [ 1 ] #### annotations labels = [ \"True Neg\" , \"False Pos\" , \"False Neg\" , \"True Pos\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in cf_mat . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in cf_mat . flatten () / np . sum ( cf_mat )] #### final annotations label = ( np . array ([ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )]) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = cf_mat , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = axes , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) axes . text ( 0 , - 0 , \" {} \" . format ( algo ), { \"size\" : 12 , \"color\" : \"black\" , \"weight\" : \"bold\" }) axes . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) axes . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( np . round ( positive_class_auroc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels axes . set_xticklabels ( \"\" ) axes . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Out Of Fold Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"This Visualization show the results of various classifiers and there respective results.\"\"\" , { \"size\" : 14 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) # fig.savefig(config.oof_confusion_matrix, format='png', dpi=300) Hyperparameter # create a feature preparation pipeline for a model def make_finetuning_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline pipeline_logistic = make_finetuning_pipeline ( linear_model . LogisticRegression ( random_state = config . seed , solver = 'liblinear' )) pipeline_decision_tree = make_finetuning_pipeline ( tree . DecisionTreeClassifier ( random_state = config . seed )) debug = True if debug : pipeline_random_forest = make_finetuning_pipeline ( ensemble . RandomForestClassifier ( n_estimators = 1 , random_state = config . seed )) else : pipeline_random_forest = make_finetuning_pipeline ( ensemble . RandomForestClassifier ( n_estimators = 100 , random_state = config . seed )) logistic_param_grid = dict ( model__C = np . logspace ( - 4 , 4 , 10 ), model__penalty = [ \"l1\" , \"l2\" ]) # https://stackoverflow.com/questions/46480457/difference-between-min-samples-split-and-min-samples-leaf-in-sklearn-decisiontre # min samples in a leaf should necessarily be odd to avoid tie in majority voting. # https://stackoverflow.com/questions/43963365/how-to-tune-sklearns-randomforest-max-depth-vs-min-samples-leaf dt_param_grid = dict ( model__criterion = [ \"gini\" , \"entropy\" ], model__max_depth = [ i for i in range ( 1 , 15 , 1 )], model__min_samples_leaf = [ 3 , 5 , 7 , 9 , 11 , 13 ], ) rf_param_grid = dict ( model__criterion = [ \"gini\" , \"entropy\" ], model__max_depth = [ i for i in range ( 1 , 15 , 1 )], model__min_samples_leaf = [ 3 , 5 , 7 , 9 , 11 , 13 ], ) grid_logistic = model_selection . GridSearchCV ( pipeline_logistic , param_grid = logistic_param_grid , cv = 5 , refit = True , verbose = 0 , scoring = \"roc_auc\" ) _ = grid_logistic . fit ( X_train . values , y_train . values ) grid_dt = model_selection . GridSearchCV ( pipeline_decision_tree , param_grid = dt_param_grid , cv = 5 , refit = True , verbose = 0 , scoring = \"roc_auc\" ) _ = grid_dt . fit ( X_train . values , y_train . values ) grid_rf = model_selection . GridSearchCV ( pipeline_random_forest , param_grid = rf_param_grid , cv = 5 , refit = True , verbose = 0 , scoring = \"roc_auc\" ) _ = grid_rf . fit ( X_train . values , y_train . values ) cv_results = pd . DataFrame ( grid_logistic . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in logistic_param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" ] cv_results = cv_results [ column_results ] def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C penalty mean_test_score std_test_score rank_test_score 8 0.359381 l1 0.817990 0.007830 1 10 2.782559 l1 0.817900 0.007826 2 12 21.544347 l1 0.817868 0.007823 3 9 0.359381 l2 0.817868 0.007848 4 11 2.782559 l2 0.817867 0.007828 5 14 166.810054 l1 0.817860 0.007818 6 17 1291.549665 l2 0.817859 0.007815 7 19 10000.0 l2 0.817859 0.007815 7 15 166.810054 l2 0.817858 0.007815 9 16 1291.549665 l1 0.817858 0.007817 10 18 10000.0 l1 0.817858 0.007817 10 13 21.544347 l2 0.817857 0.007817 12 6 0.046416 l1 0.817795 0.008225 13 7 0.046416 l2 0.817775 0.008023 14 5 0.005995 l2 0.816659 0.008399 15 3 0.000774 l2 0.807662 0.006027 16 4 0.005995 l1 0.795119 0.004219 17 1 0.0001 l2 0.792937 0.002506 18 2 0.000774 l1 0.500000 0.000000 19 0 0.0001 l1 0.500000 0.000000 19 cv_results = pd . DataFrame ( grid_dt . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in dt_param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" ] cv_results = cv_results [ column_results ] def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } criterion max_depth min_samples_leaf mean_test_score std_test_score rank_test_score 21 gini 4 9 0.818482 0.005350 1 22 gini 4 11 0.818390 0.005198 2 23 gini 4 13 0.818338 0.005103 3 20 gini 4 7 0.818105 0.005469 4 19 gini 4 5 0.817740 0.005811 5 ... ... ... ... ... ... ... 163 entropy 14 5 0.754388 0.017172 164 79 gini 14 5 0.750641 0.016970 165 162 entropy 14 3 0.748512 0.018148 166 72 gini 13 3 0.746442 0.020769 167 78 gini 14 3 0.736971 0.025007 168 168 rows \u00d7 6 columns cv_results = pd . DataFrame ( grid_rf . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in rf_param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" ] cv_results = cv_results [ column_results ] def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } criterion max_depth min_samples_leaf mean_test_score std_test_score rank_test_score 19 gini 4 5 0.809695 0.008776 1 106 entropy 4 11 0.809282 0.006761 2 107 entropy 4 13 0.809282 0.006761 2 23 gini 4 13 0.808963 0.009294 4 22 gini 4 11 0.808963 0.009294 4 ... ... ... ... ... ... ... 144 entropy 11 3 0.755646 0.011515 164 156 entropy 13 3 0.743200 0.030837 165 78 gini 14 3 0.742452 0.013079 166 162 entropy 14 3 0.740631 0.022496 167 72 gini 13 3 0.734308 0.009049 168 168 rows \u00d7 6 columns Export Tree Viz import graphviz dot_data = tree . export_graphviz ( grid_dt . best_estimator_ [ 'model' ], out_file = None , filled = True , rounded = True , feature_names = predictor_cols , class_names = [ '0' , '1' ], special_characters = True ) graph = graphviz . Source ( dot_data ) # graph.format = \"png\" # graph.render(\"health\") fig = plt . figure ( figsize = ( 50 , 35 ), dpi = 300 ) _ = tree . plot_tree ( grid_dt . best_estimator_ [ 'model' ], filled = True , rounded = True , feature_names = predictor_cols , class_names = [ '0' , '1' ]) fig . savefig ( \"dt_plot.png\" , dpi = 300 ) # plt.show() Feature Importance def get_feature_importance ( estimator : Callable , predictor_cols : List [ str ], has_feature_selection : bool = False ): \"\"\" If there is custom preprocessing feature selection step, like VIF, then we need to use different method to extract the features. predictor_cols: must be in sequence, how you pass in to train should preserve sequence. \"\"\" linear_models = [ 'LinearRegression' , 'LogisticRegression' ] tree_models = [ 'DecisionTreeClassifier' , 'RandomForestClassifier' ] estimator_name = estimator . __class__ . __name__ if has_feature_selection : pass else : if estimator_name in linear_models : coefficient = pd . DataFrame ( estimator . coef_ . flatten (), columns = [ 'Coefficients' ], index = predictor_cols ) coefficient . sort_values ( by = 'Coefficients' ) . plot ( kind = 'barh' , figsize = ( 9 , 7 )) plt . title ( f \" { estimator_name } \" ) plt . axvline ( x = 0 , color = '.5' ) plt . subplots_adjust ( left = .3 ) elif estimator_name in tree_models : coefficient = pd . DataFrame ( estimator . feature_importances_ , columns = [ 'Coefficients' ], index = predictor_cols ) coefficient . sort_values ( by = 'Coefficients' ) . plot ( kind = 'barh' , figsize = ( 9 , 7 )) plt . title ( f \" { estimator_name } \" ) plt . axvline ( x = 0 , color = '.5' ) plt . subplots_adjust ( left = .3 ) return coefficient predictor_cols = X_train . columns coef_logistic = get_feature_importance ( estimator = grid_logistic . best_estimator_ [ 'model' ], predictor_cols = predictor_cols ) coef_dt = get_feature_importance ( estimator = grid_dt . best_estimator_ [ 'model' ], predictor_cols = predictor_cols ) coef_rf = get_feature_importance ( estimator = grid_rf . best_estimator_ [ 'model' ], predictor_cols = predictor_cols ) Evaluate Performance on Test Set def plot_roc_curve ( fpr , tpr , label = None ): \"\"\" The ROC curve, modified from Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( 'ROC Curve' ) plt . plot ( fpr , tpr , linewidth = 2 , label = label ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . axis ([ - 0.005 , 1 , 0 , 1.005 ]) plt . xticks ( np . arange ( 0 , 1 , 0.05 ), rotation = 90 ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate (Recall)\" ) plt . legend ( loc = 'best' ) # plt.savefig(config.roc_plot, format=\"png\", dpi=300) y_test_gt = np . asarray ( y_test ) y_test_logistic_prob = grid_logistic . predict_proba ( X_test . values )[:, 1 ] roc_auc_logistic = metrics . roc_auc_score ( y_test_gt , y_test_logistic_prob ) print ( roc_auc_logistic ) 0.81786215276976 # generate the precision recall curve fpr , tpr , roc_thresholds = metrics . roc_curve ( y_test_gt , y_test_logistic_prob , pos_label = 1 ) plot_roc_curve ( fpr , tpr , label = \"ROC Curve on Test Set using Logistic\" ) Benefit Structure def adjusted_classes ( y_scores , t ): \"\"\" This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" return [ 1 if y >= t else 0 for y in y_scores ] def return_benefit_structure ( thresholds : List [ float ], y_true : np . ndarray , y_prob : np . ndarray ): columns = [ \"threshold\" , \"tp\" , \"fn\" , \"fp\" , \"tn\" , \"benefit_cost_old\" , \"benefit_cost_new\" ] benefit_cost_list = [] for t in threshold_list : y_pred_adj = adjusted_classes ( y_prob , t = t ) cm = metrics . confusion_matrix ( y_true = y_true , y_pred = y_pred_adj ) tn , fp , fn , tp = metrics . confusion_matrix ( y_true = y_true , y_pred = y_pred_adj ) . ravel () # this one check if it is correct formula benefit_cost_old = tp * 10 - fn * 10 - fp * 2 - ( tp + fp ) * 1 benefit_cost_new = tp * 100 - fn * 100 - fp * 2 - ( tp + fp ) * 1 benefit_cost_list . append ([ t , tn , fn , fp , tn , benefit_cost_old , benefit_cost_new ]) benefit_df = pd . DataFrame ( benefit_cost_list , columns = columns ) return benefit_df threshold_list : List [ float ] = [ 0.01 , 0.1 , 0.2 , 0.5 ] y_test_gt = np . asarray ( y_test ) y_test_logistic_prob = grid_logistic . predict_proba ( X_test . values )[:, 1 ] y_test_dt_prob = grid_dt . predict_proba ( X_test . values )[:, 1 ] y_test_rf_prob = grid_rf . predict_proba ( X_test . values )[:, 1 ] return_benefit_structure ( thresholds = threshold_list , y_true = y_test_gt , y_prob = y_test_logistic_prob ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } threshold tp fn fp tn benefit_cost_old benefit_cost_new 0 0.01 4622 19 4014 4622 -127 119213 1 0.10 4780 29 3856 4780 157 117697 2 0.20 5906 233 2730 5906 -341 80479 3 0.50 8636 1364 0 8636 -13640 -136400 return_benefit_structure ( thresholds = threshold_list , y_true = y_test_gt , y_prob = y_test_dt_prob ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } threshold tp fn fp tn benefit_cost_old benefit_cost_new 0 0.01 4412 16 4224 4412 -700 119180 1 0.10 5092 68 3544 5092 352 110872 2 0.20 6111 245 2525 6111 46 78706 3 0.50 8636 1364 0 8636 -13640 -136400 return_benefit_structure ( thresholds = threshold_list , y_true = y_test_gt , y_prob = y_test_rf_prob ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } threshold tp fn fp tn benefit_cost_old benefit_cost_new 0 0.01 3821 10 4815 3821 -2359 118601 1 0.10 5017 50 3619 5017 469 114229 2 0.20 5868 187 2768 5868 419 89519 3 0.50 8636 1364 0 8636 -13640 -136400 Bias-Variance Tradeoff # avg_expected_loss, avg_bias, avg_var = bias_variance_decomp( # grid.best_estimator_['model'], X_train.values, y_train.values, X_test.values, y_test.values, # loss='0-1_loss', # random_seed=123) # print('Average expected loss: %.3f' % avg_expected_loss) # print('Average bias: %.3f' % avg_bias) # print('Average variance: %.3f' % avg_var)","title":"DBA 3803 Project   Hongnan (1)"},{"location":"supervised_learning/regression/DBA%203803%20Project%20-%20Hongnan%20%281%29/#utils","text":"","title":"Utils"},{"location":"supervised_learning/regression/DBA%203803%20Project%20-%20Hongnan%20%281%29/#results-class","text":"This class is an abstract data structure that takes in default result names and store the results in columnwise format. This is abstract also because if you call model_results which is a dictionary, it returns {'DummyClassifier': <__main__.Results at 0x1fd24436ca0>} . default_result_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" , \"brier_loss\" , \"roc\" , \"precision\" , \"recall\" , \"f1\" , \"confusion_matrix\" , ] default_logit_names = [ \"y_true\" , \"y_pred\" , \"y_prob\" ] default_score_names = [ \"brier_loss\" , \"roc\" , \"precision\" , \"recall\" , \"f1\" , \"confusion_matrix\" ] class Results : \"\"\"Stores results for model training in columnwise format.\"\"\" _result_dict : Dict logit_names : List [ str ] score_names : List [ str ] def __init__ ( self , logit_names : List [ str ] = default_logit_names , score_names : List [ str ] = default_score_names , existing_dict : Optional [ Dict ] = None , ): \"\"\"Construct a new results store.\"\"\" self . logit_names = logit_names self . score_names = score_names if existing_dict is not None : self . _result_dict = copy . deepcopy ( existing_dict ) return dict_keys = [ \"identifier\" , * logit_names , * score_names ] self . _result_dict = { key : [] for key in dict_keys } def add ( self , identifier : str , results : Dict , in_place = False ): \"\"\"Add a new results row.\"\"\" if not in_place : return Results ( self . logit_names , self . score_names , self . _result_dict ) . add ( identifier , results , in_place = True ) self . _result_dict [ \"identifier\" ] . append ( identifier ) for result_name in set ([ * results . keys (), * self . logit_names , * self . score_names ]): result_value = results . get ( result_name , np . nan ) self . _result_dict [ result_name ] . append ( result_value ) return self def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } def get_result_values ( self , result_name : str ) -> List [ Any ]: \"\"\"Get a list of values for a result.\"\"\" return self . _result_dict [ result_name ] def to_dataframe ( self ) -> pd . DataFrame : \"\"\"Get a Data Frame containing the results.\"\"\" return pd . DataFrame . from_dict ( self . _result_dict ) def to_dict ( self ) -> Dict : \"\"\"Get a dictionary containing the results. Returns: Dict[str, List[Any]]: Dictionary of result columns \"\"\" return self . _result_dict","title":"Results Class"},{"location":"supervised_learning/regression/DBA%203803%20Project%20-%20Hongnan%20%281%29/#metrics","text":"def multiclass_label_binarize ( y , classes , pos_label = 1 , neg_label = 0 ): \"\"\"Binarize labels in one-vs-all fashion. Args: y (np.ndarray) Sequence of integer labels to encode classes (array-like) Labels for each class pos_label (int) Value for positive labels neg_label (int) Value for negative labels Returns: np.ndarray of shape (n_samples, n_classes) Encoded dataset \"\"\" columns = [ np . where ( y == label , pos_label , neg_label ) for label in classes ] return np . column_stack ( columns ) def multiclass_roc_auc_score ( y_true , y_score , classes = None ): \"\"\"Compute ROC-AUC score for each class in a multiclass dataset. Args: y_true (np.ndarray of shape (n_samples, n_classes)) True labels y_score (np.ndarray of shape (n_samples, n_classes)) Target scores classes (array-like of shape (n_classes,)) List of dataset classes. If `None`, the lexicographical order of the labels in `y_true` is used. Returns: array-like: ROC-AUC score for each class, in the same order as `classes` \"\"\" classes = ( np . unique ( y_true ) if classes is None else classes ) y_true_multiclass = multiclass_label_binarize ( y_true , classes = classes ) def oneclass_roc_auc_score ( class_id ): y_true_class = y_true_multiclass [:, class_id ] y_score_class = y_score [:, class_id ] fpr , tpr , _ = metrics . roc_curve ( y_true = y_true_class , y_score = y_score_class , pos_label = 1 ) return metrics . auc ( fpr , tpr ) return [ oneclass_roc_auc_score ( class_id ) for class_id in range ( len ( classes )) ] def compute_metrics ( logits : Dict [ str , np . ndarray ]) -> Dict [ str , Any ]: \"\"\"Compute metrics from logits.\"\"\" y_val , y_val_pred , y_val_prob = logits [ \"y_true\" ], logits [ \"y_pred\" ], logits [ \"y_prob\" ] #val_score = metrics.roc_auc_score( # y_true=y_val, # y_score=y_val_prob #) val_score = multiclass_roc_auc_score ( y_true = y_val , y_score = y_val_prob ) precision , recall , fbeta_score , _ = metrics . precision_recall_fscore_support ( y_true = y_val , y_pred = y_val_pred , labels = np . unique ( y_val ), average = None ) brier_loss = ( metrics . brier_score_loss ( y_true = y_val , y_prob = y_val_prob [:, 1 ] ) if config . classification_type == \"binary\" else np . nan ) confusion_matrix = metrics . confusion_matrix ( y_val , y_val_pred ) return { \"roc\" : val_score , \"precision\" : precision , \"recall\" : recall , \"f1\" : fbeta_score , \"brier_loss\" : brier_loss , \"confusion_matrix\" : confusion_matrix } def prepare_y ( y ): return ( y . ravel () if config . classification_type == \"binary\" else y ) def mean_score ( score_values ) -> Union [ float , np . ndarray ]: \"\"\"Compute the mean score.\"\"\" score_values = np . array ( score_values ) shape = score_values . shape if len ( shape ) == 1 : return score_values . mean () return score_values . mean ( axis = 0 ) def mean_cv_results ( model_results : Results ) -> Dict : \"\"\"Add mean cross-validation results. This method computes the mean value for all score types in the model_results, including for scores (e.g., confusion matrix) where the mean value may contain decimal places. \"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result )) for y_result in model_results . logit_names } cv_scores = { score : mean_score ( model_results . get_result_values ( score ) ) for score in model_results . score_names } return { ** cv_logits , ** cv_scores , } def oof_cv_results ( model_results : Results ) -> Dict : \"\"\"Add OOF cross-validation results.\"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result ) ) for y_result in model_results . logit_names } cv_scores = compute_metrics ( cv_logits ) return { ** cv_logits , ** cv_scores , } def add_cv_results ( model_results : Results ): \"\"\"Add cross-validation results. This method returns a copy of the given model results with summary columns for mean and CV cross-validation. \"\"\" mean_cv = mean_cv_results ( model_results ) oof_cv = oof_cv_results ( model_results ) return ( model_results . add ( \"mean_cv\" , mean_cv ) . add ( \"oof_cv\" , oof_cv ) ) def train_on_fold ( df_folds : pd . DataFrame , models : List [ Callable ], num_folds : int , predictor_col : List , target_col : List , ) -> Dict [ str , List ]: \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results. Args: df_folds (pd.DataFrame): [description] model (Callable): [description] num_folds (int): [description] predictor_col (List): [description] target_col (List): [description] Returns: Dict[str, List]: [description] \"\"\" y_true = df_folds [ target_col ] . values . flatten () # test_pred_arr: np.ndarray = np.zeros(len(X_test)) model_dict = {} for model in models : model_results = Results () if isinstance ( model , pipeline . Pipeline ): model_name = model [ \"model\" ] . __class__ . __name__ else : model_name = model . __class__ . __name__ # out-of-fold validation predictions oof_pred_arr : np . ndarray = np . zeros ( len ( df_folds )) for fold in range ( 1 , num_folds + 1 ): train_df = df_folds [ df_folds [ \"fold\" ] != fold ] . reset_index ( drop = True ) val_df = df_folds [ df_folds [ \"fold\" ] == fold ] . reset_index ( drop = True ) val_idx = df_folds [ df_folds [ \"fold\" ] == fold ] . index . values X_train , y_train = train_df [ predictor_col ] . values , prepare_y ( train_df [ target_col ] . values ) X_val , y_val = val_df [ predictor_col ] . values , prepare_y ( val_df [ target_col ] . values ) model . fit ( X_train , y_train ) y_val_pred = model . predict ( X_val ) y_val_prob = model . predict_proba ( X_val ) logits = { \"y_true\" : y_val , \"y_pred\" : y_val_pred , \"y_prob\" : y_val_prob , } metrics = compute_metrics ( logits ) model_results . add ( f \"fold { fold } \" , { ** logits , ** metrics }, in_place = True ) if model_name not in model_dict : model_dict [ model_name ] = model_results return model_dict model_dict = train_on_fold ( df_folds , models = classifiers , num_folds = 5 , predictor_col = predictor_cols , target_col = config . target ) C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) model_dict_with_summary = { model : add_cv_results ( model_results ) for model , model_results in model_dict . items () } results_df = pd . concat ({ name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () }, axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv LogisticRegression identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[0.9913007976279888, 0.00869920237201111], [0... [[0.6945486216835197, 0.30545137831648034], [0... [[0.9958858145499713, 0.004114185450028691], [... [[0.8859877852400798, 0.11401221475992017], [0... [[0.9977204955737388, 0.0022795044262612914], ... [[0.9913007976279888, 0.00869920237201111], [0... [[0.9913007976279888, 0.00869920237201111], [0... brier_loss 0.095722 0.097785 0.096882 0.096771 0.09971 0.097374 0.097374 roc [0.829466586917514, 0.8294665869175141] [0.811457778261106, 0.811457778261106] [0.82251101082051, 0.82251101082051] [0.8232026534718069, 0.8232026534718069] [0.7994488817198706, 0.7994488817198706] [0.8172173822381614, 0.8172173822381614] [0.816942573130776, 0.8169425731307758] precision [0.8674337168584292, 0.0] [0.8675, 0.0] [0.8675, 0.0] [0.8678017025538307, 0.3333333333333333] [0.867, 0.0] [0.8674470838824521, 0.06666666666666667] [0.8674469787915166, 0.25] recall [0.9994236311239193, 0.0] [1.0, 0.0] [1.0, 0.0] [0.9988472622478386, 0.0037735849056603774] [1.0, 0.0] [0.9996541786743517, 0.0007547169811320754] [0.999654138805626, 0.0007541478129713424] f1 [0.9287627209426889, 0.0] [0.92904953145917, 0.0] [0.92904953145917, 0.0] [0.9287245444801714, 0.007462686567164179] [0.9287627209426887, 0.0] [0.9288698098567778, 0.0014925373134328358] [0.9288698446705945, 0.0015037593984962407] confusion_matrix [[1734, 1], [265, 0]] [[1735, 0], [265, 0]] [[1735, 0], [265, 0]] [[1733, 2], [264, 1]] [[1734, 0], [266, 0]] [[1734.2, 0.6], [265.0, 0.2]] [[8671, 3], [1325, 1]] DecisionTreeClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... brier_loss 0.194 0.188 0.2045 0.186 0.1895 0.1924 0.1924 roc [0.6164210755260726, 0.6164210755260725] [0.6198792887825567, 0.6198792887825567] [0.5656081779131097, 0.5656081779131097] [0.5938556902832908, 0.5938556902832907] [0.6090464049396849, 0.6090464049396849] [0.600962127488943, 0.600962127488943] [0.6009676462415547, 0.6009676462415547] precision [0.8992294013040901, 0.3035143769968051] [0.899941141848146, 0.31561461794019935] [0.8850174216027874, 0.24100719424460432] [0.8923431203223949, 0.2965779467680608] [0.8964306612053833, 0.30584192439862545] [0.8945923492565603, 0.292511212069659] [0.894552256254384, 0.29322268326417705] recall [0.8743515850144092, 0.3584905660377358] [0.8812680115273775, 0.3584905660377358] [0.878386167146974, 0.2528301886792453] [0.8933717579250721, 0.2943396226415094] [0.8835063437139562, 0.33458646616541354] [0.8821767730655579, 0.319747481912328] [0.8821766197832603, 0.31975867269984914] f1 [0.886616014026885, 0.328719723183391] [0.8905066977285965, 0.33568904593639576] [0.8816893260052068, 0.24677716390423574] [0.8928571428571428, 0.2954545454545454] [0.8899215800174267, 0.31956912028725315] [0.8883181521270516, 0.3052419197531642] [0.8883213373577897, 0.3059163059163059] confusion_matrix [[1517, 218], [170, 95]] [[1529, 206], [170, 95]] [[1524, 211], [198, 67]] [[1550, 185], [187, 78]] [[1532, 202], [177, 89]] [[1530.4, 204.4], [180.4, 84.8]] [[7652, 1022], [902, 424]] RandomForestClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[0.81, 0.19], [0.88, 0.12], [1.0, 0.0], [1.0,... [[1.0, 0.0], [1.0, 0.0], [0.64, 0.36], [0.97, ... [[0.98, 0.02], [0.49, 0.51], [0.99, 0.01], [0.... [[1.0, 0.0], [0.54, 0.46], [1.0, 0.0], [0.69, ... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... brier_loss 0.107495 0.106624 0.106167 0.103634 0.10682 0.106148 0.106148 roc [0.7979500842803545, 0.7979500842803545] [0.7956076341688869, 0.7956076341688869] [0.798345930074493, 0.798345930074493] [0.8149660159860801, 0.8149660159860802] [0.7988808526506578, 0.7988808526506577] [0.8011501034320945, 0.8011501034320944] [0.8012694444763238, 0.8012694444763238] precision [0.8798511430090378, 0.3277310924369748] [0.8757861635220126, 0.30434782608695654] [0.8771097046413502, 0.3076923076923077] [0.8760460251046025, 0.3181818181818182] [0.8754598003152917, 0.29896907216494845] [0.876850567318459, 0.31138442331260113] [0.8768421052631579, 0.312] recall [0.9538904899135446, 0.1471698113207547] [0.9631123919308358, 0.10566037735849057] [0.9585014409221903, 0.12075471698113208] [0.9654178674351584, 0.10566037735849057] [0.9607843137254902, 0.10902255639097744] [0.9603413007854439, 0.11765356788196908] [0.9603412497117824, 0.11764705882352941] f1 [0.9153761061946902, 0.203125] [0.9173757891847379, 0.1568627450980392] [0.9160011016248968, 0.1734417344173442] [0.9185632026323005, 0.15864022662889515] [0.9161396755567776, 0.15977961432506887] [0.9166911750386806, 0.1703698640938695] [0.9166941784967537, 0.17086527929901427] confusion_matrix [[1655, 80], [226, 39]] [[1671, 64], [237, 28]] [[1663, 72], [233, 32]] [[1675, 60], [237, 28]] [[1666, 68], [237, 29]] [[1666.0, 68.8], [234.0, 31.2]] [[8330, 344], [1170, 156]] def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } results_df = pd . concat ({ name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () }, axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv LogisticRegression identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[0.9913007976279888, 0.00869920237201111], [0... [[0.6945486216835197, 0.30545137831648034], [0... [[0.9958858145499713, 0.004114185450028691], [... [[0.8859877852400798, 0.11401221475992017], [0... [[0.9977204955737388, 0.0022795044262612914], ... [[0.9913007976279888, 0.00869920237201111], [0... [[0.9913007976279888, 0.00869920237201111], [0... brier_loss 0.095722 0.097785 0.096882 0.096771 0.09971 0.097374 0.097374 roc [0.829466586917514, 0.8294665869175141] [0.811457778261106, 0.811457778261106] [0.82251101082051, 0.82251101082051] [0.8232026534718069, 0.8232026534718069] [0.7994488817198706, 0.7994488817198706] [0.8172173822381614, 0.8172173822381614] [0.816942573130776, 0.8169425731307758] precision [0.8674337168584292, 0.0] [0.8675, 0.0] [0.8675, 0.0] [0.8678017025538307, 0.3333333333333333] [0.867, 0.0] [0.8674470838824521, 0.06666666666666667] [0.8674469787915166, 0.25] recall [0.9994236311239193, 0.0] [1.0, 0.0] [1.0, 0.0] [0.9988472622478386, 0.0037735849056603774] [1.0, 0.0] [0.9996541786743517, 0.0007547169811320754] [0.999654138805626, 0.0007541478129713424] f1 [0.9287627209426889, 0.0] [0.92904953145917, 0.0] [0.92904953145917, 0.0] [0.9287245444801714, 0.007462686567164179] [0.9287627209426887, 0.0] [0.9288698098567778, 0.0014925373134328358] [0.9288698446705945, 0.0015037593984962407] confusion_matrix [[1734, 1], [265, 0]] [[1735, 0], [265, 0]] [[1735, 0], [265, 0]] [[1733, 2], [264, 1]] [[1734, 0], [266, 0]] [[1734.2, 0.6], [265.0, 0.2]] [[8671, 3], [1325, 1]] DecisionTreeClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0... [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0... brier_loss 0.194 0.188 0.2045 0.186 0.1895 0.1924 0.1924 roc [0.6164210755260726, 0.6164210755260725] [0.6198792887825567, 0.6198792887825567] [0.5656081779131097, 0.5656081779131097] [0.5938556902832908, 0.5938556902832907] [0.6090464049396849, 0.6090464049396849] [0.600962127488943, 0.600962127488943] [0.6009676462415547, 0.6009676462415547] precision [0.8992294013040901, 0.3035143769968051] [0.899941141848146, 0.31561461794019935] [0.8850174216027874, 0.24100719424460432] [0.8923431203223949, 0.2965779467680608] [0.8964306612053833, 0.30584192439862545] [0.8945923492565603, 0.292511212069659] [0.894552256254384, 0.29322268326417705] recall [0.8743515850144092, 0.3584905660377358] [0.8812680115273775, 0.3584905660377358] [0.878386167146974, 0.2528301886792453] [0.8933717579250721, 0.2943396226415094] [0.8835063437139562, 0.33458646616541354] [0.8821767730655579, 0.319747481912328] [0.8821766197832603, 0.31975867269984914] f1 [0.886616014026885, 0.328719723183391] [0.8905066977285965, 0.33568904593639576] [0.8816893260052068, 0.24677716390423574] [0.8928571428571428, 0.2954545454545454] [0.8899215800174267, 0.31956912028725315] [0.8883181521270516, 0.3052419197531642] [0.8883213373577897, 0.3059163059163059] confusion_matrix [[1517, 218], [170, 95]] [[1529, 206], [170, 95]] [[1524, 211], [198, 67]] [[1550, 185], [187, 78]] [[1532, 202], [177, 89]] [[1530.4, 204.4], [180.4, 84.8]] [[7652, 1022], [902, 424]] RandomForestClassifier identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ... [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ... y_pred [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... y_prob [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[0.81, 0.19], [0.88, 0.12], [1.0, 0.0], [1.0,... [[1.0, 0.0], [1.0, 0.0], [0.64, 0.36], [0.97, ... [[0.98, 0.02], [0.49, 0.51], [0.99, 0.01], [0.... [[1.0, 0.0], [0.54, 0.46], [1.0, 0.0], [0.69, ... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... [[1.0, 0.0], [0.32, 0.68], [0.53, 0.47], [0.9,... brier_loss 0.107495 0.106624 0.106167 0.103634 0.10682 0.106148 0.106148 roc [0.7979500842803545, 0.7979500842803545] [0.7956076341688869, 0.7956076341688869] [0.798345930074493, 0.798345930074493] [0.8149660159860801, 0.8149660159860802] [0.7988808526506578, 0.7988808526506577] [0.8011501034320945, 0.8011501034320944] [0.8012694444763238, 0.8012694444763238] precision [0.8798511430090378, 0.3277310924369748] [0.8757861635220126, 0.30434782608695654] [0.8771097046413502, 0.3076923076923077] [0.8760460251046025, 0.3181818181818182] [0.8754598003152917, 0.29896907216494845] [0.876850567318459, 0.31138442331260113] [0.8768421052631579, 0.312] recall [0.9538904899135446, 0.1471698113207547] [0.9631123919308358, 0.10566037735849057] [0.9585014409221903, 0.12075471698113208] [0.9654178674351584, 0.10566037735849057] [0.9607843137254902, 0.10902255639097744] [0.9603413007854439, 0.11765356788196908] [0.9603412497117824, 0.11764705882352941] f1 [0.9153761061946902, 0.203125] [0.9173757891847379, 0.1568627450980392] [0.9160011016248968, 0.1734417344173442] [0.9185632026323005, 0.15864022662889515] [0.9161396755567776, 0.15977961432506887] [0.9166911750386806, 0.1703698640938695] [0.9166941784967537, 0.17086527929901427] confusion_matrix [[1655, 80], [226, 39]] [[1671, 64], [237, 28]] [[1663, 72], [233, 32]] [[1675, 60], [237, 28]] [[1666, 68], [237, 29]] [[1666.0, 68.8], [234.0, 31.2]] [[8330, 344], [1170, 156]] def summarize_metrics ( model_dict : Dict , metric_name : str = \"roc\" , pos_label : int = 1 ): \"\"\" Summarize metrics of each fold with its standard error. We also plot a boxplot to show the results. \"\"\" results = [] for model_name , model_results in model_dict . items (): result_dict = model_results . get_result ( result_name = metric_name ) tmp_score = [] for fold , metric in result_dict . items (): pos_class_score = metric [ pos_label ] results . append (( model_name , fold , pos_class_score )) tmp_score . append ( pos_class_score ) # append the Standard Error of K folds results . append (( model_name , \"SE\" , np . std ( tmp_score , ddof = 1 ) / len ( tmp_score ) ** 0.5 )) summary_df = pd . DataFrame ( results , columns = [ \"model\" , \"fold\" , metric_name ]) fig , ax = plt . subplots ( figsize = ( 15 , 8 )) _ = sns . boxplot ( x = \"model\" , y = metric_name , data = summary_df [( summary_df [ 'model' ] != 'DummyClassifier' ) & ( summary_df [ 'fold' ] != 'SE' )], ax = ax ) # fig.savefig(config.spot_checking_boxplot, format='png', dpi=300) return summary_df summary_df = summarize_metrics ( model_dict = model_dict , metric_name = \"roc\" ) display ( summary_df . tail ( 12 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model fold roc 6 DecisionTreeClassifier fold 1 0.616421 7 DecisionTreeClassifier fold 2 0.619879 8 DecisionTreeClassifier fold 3 0.565608 9 DecisionTreeClassifier fold 4 0.593856 10 DecisionTreeClassifier fold 5 0.609046 11 DecisionTreeClassifier SE 0.009906 12 RandomForestClassifier fold 1 0.797950 13 RandomForestClassifier fold 2 0.795608 14 RandomForestClassifier fold 3 0.798346 15 RandomForestClassifier fold 4 0.814966 16 RandomForestClassifier fold 5 0.798881 17 RandomForestClassifier SE 0.003499 model_names = [ model for model in model_dict . keys ()] fig , ax = plt . subplots ( 1 , 3 , figsize = ( 10 , 10 )) for axes , algo in zip ( ax . ravel (), model_names ): cf_mat = results_df . oof_cv [ algo ] . confusion_matrix #### scores positive_class_auroc = results_df . oof_cv [ algo ] . roc [ 1 ] #### annotations labels = [ \"True Neg\" , \"False Pos\" , \"False Neg\" , \"True Pos\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in cf_mat . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in cf_mat . flatten () / np . sum ( cf_mat )] #### final annotations label = ( np . array ([ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )]) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = cf_mat , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = axes , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) axes . text ( 0 , - 0 , \" {} \" . format ( algo ), { \"size\" : 12 , \"color\" : \"black\" , \"weight\" : \"bold\" }) axes . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) axes . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( np . round ( positive_class_auroc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels axes . set_xticklabels ( \"\" ) axes . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Out Of Fold Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"This Visualization show the results of various classifiers and there respective results.\"\"\" , { \"size\" : 14 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) # fig.savefig(config.oof_confusion_matrix, format='png', dpi=300)","title":"Metrics"},{"location":"supervised_learning/regression/DBA%203803%20Project%20-%20Hongnan%20%281%29/#hyperparameter","text":"# create a feature preparation pipeline for a model def make_finetuning_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline pipeline_logistic = make_finetuning_pipeline ( linear_model . LogisticRegression ( random_state = config . seed , solver = 'liblinear' )) pipeline_decision_tree = make_finetuning_pipeline ( tree . DecisionTreeClassifier ( random_state = config . seed )) debug = True if debug : pipeline_random_forest = make_finetuning_pipeline ( ensemble . RandomForestClassifier ( n_estimators = 1 , random_state = config . seed )) else : pipeline_random_forest = make_finetuning_pipeline ( ensemble . RandomForestClassifier ( n_estimators = 100 , random_state = config . seed )) logistic_param_grid = dict ( model__C = np . logspace ( - 4 , 4 , 10 ), model__penalty = [ \"l1\" , \"l2\" ]) # https://stackoverflow.com/questions/46480457/difference-between-min-samples-split-and-min-samples-leaf-in-sklearn-decisiontre # min samples in a leaf should necessarily be odd to avoid tie in majority voting. # https://stackoverflow.com/questions/43963365/how-to-tune-sklearns-randomforest-max-depth-vs-min-samples-leaf dt_param_grid = dict ( model__criterion = [ \"gini\" , \"entropy\" ], model__max_depth = [ i for i in range ( 1 , 15 , 1 )], model__min_samples_leaf = [ 3 , 5 , 7 , 9 , 11 , 13 ], ) rf_param_grid = dict ( model__criterion = [ \"gini\" , \"entropy\" ], model__max_depth = [ i for i in range ( 1 , 15 , 1 )], model__min_samples_leaf = [ 3 , 5 , 7 , 9 , 11 , 13 ], ) grid_logistic = model_selection . GridSearchCV ( pipeline_logistic , param_grid = logistic_param_grid , cv = 5 , refit = True , verbose = 0 , scoring = \"roc_auc\" ) _ = grid_logistic . fit ( X_train . values , y_train . values ) grid_dt = model_selection . GridSearchCV ( pipeline_decision_tree , param_grid = dt_param_grid , cv = 5 , refit = True , verbose = 0 , scoring = \"roc_auc\" ) _ = grid_dt . fit ( X_train . values , y_train . values ) grid_rf = model_selection . GridSearchCV ( pipeline_random_forest , param_grid = rf_param_grid , cv = 5 , refit = True , verbose = 0 , scoring = \"roc_auc\" ) _ = grid_rf . fit ( X_train . values , y_train . values ) cv_results = pd . DataFrame ( grid_logistic . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in logistic_param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" ] cv_results = cv_results [ column_results ] def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C penalty mean_test_score std_test_score rank_test_score 8 0.359381 l1 0.817990 0.007830 1 10 2.782559 l1 0.817900 0.007826 2 12 21.544347 l1 0.817868 0.007823 3 9 0.359381 l2 0.817868 0.007848 4 11 2.782559 l2 0.817867 0.007828 5 14 166.810054 l1 0.817860 0.007818 6 17 1291.549665 l2 0.817859 0.007815 7 19 10000.0 l2 0.817859 0.007815 7 15 166.810054 l2 0.817858 0.007815 9 16 1291.549665 l1 0.817858 0.007817 10 18 10000.0 l1 0.817858 0.007817 10 13 21.544347 l2 0.817857 0.007817 12 6 0.046416 l1 0.817795 0.008225 13 7 0.046416 l2 0.817775 0.008023 14 5 0.005995 l2 0.816659 0.008399 15 3 0.000774 l2 0.807662 0.006027 16 4 0.005995 l1 0.795119 0.004219 17 1 0.0001 l2 0.792937 0.002506 18 2 0.000774 l1 0.500000 0.000000 19 0 0.0001 l1 0.500000 0.000000 19 cv_results = pd . DataFrame ( grid_dt . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in dt_param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" ] cv_results = cv_results [ column_results ] def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } criterion max_depth min_samples_leaf mean_test_score std_test_score rank_test_score 21 gini 4 9 0.818482 0.005350 1 22 gini 4 11 0.818390 0.005198 2 23 gini 4 13 0.818338 0.005103 3 20 gini 4 7 0.818105 0.005469 4 19 gini 4 5 0.817740 0.005811 5 ... ... ... ... ... ... ... 163 entropy 14 5 0.754388 0.017172 164 79 gini 14 5 0.750641 0.016970 165 162 entropy 14 3 0.748512 0.018148 166 72 gini 13 3 0.746442 0.020769 167 78 gini 14 3 0.736971 0.025007 168 168 rows \u00d7 6 columns cv_results = pd . DataFrame ( grid_rf . cv_results_ ) . sort_values ( \"mean_test_score\" , ascending = False ) # get the parameter names column_results = [ f \"param_ { name } \" for name in rf_param_grid . keys ()] column_results += [ \"mean_test_score\" , \"std_test_score\" , \"rank_test_score\" ] cv_results = cv_results [ column_results ] def shorten_param ( param_name ): if \"__\" in param_name : return param_name . rsplit ( \"__\" , 1 )[ 1 ] return param_name cv_results = cv_results . rename ( shorten_param , axis = 1 ) cv_results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } criterion max_depth min_samples_leaf mean_test_score std_test_score rank_test_score 19 gini 4 5 0.809695 0.008776 1 106 entropy 4 11 0.809282 0.006761 2 107 entropy 4 13 0.809282 0.006761 2 23 gini 4 13 0.808963 0.009294 4 22 gini 4 11 0.808963 0.009294 4 ... ... ... ... ... ... ... 144 entropy 11 3 0.755646 0.011515 164 156 entropy 13 3 0.743200 0.030837 165 78 gini 14 3 0.742452 0.013079 166 162 entropy 14 3 0.740631 0.022496 167 72 gini 13 3 0.734308 0.009049 168 168 rows \u00d7 6 columns","title":"Hyperparameter"},{"location":"supervised_learning/regression/DBA%203803%20Project%20-%20Hongnan%20%281%29/#export-tree-viz","text":"import graphviz dot_data = tree . export_graphviz ( grid_dt . best_estimator_ [ 'model' ], out_file = None , filled = True , rounded = True , feature_names = predictor_cols , class_names = [ '0' , '1' ], special_characters = True ) graph = graphviz . Source ( dot_data ) # graph.format = \"png\" # graph.render(\"health\") fig = plt . figure ( figsize = ( 50 , 35 ), dpi = 300 ) _ = tree . plot_tree ( grid_dt . best_estimator_ [ 'model' ], filled = True , rounded = True , feature_names = predictor_cols , class_names = [ '0' , '1' ]) fig . savefig ( \"dt_plot.png\" , dpi = 300 ) # plt.show()","title":"Export Tree Viz"},{"location":"supervised_learning/regression/DBA%203803%20Project%20-%20Hongnan%20%281%29/#feature-importance","text":"def get_feature_importance ( estimator : Callable , predictor_cols : List [ str ], has_feature_selection : bool = False ): \"\"\" If there is custom preprocessing feature selection step, like VIF, then we need to use different method to extract the features. predictor_cols: must be in sequence, how you pass in to train should preserve sequence. \"\"\" linear_models = [ 'LinearRegression' , 'LogisticRegression' ] tree_models = [ 'DecisionTreeClassifier' , 'RandomForestClassifier' ] estimator_name = estimator . __class__ . __name__ if has_feature_selection : pass else : if estimator_name in linear_models : coefficient = pd . DataFrame ( estimator . coef_ . flatten (), columns = [ 'Coefficients' ], index = predictor_cols ) coefficient . sort_values ( by = 'Coefficients' ) . plot ( kind = 'barh' , figsize = ( 9 , 7 )) plt . title ( f \" { estimator_name } \" ) plt . axvline ( x = 0 , color = '.5' ) plt . subplots_adjust ( left = .3 ) elif estimator_name in tree_models : coefficient = pd . DataFrame ( estimator . feature_importances_ , columns = [ 'Coefficients' ], index = predictor_cols ) coefficient . sort_values ( by = 'Coefficients' ) . plot ( kind = 'barh' , figsize = ( 9 , 7 )) plt . title ( f \" { estimator_name } \" ) plt . axvline ( x = 0 , color = '.5' ) plt . subplots_adjust ( left = .3 ) return coefficient predictor_cols = X_train . columns coef_logistic = get_feature_importance ( estimator = grid_logistic . best_estimator_ [ 'model' ], predictor_cols = predictor_cols ) coef_dt = get_feature_importance ( estimator = grid_dt . best_estimator_ [ 'model' ], predictor_cols = predictor_cols ) coef_rf = get_feature_importance ( estimator = grid_rf . best_estimator_ [ 'model' ], predictor_cols = predictor_cols )","title":"Feature Importance"},{"location":"supervised_learning/regression/DBA%203803%20Project%20-%20Hongnan%20%281%29/#evaluate-performance-on-test-set","text":"def plot_roc_curve ( fpr , tpr , label = None ): \"\"\" The ROC curve, modified from Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( 'ROC Curve' ) plt . plot ( fpr , tpr , linewidth = 2 , label = label ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . axis ([ - 0.005 , 1 , 0 , 1.005 ]) plt . xticks ( np . arange ( 0 , 1 , 0.05 ), rotation = 90 ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate (Recall)\" ) plt . legend ( loc = 'best' ) # plt.savefig(config.roc_plot, format=\"png\", dpi=300) y_test_gt = np . asarray ( y_test ) y_test_logistic_prob = grid_logistic . predict_proba ( X_test . values )[:, 1 ] roc_auc_logistic = metrics . roc_auc_score ( y_test_gt , y_test_logistic_prob ) print ( roc_auc_logistic ) 0.81786215276976 # generate the precision recall curve fpr , tpr , roc_thresholds = metrics . roc_curve ( y_test_gt , y_test_logistic_prob , pos_label = 1 ) plot_roc_curve ( fpr , tpr , label = \"ROC Curve on Test Set using Logistic\" )","title":"Evaluate Performance on Test Set"},{"location":"supervised_learning/regression/DBA%203803%20Project%20-%20Hongnan%20%281%29/#benefit-structure","text":"def adjusted_classes ( y_scores , t ): \"\"\" This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" return [ 1 if y >= t else 0 for y in y_scores ] def return_benefit_structure ( thresholds : List [ float ], y_true : np . ndarray , y_prob : np . ndarray ): columns = [ \"threshold\" , \"tp\" , \"fn\" , \"fp\" , \"tn\" , \"benefit_cost_old\" , \"benefit_cost_new\" ] benefit_cost_list = [] for t in threshold_list : y_pred_adj = adjusted_classes ( y_prob , t = t ) cm = metrics . confusion_matrix ( y_true = y_true , y_pred = y_pred_adj ) tn , fp , fn , tp = metrics . confusion_matrix ( y_true = y_true , y_pred = y_pred_adj ) . ravel () # this one check if it is correct formula benefit_cost_old = tp * 10 - fn * 10 - fp * 2 - ( tp + fp ) * 1 benefit_cost_new = tp * 100 - fn * 100 - fp * 2 - ( tp + fp ) * 1 benefit_cost_list . append ([ t , tn , fn , fp , tn , benefit_cost_old , benefit_cost_new ]) benefit_df = pd . DataFrame ( benefit_cost_list , columns = columns ) return benefit_df threshold_list : List [ float ] = [ 0.01 , 0.1 , 0.2 , 0.5 ] y_test_gt = np . asarray ( y_test ) y_test_logistic_prob = grid_logistic . predict_proba ( X_test . values )[:, 1 ] y_test_dt_prob = grid_dt . predict_proba ( X_test . values )[:, 1 ] y_test_rf_prob = grid_rf . predict_proba ( X_test . values )[:, 1 ] return_benefit_structure ( thresholds = threshold_list , y_true = y_test_gt , y_prob = y_test_logistic_prob ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } threshold tp fn fp tn benefit_cost_old benefit_cost_new 0 0.01 4622 19 4014 4622 -127 119213 1 0.10 4780 29 3856 4780 157 117697 2 0.20 5906 233 2730 5906 -341 80479 3 0.50 8636 1364 0 8636 -13640 -136400 return_benefit_structure ( thresholds = threshold_list , y_true = y_test_gt , y_prob = y_test_dt_prob ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } threshold tp fn fp tn benefit_cost_old benefit_cost_new 0 0.01 4412 16 4224 4412 -700 119180 1 0.10 5092 68 3544 5092 352 110872 2 0.20 6111 245 2525 6111 46 78706 3 0.50 8636 1364 0 8636 -13640 -136400 return_benefit_structure ( thresholds = threshold_list , y_true = y_test_gt , y_prob = y_test_rf_prob ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } threshold tp fn fp tn benefit_cost_old benefit_cost_new 0 0.01 3821 10 4815 3821 -2359 118601 1 0.10 5017 50 3619 5017 469 114229 2 0.20 5868 187 2768 5868 419 89519 3 0.50 8636 1364 0 8636 -13640 -136400","title":"Benefit Structure"},{"location":"supervised_learning/regression/DBA%203803%20Project%20-%20Hongnan%20%281%29/#bias-variance-tradeoff","text":"# avg_expected_loss, avg_bias, avg_var = bias_variance_decomp( # grid.best_estimator_['model'], X_train.values, y_train.values, X_test.values, y_test.values, # loss='0-1_loss', # random_seed=123) # print('Average expected loss: %.3f' % avg_expected_loss) # print('Average bias: %.3f' % avg_bias) # print('Average variance: %.3f' % avg_var)","title":"Bias-Variance Tradeoff"},{"location":"supervised_learning/regression/Untitled/","text":"Readings DIVE INTO DEEP LEARNING (Chapter 3) INTRODUCTION TO STATISTICAL LEARNING (Chapter 3) Simple Linear Regression Conditional Mean and Expectation Function to predict y Function of Residuals Function of Residual Sum of Squared Error and OLS Prediction Interpretation R-Squared Calculation of R-Squared Multiple Linear Regression (MLR) Assumptions of Linear Regression Linearity Homoscedasticity Normality of the Error Terms No Autocorrelation between Error Terms Multicollinearity among Predictors Notations and Matrix Representation of Linear Regression Break down of the Matrix Representation Optimal \\(\\beta\\) - Normal Equation Prediction Feature Scaling Hypothesis Testing on \\(\\beta\\) T-Statistics Time Complexity Preamble for the next series on Linear Regression Orthogonalization Regularization Statistical and Interpretation of Linear Regression Python Implementation References and Citations Our dataset is fairly simple, here is a brief overview of the first five rows of it. sqft bdrms age price 2104 3 70 399900 1600 3 28 329900 2400 3 44 369000 1416 2 49 232000 3000 4 75 539900 The columns are: sqft: the size of the house in sq. ft bdrms: number of bedrooms age: age in years of house price: the price of the house In the following sections, we will divide the price by \\(1000\\) , this is to pseudo-standardize the data. One note that we may bring forward in the section is that in Linear Regression, we generally don't need to standardize or center the predictors (see proof in Section: Feature Scaling). Simple Linear Regression We will first start off by constructing the Simple Linear Regression (SLR). \\[y = \\beta_0 + \\beta_1 x + \\epsilon\\] \\[\\text{price} = \\beta_0 + \\beta_1 \\text{sqft} + \\epsilon\\] where \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) the coefficient and \\(\\epsilon\\) is the error term. The error term can be thought of as the errors in the random universe, and makes up for the deviations in the model that cannot be prevented, we will not touch too much on the \\(\\epsilon\\) and will take it as the difference between the predicted and true values which are not being explained by the variables \\(x\\) in the SLR model. Without the intercept term the regression line would always have to pass through the origin, which is almost never an optimal way to represent the relationship between our target and predictor variable. This concept is closely linked to the reason why a model must always have a bias term. Bias: If there is no bias term in the model, you will lose the flexibility of your model. Imagine a simple linear regression model without a bias term, then your linear equation \\(y=mx\\) will only pass through the origin. Therefore, if your underlying data (pretend that we know that the underlying data's actual function \\(y = 3x + 5\\) ), then your Linear Regression model will never find the \"best fit line\" simply because we already assume that our prediction is governed by the slope \\(m\\) , and there does not exist our \\(c\\) , the intercept/bias term. Therefore, it is usually the case whereby we always add in an intercept term. We intend to estimate the values of \\(y\\) given \\(x\\) . Each value of \\(x\\) is multiplied by \\(\\beta_{1}\\) , with a constant intercept term \\(\\beta_{0}\\) . We end this section by knowing that 1 unit increase in \\(x\\) will correspond to a \\(\\beta_1\\) unit increase in \\(y\\) according to the model, while always remebering to add the intercept term. Conditional Mean and Expectation Something that most formal textbooks will mention is that a linear regression model is predicted via a conditional expectation \\[E(y|x)=\\beta_0 + \\beta_1 x + \\epsilon\\] In the probability model underlying linear regression, X and Y are random variables. if so, as an example, if Y = obesity and X = age, if we take the conditional expectation \\(E(Y|X=35)\\) meaning, whats the expected value of being obese if the individual is 35 across the sample, we just take the average(arithmetic mean) of y for those observations where X=35? But this means that each yi has in principle a different expected value : so the yi's here do not come from an identically distributed population. If they don't, then our sample {Y,X}, that contains as a random variable only the Y is not \"random\" (i.e. it is not i.i.d), due to the assumption that the X's are deterministic. So given a prediction vector y hat, this set of y hat (note not just single prediction) gives rise to the lowest L2 Loss, note again, this \"unique\" set of y hat will give rise to the lowest L2 loss. Now, this set of y hat is also the conditional mean of y given the training set X, technically, say our yhat[0] = 2.3, then this means, the corresponding X[0], say 1.5 (only 1 feature), is corresponding to this point 2.3, and this means on average, points residing with the input x = 1.5, will give an expectation value of 2.3. Now, if we go to yhat[1] = 3.3, with X[1] = 1.8, then the same logic applies. For more intuition, refer to the lecture/conditional_mean , note this is an often overlooked information and one should not neglect it. Function to predict y An simple formula that predicts the value of the house ( \\(\\hat{y}\\) ) given input variable square feet ( \\(x\\) ) is as follows: \\[\\hat{y} = \\beta_0 + \\beta_1 x\\] There are no more error terms \\(\\epsilon\\) and rightfully so, because if our model can know the unknown errors, it will be a perfect model, however, in reality, this SLR is just an estimation of \\(y\\) . Function of Residuals The definition of residuals is easy, it is simply the difference between the predicted \\(y\\) value and the actual \\(y\\) value. For more rigorous understanding, see the notes I made for myself. \\[\\text{Residuals}_{i} = y_i - \\hat{y}_i ~ \\forall i\\] Function of Residual Sum of Squared Error and OLS This is important to understand. Informally, we call this RSS, whereby it is a function \\(J(\\beta)\\) that we want to minimize on. We usually call \\(J(\\beta)\\) the loss function, as we want to minimize the loss. Slightly more formally, we can say that we want to find the optimal \\(\\beta\\) that gives rise to the minimum of \\(f = \\text{RSS}\\) . Mathematically, we express this as \\(\\text{argmin}_{\\beta \\in \\R}J(\\beta)\\) . We will touch on this later in Multiple Linear Regression, where a more rigorous form is being presented, and talking about its global minimum. In other words, we want to find the \\(\\beta_0\\) and \\(\\beta_1\\) such that, \\(J(\\beta)\\) is at a minimum. We choose a reasonable function \\(J(\\beta)\\) to be the Residual Sum of Squared Error. We will solve for \\(J(\\beta)\\) and get the best \\(\\beta\\) so that our predicted \\(\\hat{y}\\) will be as close to the ground truth \\(y\\) as possible. As of now, we denote our loss function \\(J(\\beta)\\) as follows: \\[J(\\beta)=\\sum(y-\\hat{y})^2=\\sum(y-\\beta_0-\\beta_1x)^2 = \\text{RSS}=\\text{SSR}\\] The reason we chose such a function is because of its convexity, and of course, that it is also the well known Ordinary Least Squares Estimator of \\(\\beta\\) . In addition, why the squared residuals instead of just the absolute value of the residuals? Well, both can be used \u2013 absolute value of residuals is often used when there are large outliers or other abnormalities in variables. Solving for the least absolute deviations (LAD) is a type of \"robust\" regression. In High School Calculus, we recall that to find a minimum of a function \\(J(\\beta)\\) , we take the derivative and set it to 0. We do the exact same thing here: \\[\\dfrac{dS}{d\\beta_1} = -2\\sum x(y-\\beta_0-\\beta_1x)\\\\ \\dfrac{dS}{d\\beta_0} = -2\\sum (y-\\beta_0-\\beta_1 x)\\] After setting both the equations to 0 and solving it, we note that \\(J(\\beta)\\) is a convex function, and therefore a minima is guaranteed. We present the optimal \\(\\beta\\) that minimizes the loss function \\(J(\\beta)\\) : \\[\\hat{\\beta}_1 = \\dfrac{\\sum(x-\\bar{x})(y-\\bar{y})}{(x-\\bar{x})^2} = r_{XY} \\frac{s_Y}{s_X}\\] \\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] where \\(\\bar{y}\\) : the sample mean of observed values \\(Y\\) \\(\\bar{x}\\) : the sample mean of observed values \\(X\\) \\(s_Y\\) : the sample standard deviation of observed values \\(Y\\) \\(s_X\\) : the sample standard deviation of observed values \\(X\\) \\(r_{XY}\\) : the sample Pearson correlation coefficient between observed \\(X\\) and \\(Y\\) Note that I will not continue to put a hat notation on top of the parameters \\(\\beta\\) , but keep in mind that the \\(\\beta\\) we found are not the population parameter, instead it is the sample parameter. In statistics, we often call \\(\\beta\\) as the true population parameter , but in reality, we do not have knowledge of what the underlying parameter is, and therefore we minimize the loss function to find the best estimate for the true population parameter \\(\\beta\\) , we denote it as \\(\\hat{\\beta}\\) and call it a statistics . We end this section off with a note that this method is called the Ordinary Least Squares (OLS). Prediction Since we have the formula to calculate \\(\\beta_0, \\beta_1\\) , we can use python to do the dirty work for us. For the full code, please refer to appendix . \\[\\hat{y} = 71+0.135x\\] Hereby attached is also a nice plot visualization. Some explanation is as follows: The graph below is 3 graphs stacked together - the blue dots represent \\((x_i, y_i)\\) where it represents a scatter plot of the original values of x and its ground truth y, one can observe that the scatter plot of the original dataset vaguely describes a linear relationship; the red dots represent \\((x_i, \\hat{y}_i)\\) represents a scatter plot of the x and the predicted values \\(\\hat{y}\\) ; last but not least, we draw the best fit line across. Scatter plot Interpretation One thing that is worth highlighting is I did a simple standardization of the \\(y\\) value through a division of 1000. Therefore, our predicted SLR model says that given a constant intercept of \\(71 \\times 1000 = 71000\\) , we expect that every unit increase of square feet brings about an increase of price of \\(0.135 \\times 1000 = 135\\) dollars. In other words, if you were to purchase a house which is \\(100\\) square feet more than your current house, be ready to fork out an additional \\(13500\\) bucks. (Hmm, kinda cheap though \ud83d\ude02, my country Singapore has way higher housing price than this \ud83d\ude10) We also can calculate the loss function, or preferably the Residual Sum of Squares (SSR), to be \\(193464\\) . Note that this is the lowest number that this model can get, although it seems high, but mathematically, there does not exist a number smaller than the aforementioned, solely because we already minimized the loss function to its global minimum. R-Squared We can't leave SLR without discussing the most notable metrics to access the model's performance called R-Squared . Although in all seriousness, it may no longer be the \"best\" metric due to the following two reasons from Reference from Minitab Every time you add a predictor to a model, the R-squared increases, even if due to chance alone. It never decreases. Consequently, a model with more terms may appear to have a better fit simply because it has more terms. If a model has too many predictors and higher order polynomials, it begins to model the random noise in the data. This condition is known as overfitting the model and it produces misleadingly high R-squared values and a lessened ability to make predictions. However, we are still going to go through the motion and discuss it. (Reference from Minitab.) R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. The definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model \u2192 R-squared = Explained variation / Total variation R-squared is always between 0 and 100% where 0% indicates that the model explains none of the variability of the response data around its mean. 100% indicates that the model explains all the variability of the response data around its mean. In general, the higher the R-squared, the better the model fits your data. But do remember that the more predictors you add to a LR model, the R-Squared will going to increase regardless. Calculation of R-Squared This is simple enough, the formula is given by: \\[R^2=1-\\frac{\\text{SSR}}{\\text{SST}}\\] where The total sum of squares is defined: \\[\\text{SST}= \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2\\] The residual sum of squares you are already familiar with. It is defined: \\[\\text{SSR} = \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2\\] With the use of python , \\(R^2=0.73\\) Multiple Linear Regression (MLR) Instead of using just one predictor to estimate a continuous target, we build a model with multiple predictor variables. You will be using MLR way more than SLR going forward. Remember the dataset on house pricing? We just now used the most obvious one to predict the price of a house , given the predictor variable , square feet. However, there are two other variables, number of bedrooms and age of the house . We can instead model these two variables alongside with square feet to predict the price of the house. In general, if these variables all play a crucial role in affecting the price of the house, then including these 3 variables will make the model more accurate. However, we have to take note of a few important assumptions of MLR (SLR included). We will mention it here. [Reference]: Linear Regression Assumptions by Jeff Macaluso from Microsoft Assumptions of Linear Regression Linearity This assumes that there is a linear relationship between the predictors (e.g. independent variables or features) and the response variable (e.g. dependent variable or label). This also assumes that the predictors are additive. Why it can happen: There may not just be a linear relationship among the data. Modeling is about trying to estimate a function that explains a process, and linear regression would not be a fitting estimator (pun intended) if there is no linear relationship. What it will affect: The predictions will be extremely inaccurate because our model is underfitting . This is a serious violation that should not be ignored. How to detect it: If there is only one predictor, this is pretty easy to test with a scatter plot. Most cases aren\u2019t so simple, so we\u2019ll have to modify this by using a scatter plot to see our predicted values versus the actual values (in other words, view the residuals). Ideally, the points should lie on or around a diagonal line on the scatter plot. How to fix it: Either adding polynomial terms to some of the predictors or applying nonlinear transformations . If those do not work, try adding additional variables to help capture the relationship between the predictors and the label. Homoscedasticity This assumes homoscedasticity, which is the same variance within our error terms. Heteroscedasticity, the violation of homoscedasticity, occurs when we don\u2019t have an even variance across the error terms. Why it can happen: Our model may be giving too much weight to a subset of the data, particularly where the error variance was the largest. What it will affect: Significance tests for coefficients due to the standard errors being biased. Additionally, the confidence intervals will be either too wide or too narrow. How to detect it: Plot the residuals and see if the variance appears to be uniform. How to fix it: Heteroscedasticity (can you tell I like the scedasticity words?) can be solved either by using weighted least squares regression instead of the standard OLS or transforming either the dependent or highly skewed variables. Performing a log transformation on the dependent variable is not a bad place to start. Normality of the Error Terms More specifically, this assumes that the error terms of the model are normally distributed . Linear regressions other than Ordinary Least Squares (OLS) may also assume normality of the predictors or the label, but that is not the case here. Why it can happen: This can actually happen if either the predictors or the label are significantly non-normal. Other potential reasons could include the linearity assumption being violated or outliers affecting our model. What it will affect: A violation of this assumption could cause issues with either shrinking or inflating our confidence intervals. How to detect it: There are a variety of ways to do so, but we\u2019ll look at both a histogram and the p-value from the Anderson-Darling test for normality. How to fix it: It depends on the root cause, but there are a few options. Nonlinear transformations of the variables, excluding specific variables (such as long-tailed variables), or removing outliers may solve this problem. No Autocorrelation between Error Terms This assumes no autocorrelation of the error terms. Autocorrelation being present typically indicates that we are missing some information that should be captured by the model. Why it can happen: In a time series scenario, there could be information about the past that we aren\u2019t capturing. In a non-time series scenario, our model could be systematically biased by either under or over predicting in certain conditions. Lastly, this could be a result of a violation of the linearity assumption. What it will affect: This will impact our model estimates. How to detect it: We will perform a Durbin-Watson test to determine if either positive or negative correlation is present. Alternatively, you could create plots of residual autocorrelations. How to fix it: A simple fix of adding lag variables can fix this problem. Alternatively, interaction terms, additional variables, or additional transformations may fix this. Multicollinearity among Predictors This assumes that the predictors used in the regression are not correlated with each other. This won\u2019t render our model unusable if violated, but it will cause issues with the interpretability of the model. This is why in the previous section, we need to make sure that the 3 variables, square feet, number of bedrooms, age of house are not highly correlated with each other, else additive effects may happen. Why it can happen: A lot of data is just naturally correlated. For example, if trying to predict a house price with square footage, the number of bedrooms, and the number of bathrooms, we can expect to see correlation between those three variables because bedrooms and bathrooms make up a portion of square footage. What it will affect: Multicollinearity causes issues with the interpretation of the coefficients. Specifically, you can interpret a coefficient as \u201can increase of 1 in this predictor results in a change of (coefficient) in the response variable, holding all other predictors constant.\u201d This becomes problematic when multicollinearity is present because we can\u2019t hold correlated predictors constant. Additionally, it increases the standard error of the coefficients, which results in them potentially showing as statistically insignificant when they might actually be significant. How to detect it: There are a few ways, but we will use a heatmap of the correlation as a visual aid and examine the variance inflation factor (VIF) . How to fix it: This can be fixed by other removing predictors with a high variance inflation factor (VIF) or performing dimensionality reduction. Notations and Matrix Representation of Linear Regression [Reference to Stanford and Andrew Ng, both different notations] We first establish that our regression model is defined as \\[\\left| \\begin{array}{l} \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\varepsilon} \\\\ \\mathbf{\\varepsilon} \\sim N(0, \\sigma^2 \\mathbf{I}) \\end{array} \\right.\\] where X is the Design Matrix: Let X be the design matrix of dimensions m \u2005\u00d7\u2005( n \u2005+\u20051) where m is the number of observations (training samples) and n independent feature/input variables. \\[\\mathbf{X} = \\begin{bmatrix} 1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\ 1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\end{bmatrix}_{m \\times (n+1)} = \\begin{bmatrix} (\\mathbf{x^{(1)}})^{T} \\\\ (\\mathbf{x^{(2)}})^{T} \\\\ \\vdots \\\\ (\\mathbf{x^{(m)}})^{T}\\end{bmatrix}\\] The ith column of X is defined as \\(x^{(i)}\\) , which is also known as the i th training sample, represented as a n \u2005\u00d7\u20051 vector. \\[\\mathbf{x^{(i)}} = \\begin{bmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\\\ \\vdots \\\\ x_n^{(i)} \\end{bmatrix}_{n \\times 1}\\] where \\(x^{(i)}_j\\) is the value of feature j in the i th training instance. y the output vector: The column vector y contains the output for the m observations. \\[\\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}_{m \\times 1}\\] \u03b2 the vector of coefficients/parameters: The column vector \u03b2 contains all the coefficients of the linear model. \\[\\mathbf{\\beta}=\\begin{bmatrix} \\beta_ 1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_n\\end{bmatrix}_{n \\times 1}\\] \u03b5 the random vector of the error terms: The column vector \u03b5 contains m error terms corresponding to the m observations. \\[\\mathbf{\\varepsilon} = \\begin{bmatrix} \\varepsilon^{(1)} \\\\ \\varepsilon^{(2)} \\\\ \\vdots \\\\ \\varepsilon^{(m)} \\end{bmatrix}_{m \\times 1}\\] As we move along, we will make slight modification to the variables above, to accommodate the intercept term as seen in the Design Matrix. On a side note, we present another way to represent the above vectors and matrix, the above is the Machine Learning way, while below is the more Statistical way. \\[\\mathbf{X} = \\begin{bmatrix} 1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\ 1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_{m,1} & x_{m,2} & \\cdots & x_{m,n} \\end{bmatrix}_{m \\times (n+1)}\\] Break down of the Matrix Representation Our dataset has 47 samples, we can generalize it further to a data set with m independent observations, $$(x^{(1)}, y^{(1)}),\u2006( x^{( 2)},\u2006 y^{ (2)),\u2006...,\u2006( x ( m ),\u2006 y ( m )) where x ( i ) is a m \u2005\u00d7\u20051 vector, and y ( i ) a scalar. A multivariate linear regression problem between an input variable \\(x^{(i)}\\) and output variable \\(y^{(i)}\\) can be represented as such: \\[y^{(i)}\u2004=\u2004\u03b2_0\u2005+\u2005\u03b2_1x_1^{(i)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(i)}\u2005+\u2005\u03b5^{(i)} \\text{where } \u03b5^{(i)}\\sim^{\\text{i.i.d}}N(0,\u2006\u03c3^2)\\] Since there exists m observations, we can write an equation for each observation: \\[y^{(1)}\u2004=\u2004\u03b2_0\u2005+\u2005\u03b2_1x_1^{(1)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(1)}\u2005+\u2005\u03b5^{(1)}\\\\ y^{(2)}\u2004=\u2004\u03b2_0\u2005+\u2005\u03b2_1x_1^{(2)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(2)}\u2005+\u2005\u03b5^{(2)}\\\\ \\vdots\\\\ y^{(m)}\u2004=\u2004\u03b2_0\u2005+\u2005\u03b2_1x_1^{(m)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(m)}\u2005+\u2005\u03b5^{(m)}\\\\\\] However, linear regression model usually have an intercept term, it is necessary to include a constant variable term \\(\\mathbf{x_{0}}\u2004=\u20041_{m\u2005\u00d7\u20051}\\) such that our linear regression can be expressed compactly in matrix algebra form. Adding the intercept term \\(x_0\\) , we have the following: \\[y^{(1)}\u2004=\u2004\u03b2_0x_0^{(1)}\u2005+\u2005\u03b2_1x_1^{(1)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(1)}\u2005+\u2005\u03b5^{(1)}\\\\ y^{(2)}\u2004=\u2004\u03b2_0\u2005x_0^{(2)}+\u2005\u03b2_1x_1^{(2)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(2)}\u2005+\u2005\u03b5^{(2)}\\\\ \\vdots\\\\ y^{(m)}\u2004=\u2004\u03b2_0x_0^{(m)}+\u2005\u03b2_1x_1^{(m)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(m)}\u2005+\u2005\u03b5^{(m)}\\\\\\] We transform the above system of linear equations into matrix form as follows: \\[ \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ y^{(3)} \\\\ \\vdots \\\\ \\mathbf{y}^{(m)} \\end{bmatrix}_{m \\times 1} = \\begin{bmatrix} 1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\ 1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\end{bmatrix}_{m \\times (n+1)} \\begin{bmatrix} \\beta_0 \\\\ \\beta_ 1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_n\\end{bmatrix}_{(n+1) \\times 1} + \\begin{bmatrix} \\varepsilon^{(1)} \\\\ \\varepsilon^{(2)} \\\\ \\varepsilon^{(3)} \\\\ \\vdots \\\\ \\varepsilon^{(m)} \\end{bmatrix}_{m \\times 1}\\] We then write the above system of linear equations more compactly as y \u2004=\u2004 X\u03b2 \u2005+\u2005 \u03b5 where \\(\u03b5\\sim^{\\text{i.i.d}}N(0,\u2006\u03c3^2)\\) recovering back the equation at the start. This is assumed to be an accurate reflection of the real world. The model has a systematic component X\u03b2 and a stochastic component \u03b5 . Our goal is then to obtain estimates of the population parameter \u03b2 . Optimal \\(\\beta\\) - Normal Equation Just as in SLR, we aim to minimize the loss function (note if you average across the samples, we also call them the cost function, but at this stage, we do not differentiate the two words). We introduce a new approach to solve the loss function analytically, called the Normal Equation. This method solves for us easily the optimal \\(\\beta\\) to be \\(\\beta = (X^TX)^{-1}X^Ty\\) . A brief derivation will be shown below. [Reference to Stanford] Slightly more formally, we can say that we want to find the optimal \\(\\beta\\) that gives rise to the minimum of \\(f = \\text{RSS}\\) . Mathematically, we express this as \\(\\text{argmin}_{\\beta \\in \\R}J(\\beta)\\) . Given the notations above, we compute the Sum of Squared Residuals (SSR) to be \\[J(\\beta)=\\sum_{i=1}^{m}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{m}(y_{i}-(\\beta_0+\\sum_{j=1}^{n}\\beta_{j}x_{i,j}))^2\\] To understand the above summation, it is paramount to extend the idea of SLR's SSR function to here, back then, we simply calculate the difference of the y_true and y_hat for each and every sample, square it, and sum all the residuals up for all the samples. We extend this idea to MLR and realize it is the same formula, just that now the representation of \\(y\\) and \\(\\hat{y}\\) are different, as shown above. With the assumption that \\(X\\) is a full rank (invertible) matrix, we can even further reduce the cost/loss function into matrix multiplication (see Normal Equation Derivation II) \\[J(\\beta)=\\dfrac{1}{2m}(X\\beta-y)^T(X\\beta-y)\\] We can differentiate the above cost function with respect to each \\(\\beta_i\\) and solve the optimal \\(\\beta\\) . We will not derive the equation here but instead give the result to be \\[\\beta = (X^TX)^{-1}X^Ty\\] Prediction Here is a snippet of how I calculated the optimal \\(\\beta\\) coefficients for our dataset, note that in this example, we did not divide the house price by \\(1000\\) as opposed to what we did for SLR. We show that this does not matter in the interpretation. For details, read the next section Feature Scaling, which also applies here. XtX = np . dot ( X . T , X ) XtX_inv = np . linalg . inv ( XtX ) XtX_inv_Xt = np . dot ( XtX_inv , X . T ) _optimal_betas = np . dot ( XtX_inv_Xt , y ) \\(y = 92451+139x_1-8621x_2-81x_3\\) where \\(x_1 = \\text{square feet}, x_2 = \\text{number of bed rooms}, x_3 = \\text{age of house}\\) The coefficient value signifies how much the mean of the dependent variable \\(y\\) changes given a one-unit shift in the independent variable \\(x\\) while holding other variables in the model constant. This property of holding the other variables constant is crucial because it allows you to assess the effect of each variable in isolation from the others. Thus, we see that \\(x_2\\) actually holds an inverse relationship with the price of the house, and rightfully so, the scale/range of the variable number of bedrooms \\(x_2\\) is only \\(1-5\\) . This can be confirmed by house.bdrms.value_counts(). One unit increase of \\(x_2\\) means one more bedroom, which signifies a decrease of \\(8621\\) in the price of the house. The rest of the variables are easily interpreted in the same way. Feature Scaling Reference In addition to the remarks in the other answers, I'd like to point out that the scale and location of the explanatory variables does not affect the validity of the regression model in any way. Consider the model \\(y=\u03b2_0+\u03b2_1x_1+\u03b2_2x_2+\u2026+\u03f5\\) The least squares estimators of \\(\u03b2_1,\u03b2_2,...\\) are not affected by shifting. The reason is that these are the slopes of the fitting surface - how much the surface changes if you change \\(x_1,x_2,...\\) one unit. This does not depend on location. (The estimator of \\(\u03b2_0\\) , however, does.) By looking at the equations for the estimators you can see that scaling \\(x_1\\) with a factor \\(a\\) scales \\(\\hat{\u03b2_1}\\) by a factor \\(\\frac{1}{a}\\) . To see this, note that \\[\\hat{\u03b2_1}(x_1)=\\dfrac{\\sum_{i=1}^{n}(x_{1,i}\u2212\\bar{x}_1)(y_i\u2212\\bar{y})}{\\sum_{i=1}^{n}(x_{1,i}\u2212\\bar{x}_1)^2}\\] Thus \\[\\hat{\u03b2}_1(ax_1)=\\dfrac{\u2211_{i=1}^{n}(ax_{1,i}\u2212a\\bar{x}_1)(y_i\u2212\\bar{y})}{\u2211_{i=1}^{n}(ax_{1,i}\u2212a\\bar{x}_1)^2}=\\dfrac{\\hat{\\beta}_1(x_1)}{a}\\] By looking at the corresponding formula for \\(\\hat{\u03b2}_2\\) (for instance) it is (hopefully) clear that this scaling doesn't affect the estimators of the other slopes. Thus, scaling simply corresponds to scaling the corresponding slopes. Because if we scale square feet ( \\(x_1\\) ) by a factor of \\(\\frac{1}{10}\\) , then if the original \\(\\hat{\\beta}_1\\) when square feet is 100, then the above proof shows that the new \\(\\hat{\\beta}_1\\) will be multiplied by 10, becoming 1000, therefore, the interpretation of the coefficients did not change. However, if you are using Gradient Descent (an optimization algorithm) in Regression, then centering, or scaling the variables, may prove to be faster for convergence. Hypothesis Testing on \\(\\beta\\) Recall that we are ultimately always interested in drawing conclusions about the population, not the particular sample we observed. This is an important sentence to understand, the reason we are testing our hypothesis on the population parameter instead of the estimated parameter is because we are interested in knowing our real population parameter, and we are using the estimated parameter to provide some statistical gauge. In the SLR setting, we are often interested in learning about the population intercept \\(\\beta_0\\) and the population slope \\(\u03b2_1\\) . As you know, confidence intervals and hypothesis tests are two related, but different, ways of learning about the values of population parameters. Here, we will learn how to calculate confidence intervals and conduct different hypothesis tests for both \\(\\beta_0\\) and \\(\\beta_1\\) .We turn our heads back to the SLR section, because when we ingest and digest concepts, it is important to start from baby steps first and generalize. As we can see above from both the fitted plot and the OLS coefficients, there does seem to be a linear relationship between the two. Furthermore, the OLS regression line's equation can be easily calculated and given by (note I have not divided the price unit by \\(1000\\) here): \\[\\hat{y} = 71000+135x\\] And so we know the estimated slope parameter \\(\\hat{\u03b2_1}\\) is \\(135\\) , and apparently there exhibits a \"relationship\" between \\(x\\) and \\(y\\) . Remember, if there is no relationship, then our optimal estimated parameter \\(\\hat{\\beta}_1\\) should be 0, as a coefficient of \\(0\\) means that \\(x\\) and \\(y\\) has no relationship (or at least in the linear form, the same however, cannot be said for non-linear models!). But be careful, although we can be certain that there is a relationship between house area and the sale price , but it is only limited to the \\(47\\) ****samples that we have! In fact, we want to know if there is a relationship between the population of all of the house area and its corresponding sale price in the whole population (country). It follows that we also want to ascertain that the true population slope \\(\u03b2_1\\) is unlikely to be 0 as well. Note that \\(0\\) is a common benchmark we use in linear regression, but it, in fact can be any number. This is why we have to draw inferences from \\(\\hat{\u03b2}_1\\) to make substantiate conclusion on the true population slope \\(\u03b2_1\\) . Let us formulate our question/hypothesis by asking the question: Do our house area and sale price exhibit a true linear relationship in our population? Can we make inferences of our true population parameters based on the estimated parameters (OLS estimates)? Thus, we can use the infamous scientific method Hypothesis Testing by defining our null hypothesis and alternate hypothesis as follows: Null Hypothesis \\(H_0\\) : \\(\u03b2_1=0\\) Alternative Hypothesis \\(H_1\\) : \\(\u03b2_1\\neq 0\\) Basically, the null hypothesis says that \\(\u03b2_1=0\\) , indicating that there is no relationship between \\(X\\) and \\(y\\) . Indeed, if \\(\u03b2_1=0\\) , our original model reduces to \\(y=\u03b2_0+\u03b5\\) , and this shows \\(X\\) does not depend on \\(y\\) at all. To test the null hypothesis , we instead need to determine whether \\(\\hat{\\beta}_1\\) , our OLS estimate for \\(\u03b2_1\\) , is sufficiently far from 0 so that we are confident that the real parameter \\(\u03b2_1\\) is non-zero. Note the distinction here that we emphasized that we are performing a hypothesis testing on the true population parameter but we depend on the value of the estimate of the true population parameter since we have no way to know the underlying true population parameter. T-Statistics In statistics, the t-statistic is the ratio of the difference of the estimated value of a true population parameter from its hypothesized value to its standard error . A good intuitive of explanation of t-statistics can be read here . Let \\(\\hat{\\mathbf{\\beta}}\\) be an estimator of \\(\\mathbf{\\beta}\\) in some statistical model. Then a t-statistic for this parameter \\(\\mathbf{\\beta}\\) is any quantity of the form \\[t_{\\hat{\\mathbf{\\beta}}} = \\dfrac{\\hat{\\mathbf{\\beta}} - \\mathbf{\\beta}_H}{\\text{SE}(\\hat{\\mathbf{\\beta}})}\\] where \\(\\mathbf{\\beta}_H\\) is the value we want to test in the hypothesis. By default, statistical software sets \\(\\mathbf{\\beta}_H = 0\\) . In the regression setting, we further take note that the t-statistic for each individual coefficient \\(\\hat{\\beta}_i\\) is given by \\[t_{\\hat{\\mathbf{\\beta}}i} = [t_{\\hat{\\mathbf{\\beta}}}]_{(i+1) \\times (i+1)}\\] If our null hypothesis is really true, that \\(\u03b2_1=0\\) , then if we calculate our t-value to be 0, then we can understand it as the number of standard deviations that \\(\\hat{\u03b2}_1\\) is 0, which means that \\(\\hat{\\beta}_1\\) is 0. This might be hard to reconcile at first, but if we see the formula of the t-statistics, and that by definition we set \\(\\beta_H=0\\) , then it is apparent that if \\(t_{\\hat{\\beta}}=0\\) , it forces the formula to become \\(t_{\\hat{\\beta}}=0=\\dfrac{\\hat{\\beta}-0}{\\text{SE}(\\hat{\\beta})} \\Longrightarrow \\hat{\\beta}=0\\) ; even more concretely with an example, we replace \\(\\beta_{H}\\) with our favorite true population parameter \\(\\beta_1\\) and \\(\\hat{\\beta}\\) with \\(\\hat{\\beta}_1\\) , then it just means that if \\(\\beta_1\\) were really \\(0\\) , i.e. no relationship of \\(y\\) and \\(x_1\\) , and if we also get \\(t_{\\hat{\\beta}_1}\\) to be 0 as well (To re-explain this part as a bit cumbersome). In which case we accept the null hypothesis; on the other hand, if our t-value is none-zero, it means that \\(\\hat{\\beta}_1\u22600\\) ) Consequently, we can conclude that greater the magnitude of \\(|t|\\) ( \\(t\\) can be either positive or negative), the greater the evidence to reject the null hypothesis. The closer \\(t\\) is to 0, the more likely there isn\u2019t a significant evidence to reject the null hypothesis. Time Complexity Time Complexity is an important topic, you do not want your code to run for 1 billion years, and therefore, an efficient code will be important to businesses. That is also why Time Complexity questions are becoming increasingly popular in Machine Learning and Data Science interviews! The Linear Algorithm that we used here simply uses matrix multiplication. We will also ignore the codes that are of constant time O(1). For example, self.coef_=None in the constructor is O(1) and we do not really wish to consider this in the grand scheme of things. What is the really important ones are in code lines 37\u201340. Given X to be a m by n matrix/array, where m is the number of samples and n the number of features. In addition, y is a m by 1 vector. Refer to this Wikipedia Page for a handy helpsheet on the various time complexity for mathematical operations. Line 37: np.dot(X.T,X) In the dot product, we transpose the m \u00d7 n matrix to become n \u00d7 m, this operation takes O(m \u00d7 n) time because we are effectively performing two for loops. Next up is performing matrix multiplication, note carefully that np.dot between two 2-d arrays does not mean dot product , instead they are matrix multiplication, which takes O(m \u00d7 n\u00b2) time. The output matrix of this step is n\u00d7 n. Line 38: Inverting a n \u00d7 n matrix takes n\u00b3 time. The output matrix is n \u00d7 n. Line 39: Now we perform matrix multiplication of n \u00d7 n and n \u00d7 m, which gives O(m \u00d7 n\u00b2), the output matrix is n \u00d7 m. Line 40: Lastly, the time complexity is O(m \u00d7 n). Adding them all up gives you O(2mn+2mn\u00b2+n\u00b3) whereby simple triangle inequality of mn<mn\u00b2 implies we can remove the less dominant 2mn term. In the end, the run time complexity of this Linear Regression Algorithm using Normal Equation is O(n\u00b2(m+n)). However, you noticed that there are two variables in the bigO notation, and you wonder if we can further reduce the bigO notation to a single variable? Well, if the number of variables is small, which means n is kept small and maybe constant, we can reduce the time complexity to O(m), however, if your variables are increasing, then your time complexity will explode if n \u2192 \u221e. This ends the first series, and also the first article published by me. Stay tuned for updates and see me code various Machine Learning Algorithms from scratch. Preamble for the next series on Linear Regression Just a heads up, I may not be doing part II of the series for Linear Regression just yet, as I want to cover a wide variety of algorithms on a surface level, just enough for beginners (or intermediate) learners. However, as a preamble, I will definitely include more and touch on the following topics that are not covered in today\u2019s session. Orthogonalization We can speed up the Normal Equation\u2019s time complexity by using a technique called Orthogonalization, whereby we make use of QR Factorization so we do not need to invert the annoying \\(X^TX\\) where it took n\u00b3 time! Regularization You basically cannot leave Linear Models without knowing L1\u20132 Regularization! The Ridge, Lasso, and the ElasticNet! Note that Regularization is a broad term that traverses through all Machine Learning Models. Stay tuned on understanding how Regularization can reduce overfitting. In addition, one caveat that I didn\u2019t mention is what if \\(X^TX\\) is not invertible in our Normal Equation? This can happen if some columns of X are linearly dependent (redundancy in our feature variables), or there are too many features whereby somehow\u2026 the number of training samples m is lesser than the number of features n. If you use say, Ridge Regression, then the Modified Normal Equation guarantees a solution. We will talk about it in Part II of Linear Regression. Statistical and Interpretation of Linear Regression I didn\u2019t mention much about how to interpret Linear Regression. This is important, even if you know how to code up a Linear Regression Algorithm from scratch, if you do not know how to interpret the results in a statistically rigorous way, then that is not meaningful! Learn more on Hypothesis Testing, Standard Errors, and Confidence Levels. I may delve a bit on Maximum Likelihood Estimators as well! Conclusion on what I learnt in this few days: Returning self by method chaining. Using Decorators in Python where I have to call raise xxx error multiple times throughout the classes, which is annoying. Reference from StackOverFlow . And Real Python . Python Implementation One Hundred Page ML Book, CS229, ML Glossary, GeeksforGeeks. Take input \\(X\\) and \\(y\\) \u2192 Use either closed form solution or Gradient Descent. And remember \\(y = X\\beta\\) , use this everywhere for vectorization. Gradient Descent Define Cost Function to be MSE = \\(\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2\\) In order to compute the gradient, we can vectorize it as such: \\(\\nabla\\) MSE = \\(-\\frac{2}{N}(y_{true} - y_{pred}) @ X\\) ; This is because y_true - y_pred gives you a 1xN vector, whereby X gives you a N x (n+1) vector. Multiplying them give us 1x(n+1) vector, which is the gradient vector of MSE, looks like \\([\\beta_0, \\beta_1, ..., \\beta_n]\\) . Note \\(\\sum_{i=1}^{N}\\) is omitted due to vectorizing. Note y_pred is calculated by X @ B Question: Verify by hand that the above gradient vector is true and derive it by calculus. References and Citations Statistics by Jim - Regression Interpreting MLR Coefficients and P-values Goodness of Fit and R-Squared T-Test Normal Equation (ML Wiki) Wholesome and Mathematically Rigorous (This is a must read) Ordinary Least Squares Wikipedia Linear Regression Assumptions by Jeff Macaluso from Microsoft Stanford's STATS203 class - Consider downloading them before it's gone Kaggle Linear Regression Assumptions Linear Regression Additive Effects (PSU STATS462) Hands on Linear Regression Real Python Linear Regression Normal Equation Derivation II Feature Scaling does not affect Linear Regressions' validity of Coefficients Hypothesis Testing on Optimal Coefficients Conditional Mean and Expectation of Linear Regression","title":"Untitled"},{"location":"supervised_learning/regression/Untitled/#readings","text":"DIVE INTO DEEP LEARNING (Chapter 3) INTRODUCTION TO STATISTICAL LEARNING (Chapter 3) Simple Linear Regression Conditional Mean and Expectation Function to predict y Function of Residuals Function of Residual Sum of Squared Error and OLS Prediction Interpretation R-Squared Calculation of R-Squared Multiple Linear Regression (MLR) Assumptions of Linear Regression Linearity Homoscedasticity Normality of the Error Terms No Autocorrelation between Error Terms Multicollinearity among Predictors Notations and Matrix Representation of Linear Regression Break down of the Matrix Representation Optimal \\(\\beta\\) - Normal Equation Prediction Feature Scaling Hypothesis Testing on \\(\\beta\\) T-Statistics Time Complexity Preamble for the next series on Linear Regression Orthogonalization Regularization Statistical and Interpretation of Linear Regression Python Implementation References and Citations Our dataset is fairly simple, here is a brief overview of the first five rows of it. sqft bdrms age price 2104 3 70 399900 1600 3 28 329900 2400 3 44 369000 1416 2 49 232000 3000 4 75 539900 The columns are: sqft: the size of the house in sq. ft bdrms: number of bedrooms age: age in years of house price: the price of the house In the following sections, we will divide the price by \\(1000\\) , this is to pseudo-standardize the data. One note that we may bring forward in the section is that in Linear Regression, we generally don't need to standardize or center the predictors (see proof in Section: Feature Scaling).","title":"Readings"},{"location":"supervised_learning/regression/Untitled/#simple-linear-regression","text":"We will first start off by constructing the Simple Linear Regression (SLR). \\[y = \\beta_0 + \\beta_1 x + \\epsilon\\] \\[\\text{price} = \\beta_0 + \\beta_1 \\text{sqft} + \\epsilon\\] where \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) the coefficient and \\(\\epsilon\\) is the error term. The error term can be thought of as the errors in the random universe, and makes up for the deviations in the model that cannot be prevented, we will not touch too much on the \\(\\epsilon\\) and will take it as the difference between the predicted and true values which are not being explained by the variables \\(x\\) in the SLR model. Without the intercept term the regression line would always have to pass through the origin, which is almost never an optimal way to represent the relationship between our target and predictor variable. This concept is closely linked to the reason why a model must always have a bias term. Bias: If there is no bias term in the model, you will lose the flexibility of your model. Imagine a simple linear regression model without a bias term, then your linear equation \\(y=mx\\) will only pass through the origin. Therefore, if your underlying data (pretend that we know that the underlying data's actual function \\(y = 3x + 5\\) ), then your Linear Regression model will never find the \"best fit line\" simply because we already assume that our prediction is governed by the slope \\(m\\) , and there does not exist our \\(c\\) , the intercept/bias term. Therefore, it is usually the case whereby we always add in an intercept term. We intend to estimate the values of \\(y\\) given \\(x\\) . Each value of \\(x\\) is multiplied by \\(\\beta_{1}\\) , with a constant intercept term \\(\\beta_{0}\\) . We end this section by knowing that 1 unit increase in \\(x\\) will correspond to a \\(\\beta_1\\) unit increase in \\(y\\) according to the model, while always remebering to add the intercept term.","title":"Simple Linear Regression"},{"location":"supervised_learning/regression/Untitled/#conditional-mean-and-expectation","text":"Something that most formal textbooks will mention is that a linear regression model is predicted via a conditional expectation \\[E(y|x)=\\beta_0 + \\beta_1 x + \\epsilon\\] In the probability model underlying linear regression, X and Y are random variables. if so, as an example, if Y = obesity and X = age, if we take the conditional expectation \\(E(Y|X=35)\\) meaning, whats the expected value of being obese if the individual is 35 across the sample, we just take the average(arithmetic mean) of y for those observations where X=35? But this means that each yi has in principle a different expected value : so the yi's here do not come from an identically distributed population. If they don't, then our sample {Y,X}, that contains as a random variable only the Y is not \"random\" (i.e. it is not i.i.d), due to the assumption that the X's are deterministic. So given a prediction vector y hat, this set of y hat (note not just single prediction) gives rise to the lowest L2 Loss, note again, this \"unique\" set of y hat will give rise to the lowest L2 loss. Now, this set of y hat is also the conditional mean of y given the training set X, technically, say our yhat[0] = 2.3, then this means, the corresponding X[0], say 1.5 (only 1 feature), is corresponding to this point 2.3, and this means on average, points residing with the input x = 1.5, will give an expectation value of 2.3. Now, if we go to yhat[1] = 3.3, with X[1] = 1.8, then the same logic applies. For more intuition, refer to the lecture/conditional_mean , note this is an often overlooked information and one should not neglect it.","title":"Conditional Mean and Expectation"},{"location":"supervised_learning/regression/Untitled/#function-to-predict-y","text":"An simple formula that predicts the value of the house ( \\(\\hat{y}\\) ) given input variable square feet ( \\(x\\) ) is as follows: \\[\\hat{y} = \\beta_0 + \\beta_1 x\\] There are no more error terms \\(\\epsilon\\) and rightfully so, because if our model can know the unknown errors, it will be a perfect model, however, in reality, this SLR is just an estimation of \\(y\\) .","title":"Function to predict y"},{"location":"supervised_learning/regression/Untitled/#function-of-residuals","text":"The definition of residuals is easy, it is simply the difference between the predicted \\(y\\) value and the actual \\(y\\) value. For more rigorous understanding, see the notes I made for myself. \\[\\text{Residuals}_{i} = y_i - \\hat{y}_i ~ \\forall i\\]","title":"Function of Residuals"},{"location":"supervised_learning/regression/Untitled/#function-of-residual-sum-of-squared-error-and-ols","text":"This is important to understand. Informally, we call this RSS, whereby it is a function \\(J(\\beta)\\) that we want to minimize on. We usually call \\(J(\\beta)\\) the loss function, as we want to minimize the loss. Slightly more formally, we can say that we want to find the optimal \\(\\beta\\) that gives rise to the minimum of \\(f = \\text{RSS}\\) . Mathematically, we express this as \\(\\text{argmin}_{\\beta \\in \\R}J(\\beta)\\) . We will touch on this later in Multiple Linear Regression, where a more rigorous form is being presented, and talking about its global minimum. In other words, we want to find the \\(\\beta_0\\) and \\(\\beta_1\\) such that, \\(J(\\beta)\\) is at a minimum. We choose a reasonable function \\(J(\\beta)\\) to be the Residual Sum of Squared Error. We will solve for \\(J(\\beta)\\) and get the best \\(\\beta\\) so that our predicted \\(\\hat{y}\\) will be as close to the ground truth \\(y\\) as possible. As of now, we denote our loss function \\(J(\\beta)\\) as follows: \\[J(\\beta)=\\sum(y-\\hat{y})^2=\\sum(y-\\beta_0-\\beta_1x)^2 = \\text{RSS}=\\text{SSR}\\] The reason we chose such a function is because of its convexity, and of course, that it is also the well known Ordinary Least Squares Estimator of \\(\\beta\\) . In addition, why the squared residuals instead of just the absolute value of the residuals? Well, both can be used \u2013 absolute value of residuals is often used when there are large outliers or other abnormalities in variables. Solving for the least absolute deviations (LAD) is a type of \"robust\" regression. In High School Calculus, we recall that to find a minimum of a function \\(J(\\beta)\\) , we take the derivative and set it to 0. We do the exact same thing here: \\[\\dfrac{dS}{d\\beta_1} = -2\\sum x(y-\\beta_0-\\beta_1x)\\\\ \\dfrac{dS}{d\\beta_0} = -2\\sum (y-\\beta_0-\\beta_1 x)\\] After setting both the equations to 0 and solving it, we note that \\(J(\\beta)\\) is a convex function, and therefore a minima is guaranteed. We present the optimal \\(\\beta\\) that minimizes the loss function \\(J(\\beta)\\) : \\[\\hat{\\beta}_1 = \\dfrac{\\sum(x-\\bar{x})(y-\\bar{y})}{(x-\\bar{x})^2} = r_{XY} \\frac{s_Y}{s_X}\\] \\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] where \\(\\bar{y}\\) : the sample mean of observed values \\(Y\\) \\(\\bar{x}\\) : the sample mean of observed values \\(X\\) \\(s_Y\\) : the sample standard deviation of observed values \\(Y\\) \\(s_X\\) : the sample standard deviation of observed values \\(X\\) \\(r_{XY}\\) : the sample Pearson correlation coefficient between observed \\(X\\) and \\(Y\\) Note that I will not continue to put a hat notation on top of the parameters \\(\\beta\\) , but keep in mind that the \\(\\beta\\) we found are not the population parameter, instead it is the sample parameter. In statistics, we often call \\(\\beta\\) as the true population parameter , but in reality, we do not have knowledge of what the underlying parameter is, and therefore we minimize the loss function to find the best estimate for the true population parameter \\(\\beta\\) , we denote it as \\(\\hat{\\beta}\\) and call it a statistics . We end this section off with a note that this method is called the Ordinary Least Squares (OLS).","title":"Function of Residual Sum of Squared Error and OLS"},{"location":"supervised_learning/regression/Untitled/#prediction","text":"Since we have the formula to calculate \\(\\beta_0, \\beta_1\\) , we can use python to do the dirty work for us. For the full code, please refer to appendix . \\[\\hat{y} = 71+0.135x\\] Hereby attached is also a nice plot visualization. Some explanation is as follows: The graph below is 3 graphs stacked together - the blue dots represent \\((x_i, y_i)\\) where it represents a scatter plot of the original values of x and its ground truth y, one can observe that the scatter plot of the original dataset vaguely describes a linear relationship; the red dots represent \\((x_i, \\hat{y}_i)\\) represents a scatter plot of the x and the predicted values \\(\\hat{y}\\) ; last but not least, we draw the best fit line across. Scatter plot","title":"Prediction"},{"location":"supervised_learning/regression/Untitled/#interpretation","text":"One thing that is worth highlighting is I did a simple standardization of the \\(y\\) value through a division of 1000. Therefore, our predicted SLR model says that given a constant intercept of \\(71 \\times 1000 = 71000\\) , we expect that every unit increase of square feet brings about an increase of price of \\(0.135 \\times 1000 = 135\\) dollars. In other words, if you were to purchase a house which is \\(100\\) square feet more than your current house, be ready to fork out an additional \\(13500\\) bucks. (Hmm, kinda cheap though \ud83d\ude02, my country Singapore has way higher housing price than this \ud83d\ude10) We also can calculate the loss function, or preferably the Residual Sum of Squares (SSR), to be \\(193464\\) . Note that this is the lowest number that this model can get, although it seems high, but mathematically, there does not exist a number smaller than the aforementioned, solely because we already minimized the loss function to its global minimum.","title":"Interpretation"},{"location":"supervised_learning/regression/Untitled/#r-squared","text":"We can't leave SLR without discussing the most notable metrics to access the model's performance called R-Squared . Although in all seriousness, it may no longer be the \"best\" metric due to the following two reasons from Reference from Minitab Every time you add a predictor to a model, the R-squared increases, even if due to chance alone. It never decreases. Consequently, a model with more terms may appear to have a better fit simply because it has more terms. If a model has too many predictors and higher order polynomials, it begins to model the random noise in the data. This condition is known as overfitting the model and it produces misleadingly high R-squared values and a lessened ability to make predictions. However, we are still going to go through the motion and discuss it. (Reference from Minitab.) R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. The definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model \u2192 R-squared = Explained variation / Total variation R-squared is always between 0 and 100% where 0% indicates that the model explains none of the variability of the response data around its mean. 100% indicates that the model explains all the variability of the response data around its mean. In general, the higher the R-squared, the better the model fits your data. But do remember that the more predictors you add to a LR model, the R-Squared will going to increase regardless.","title":"R-Squared"},{"location":"supervised_learning/regression/Untitled/#calculation-of-r-squared","text":"This is simple enough, the formula is given by: \\[R^2=1-\\frac{\\text{SSR}}{\\text{SST}}\\] where The total sum of squares is defined: \\[\\text{SST}= \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2\\] The residual sum of squares you are already familiar with. It is defined: \\[\\text{SSR} = \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2\\] With the use of python , \\(R^2=0.73\\)","title":"Calculation of R-Squared"},{"location":"supervised_learning/regression/Untitled/#multiple-linear-regression-mlr","text":"Instead of using just one predictor to estimate a continuous target, we build a model with multiple predictor variables. You will be using MLR way more than SLR going forward. Remember the dataset on house pricing? We just now used the most obvious one to predict the price of a house , given the predictor variable , square feet. However, there are two other variables, number of bedrooms and age of the house . We can instead model these two variables alongside with square feet to predict the price of the house. In general, if these variables all play a crucial role in affecting the price of the house, then including these 3 variables will make the model more accurate. However, we have to take note of a few important assumptions of MLR (SLR included). We will mention it here. [Reference]: Linear Regression Assumptions by Jeff Macaluso from Microsoft","title":"Multiple Linear Regression (MLR)"},{"location":"supervised_learning/regression/Untitled/#assumptions-of-linear-regression","text":"","title":"Assumptions of Linear Regression"},{"location":"supervised_learning/regression/Untitled/#linearity","text":"This assumes that there is a linear relationship between the predictors (e.g. independent variables or features) and the response variable (e.g. dependent variable or label). This also assumes that the predictors are additive. Why it can happen: There may not just be a linear relationship among the data. Modeling is about trying to estimate a function that explains a process, and linear regression would not be a fitting estimator (pun intended) if there is no linear relationship. What it will affect: The predictions will be extremely inaccurate because our model is underfitting . This is a serious violation that should not be ignored. How to detect it: If there is only one predictor, this is pretty easy to test with a scatter plot. Most cases aren\u2019t so simple, so we\u2019ll have to modify this by using a scatter plot to see our predicted values versus the actual values (in other words, view the residuals). Ideally, the points should lie on or around a diagonal line on the scatter plot. How to fix it: Either adding polynomial terms to some of the predictors or applying nonlinear transformations . If those do not work, try adding additional variables to help capture the relationship between the predictors and the label.","title":"Linearity"},{"location":"supervised_learning/regression/Untitled/#homoscedasticity","text":"This assumes homoscedasticity, which is the same variance within our error terms. Heteroscedasticity, the violation of homoscedasticity, occurs when we don\u2019t have an even variance across the error terms. Why it can happen: Our model may be giving too much weight to a subset of the data, particularly where the error variance was the largest. What it will affect: Significance tests for coefficients due to the standard errors being biased. Additionally, the confidence intervals will be either too wide or too narrow. How to detect it: Plot the residuals and see if the variance appears to be uniform. How to fix it: Heteroscedasticity (can you tell I like the scedasticity words?) can be solved either by using weighted least squares regression instead of the standard OLS or transforming either the dependent or highly skewed variables. Performing a log transformation on the dependent variable is not a bad place to start.","title":"Homoscedasticity"},{"location":"supervised_learning/regression/Untitled/#normality-of-the-error-terms","text":"More specifically, this assumes that the error terms of the model are normally distributed . Linear regressions other than Ordinary Least Squares (OLS) may also assume normality of the predictors or the label, but that is not the case here. Why it can happen: This can actually happen if either the predictors or the label are significantly non-normal. Other potential reasons could include the linearity assumption being violated or outliers affecting our model. What it will affect: A violation of this assumption could cause issues with either shrinking or inflating our confidence intervals. How to detect it: There are a variety of ways to do so, but we\u2019ll look at both a histogram and the p-value from the Anderson-Darling test for normality. How to fix it: It depends on the root cause, but there are a few options. Nonlinear transformations of the variables, excluding specific variables (such as long-tailed variables), or removing outliers may solve this problem.","title":"Normality of the Error Terms"},{"location":"supervised_learning/regression/Untitled/#no-autocorrelation-between-error-terms","text":"This assumes no autocorrelation of the error terms. Autocorrelation being present typically indicates that we are missing some information that should be captured by the model. Why it can happen: In a time series scenario, there could be information about the past that we aren\u2019t capturing. In a non-time series scenario, our model could be systematically biased by either under or over predicting in certain conditions. Lastly, this could be a result of a violation of the linearity assumption. What it will affect: This will impact our model estimates. How to detect it: We will perform a Durbin-Watson test to determine if either positive or negative correlation is present. Alternatively, you could create plots of residual autocorrelations. How to fix it: A simple fix of adding lag variables can fix this problem. Alternatively, interaction terms, additional variables, or additional transformations may fix this.","title":"No Autocorrelation between Error Terms"},{"location":"supervised_learning/regression/Untitled/#multicollinearity-among-predictors","text":"This assumes that the predictors used in the regression are not correlated with each other. This won\u2019t render our model unusable if violated, but it will cause issues with the interpretability of the model. This is why in the previous section, we need to make sure that the 3 variables, square feet, number of bedrooms, age of house are not highly correlated with each other, else additive effects may happen. Why it can happen: A lot of data is just naturally correlated. For example, if trying to predict a house price with square footage, the number of bedrooms, and the number of bathrooms, we can expect to see correlation between those three variables because bedrooms and bathrooms make up a portion of square footage. What it will affect: Multicollinearity causes issues with the interpretation of the coefficients. Specifically, you can interpret a coefficient as \u201can increase of 1 in this predictor results in a change of (coefficient) in the response variable, holding all other predictors constant.\u201d This becomes problematic when multicollinearity is present because we can\u2019t hold correlated predictors constant. Additionally, it increases the standard error of the coefficients, which results in them potentially showing as statistically insignificant when they might actually be significant. How to detect it: There are a few ways, but we will use a heatmap of the correlation as a visual aid and examine the variance inflation factor (VIF) . How to fix it: This can be fixed by other removing predictors with a high variance inflation factor (VIF) or performing dimensionality reduction.","title":"Multicollinearity among Predictors"},{"location":"supervised_learning/regression/Untitled/#notations-and-matrix-representation-of-linear-regression","text":"[Reference to Stanford and Andrew Ng, both different notations] We first establish that our regression model is defined as \\[\\left| \\begin{array}{l} \\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\varepsilon} \\\\ \\mathbf{\\varepsilon} \\sim N(0, \\sigma^2 \\mathbf{I}) \\end{array} \\right.\\] where X is the Design Matrix: Let X be the design matrix of dimensions m \u2005\u00d7\u2005( n \u2005+\u20051) where m is the number of observations (training samples) and n independent feature/input variables. \\[\\mathbf{X} = \\begin{bmatrix} 1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\ 1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\end{bmatrix}_{m \\times (n+1)} = \\begin{bmatrix} (\\mathbf{x^{(1)}})^{T} \\\\ (\\mathbf{x^{(2)}})^{T} \\\\ \\vdots \\\\ (\\mathbf{x^{(m)}})^{T}\\end{bmatrix}\\] The ith column of X is defined as \\(x^{(i)}\\) , which is also known as the i th training sample, represented as a n \u2005\u00d7\u20051 vector. \\[\\mathbf{x^{(i)}} = \\begin{bmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\\\ \\vdots \\\\ x_n^{(i)} \\end{bmatrix}_{n \\times 1}\\] where \\(x^{(i)}_j\\) is the value of feature j in the i th training instance. y the output vector: The column vector y contains the output for the m observations. \\[\\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}_{m \\times 1}\\] \u03b2 the vector of coefficients/parameters: The column vector \u03b2 contains all the coefficients of the linear model. \\[\\mathbf{\\beta}=\\begin{bmatrix} \\beta_ 1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_n\\end{bmatrix}_{n \\times 1}\\] \u03b5 the random vector of the error terms: The column vector \u03b5 contains m error terms corresponding to the m observations. \\[\\mathbf{\\varepsilon} = \\begin{bmatrix} \\varepsilon^{(1)} \\\\ \\varepsilon^{(2)} \\\\ \\vdots \\\\ \\varepsilon^{(m)} \\end{bmatrix}_{m \\times 1}\\] As we move along, we will make slight modification to the variables above, to accommodate the intercept term as seen in the Design Matrix. On a side note, we present another way to represent the above vectors and matrix, the above is the Machine Learning way, while below is the more Statistical way. \\[\\mathbf{X} = \\begin{bmatrix} 1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\ 1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_{m,1} & x_{m,2} & \\cdots & x_{m,n} \\end{bmatrix}_{m \\times (n+1)}\\]","title":"Notations and Matrix Representation of Linear Regression"},{"location":"supervised_learning/regression/Untitled/#break-down-of-the-matrix-representation","text":"Our dataset has 47 samples, we can generalize it further to a data set with m independent observations, $$(x^{(1)}, y^{(1)}),\u2006( x^{( 2)},\u2006 y^{ (2)),\u2006...,\u2006( x ( m ),\u2006 y ( m )) where x ( i ) is a m \u2005\u00d7\u20051 vector, and y ( i ) a scalar. A multivariate linear regression problem between an input variable \\(x^{(i)}\\) and output variable \\(y^{(i)}\\) can be represented as such: \\[y^{(i)}\u2004=\u2004\u03b2_0\u2005+\u2005\u03b2_1x_1^{(i)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(i)}\u2005+\u2005\u03b5^{(i)} \\text{where } \u03b5^{(i)}\\sim^{\\text{i.i.d}}N(0,\u2006\u03c3^2)\\] Since there exists m observations, we can write an equation for each observation: \\[y^{(1)}\u2004=\u2004\u03b2_0\u2005+\u2005\u03b2_1x_1^{(1)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(1)}\u2005+\u2005\u03b5^{(1)}\\\\ y^{(2)}\u2004=\u2004\u03b2_0\u2005+\u2005\u03b2_1x_1^{(2)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(2)}\u2005+\u2005\u03b5^{(2)}\\\\ \\vdots\\\\ y^{(m)}\u2004=\u2004\u03b2_0\u2005+\u2005\u03b2_1x_1^{(m)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(m)}\u2005+\u2005\u03b5^{(m)}\\\\\\] However, linear regression model usually have an intercept term, it is necessary to include a constant variable term \\(\\mathbf{x_{0}}\u2004=\u20041_{m\u2005\u00d7\u20051}\\) such that our linear regression can be expressed compactly in matrix algebra form. Adding the intercept term \\(x_0\\) , we have the following: \\[y^{(1)}\u2004=\u2004\u03b2_0x_0^{(1)}\u2005+\u2005\u03b2_1x_1^{(1)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(1)}\u2005+\u2005\u03b5^{(1)}\\\\ y^{(2)}\u2004=\u2004\u03b2_0\u2005x_0^{(2)}+\u2005\u03b2_1x_1^{(2)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(2)}\u2005+\u2005\u03b5^{(2)}\\\\ \\vdots\\\\ y^{(m)}\u2004=\u2004\u03b2_0x_0^{(m)}+\u2005\u03b2_1x_1^{(m)}\u2005+\u2005...\u2005+\u2005\u03b2_nx_n^{(m)}\u2005+\u2005\u03b5^{(m)}\\\\\\] We transform the above system of linear equations into matrix form as follows: \\[ \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ y^{(3)} \\\\ \\vdots \\\\ \\mathbf{y}^{(m)} \\end{bmatrix}_{m \\times 1} = \\begin{bmatrix} 1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\ 1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\end{bmatrix}_{m \\times (n+1)} \\begin{bmatrix} \\beta_0 \\\\ \\beta_ 1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_n\\end{bmatrix}_{(n+1) \\times 1} + \\begin{bmatrix} \\varepsilon^{(1)} \\\\ \\varepsilon^{(2)} \\\\ \\varepsilon^{(3)} \\\\ \\vdots \\\\ \\varepsilon^{(m)} \\end{bmatrix}_{m \\times 1}\\] We then write the above system of linear equations more compactly as y \u2004=\u2004 X\u03b2 \u2005+\u2005 \u03b5 where \\(\u03b5\\sim^{\\text{i.i.d}}N(0,\u2006\u03c3^2)\\) recovering back the equation at the start. This is assumed to be an accurate reflection of the real world. The model has a systematic component X\u03b2 and a stochastic component \u03b5 . Our goal is then to obtain estimates of the population parameter \u03b2 .","title":"Break down of the Matrix Representation"},{"location":"supervised_learning/regression/Untitled/#optimal-beta-normal-equation","text":"Just as in SLR, we aim to minimize the loss function (note if you average across the samples, we also call them the cost function, but at this stage, we do not differentiate the two words). We introduce a new approach to solve the loss function analytically, called the Normal Equation. This method solves for us easily the optimal \\(\\beta\\) to be \\(\\beta = (X^TX)^{-1}X^Ty\\) . A brief derivation will be shown below. [Reference to Stanford] Slightly more formally, we can say that we want to find the optimal \\(\\beta\\) that gives rise to the minimum of \\(f = \\text{RSS}\\) . Mathematically, we express this as \\(\\text{argmin}_{\\beta \\in \\R}J(\\beta)\\) . Given the notations above, we compute the Sum of Squared Residuals (SSR) to be \\[J(\\beta)=\\sum_{i=1}^{m}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{m}(y_{i}-(\\beta_0+\\sum_{j=1}^{n}\\beta_{j}x_{i,j}))^2\\] To understand the above summation, it is paramount to extend the idea of SLR's SSR function to here, back then, we simply calculate the difference of the y_true and y_hat for each and every sample, square it, and sum all the residuals up for all the samples. We extend this idea to MLR and realize it is the same formula, just that now the representation of \\(y\\) and \\(\\hat{y}\\) are different, as shown above. With the assumption that \\(X\\) is a full rank (invertible) matrix, we can even further reduce the cost/loss function into matrix multiplication (see Normal Equation Derivation II) \\[J(\\beta)=\\dfrac{1}{2m}(X\\beta-y)^T(X\\beta-y)\\] We can differentiate the above cost function with respect to each \\(\\beta_i\\) and solve the optimal \\(\\beta\\) . We will not derive the equation here but instead give the result to be \\[\\beta = (X^TX)^{-1}X^Ty\\]","title":"Optimal \\(\\beta\\) - Normal Equation"},{"location":"supervised_learning/regression/Untitled/#prediction_1","text":"Here is a snippet of how I calculated the optimal \\(\\beta\\) coefficients for our dataset, note that in this example, we did not divide the house price by \\(1000\\) as opposed to what we did for SLR. We show that this does not matter in the interpretation. For details, read the next section Feature Scaling, which also applies here. XtX = np . dot ( X . T , X ) XtX_inv = np . linalg . inv ( XtX ) XtX_inv_Xt = np . dot ( XtX_inv , X . T ) _optimal_betas = np . dot ( XtX_inv_Xt , y ) \\(y = 92451+139x_1-8621x_2-81x_3\\) where \\(x_1 = \\text{square feet}, x_2 = \\text{number of bed rooms}, x_3 = \\text{age of house}\\) The coefficient value signifies how much the mean of the dependent variable \\(y\\) changes given a one-unit shift in the independent variable \\(x\\) while holding other variables in the model constant. This property of holding the other variables constant is crucial because it allows you to assess the effect of each variable in isolation from the others. Thus, we see that \\(x_2\\) actually holds an inverse relationship with the price of the house, and rightfully so, the scale/range of the variable number of bedrooms \\(x_2\\) is only \\(1-5\\) . This can be confirmed by house.bdrms.value_counts(). One unit increase of \\(x_2\\) means one more bedroom, which signifies a decrease of \\(8621\\) in the price of the house. The rest of the variables are easily interpreted in the same way.","title":"Prediction"},{"location":"supervised_learning/regression/Untitled/#feature-scaling","text":"Reference In addition to the remarks in the other answers, I'd like to point out that the scale and location of the explanatory variables does not affect the validity of the regression model in any way. Consider the model \\(y=\u03b2_0+\u03b2_1x_1+\u03b2_2x_2+\u2026+\u03f5\\) The least squares estimators of \\(\u03b2_1,\u03b2_2,...\\) are not affected by shifting. The reason is that these are the slopes of the fitting surface - how much the surface changes if you change \\(x_1,x_2,...\\) one unit. This does not depend on location. (The estimator of \\(\u03b2_0\\) , however, does.) By looking at the equations for the estimators you can see that scaling \\(x_1\\) with a factor \\(a\\) scales \\(\\hat{\u03b2_1}\\) by a factor \\(\\frac{1}{a}\\) . To see this, note that \\[\\hat{\u03b2_1}(x_1)=\\dfrac{\\sum_{i=1}^{n}(x_{1,i}\u2212\\bar{x}_1)(y_i\u2212\\bar{y})}{\\sum_{i=1}^{n}(x_{1,i}\u2212\\bar{x}_1)^2}\\] Thus \\[\\hat{\u03b2}_1(ax_1)=\\dfrac{\u2211_{i=1}^{n}(ax_{1,i}\u2212a\\bar{x}_1)(y_i\u2212\\bar{y})}{\u2211_{i=1}^{n}(ax_{1,i}\u2212a\\bar{x}_1)^2}=\\dfrac{\\hat{\\beta}_1(x_1)}{a}\\] By looking at the corresponding formula for \\(\\hat{\u03b2}_2\\) (for instance) it is (hopefully) clear that this scaling doesn't affect the estimators of the other slopes. Thus, scaling simply corresponds to scaling the corresponding slopes. Because if we scale square feet ( \\(x_1\\) ) by a factor of \\(\\frac{1}{10}\\) , then if the original \\(\\hat{\\beta}_1\\) when square feet is 100, then the above proof shows that the new \\(\\hat{\\beta}_1\\) will be multiplied by 10, becoming 1000, therefore, the interpretation of the coefficients did not change. However, if you are using Gradient Descent (an optimization algorithm) in Regression, then centering, or scaling the variables, may prove to be faster for convergence.","title":"Feature Scaling"},{"location":"supervised_learning/regression/Untitled/#hypothesis-testing-on-beta","text":"Recall that we are ultimately always interested in drawing conclusions about the population, not the particular sample we observed. This is an important sentence to understand, the reason we are testing our hypothesis on the population parameter instead of the estimated parameter is because we are interested in knowing our real population parameter, and we are using the estimated parameter to provide some statistical gauge. In the SLR setting, we are often interested in learning about the population intercept \\(\\beta_0\\) and the population slope \\(\u03b2_1\\) . As you know, confidence intervals and hypothesis tests are two related, but different, ways of learning about the values of population parameters. Here, we will learn how to calculate confidence intervals and conduct different hypothesis tests for both \\(\\beta_0\\) and \\(\\beta_1\\) .We turn our heads back to the SLR section, because when we ingest and digest concepts, it is important to start from baby steps first and generalize. As we can see above from both the fitted plot and the OLS coefficients, there does seem to be a linear relationship between the two. Furthermore, the OLS regression line's equation can be easily calculated and given by (note I have not divided the price unit by \\(1000\\) here): \\[\\hat{y} = 71000+135x\\] And so we know the estimated slope parameter \\(\\hat{\u03b2_1}\\) is \\(135\\) , and apparently there exhibits a \"relationship\" between \\(x\\) and \\(y\\) . Remember, if there is no relationship, then our optimal estimated parameter \\(\\hat{\\beta}_1\\) should be 0, as a coefficient of \\(0\\) means that \\(x\\) and \\(y\\) has no relationship (or at least in the linear form, the same however, cannot be said for non-linear models!). But be careful, although we can be certain that there is a relationship between house area and the sale price , but it is only limited to the \\(47\\) ****samples that we have! In fact, we want to know if there is a relationship between the population of all of the house area and its corresponding sale price in the whole population (country). It follows that we also want to ascertain that the true population slope \\(\u03b2_1\\) is unlikely to be 0 as well. Note that \\(0\\) is a common benchmark we use in linear regression, but it, in fact can be any number. This is why we have to draw inferences from \\(\\hat{\u03b2}_1\\) to make substantiate conclusion on the true population slope \\(\u03b2_1\\) . Let us formulate our question/hypothesis by asking the question: Do our house area and sale price exhibit a true linear relationship in our population? Can we make inferences of our true population parameters based on the estimated parameters (OLS estimates)? Thus, we can use the infamous scientific method Hypothesis Testing by defining our null hypothesis and alternate hypothesis as follows: Null Hypothesis \\(H_0\\) : \\(\u03b2_1=0\\) Alternative Hypothesis \\(H_1\\) : \\(\u03b2_1\\neq 0\\) Basically, the null hypothesis says that \\(\u03b2_1=0\\) , indicating that there is no relationship between \\(X\\) and \\(y\\) . Indeed, if \\(\u03b2_1=0\\) , our original model reduces to \\(y=\u03b2_0+\u03b5\\) , and this shows \\(X\\) does not depend on \\(y\\) at all. To test the null hypothesis , we instead need to determine whether \\(\\hat{\\beta}_1\\) , our OLS estimate for \\(\u03b2_1\\) , is sufficiently far from 0 so that we are confident that the real parameter \\(\u03b2_1\\) is non-zero. Note the distinction here that we emphasized that we are performing a hypothesis testing on the true population parameter but we depend on the value of the estimate of the true population parameter since we have no way to know the underlying true population parameter.","title":"Hypothesis Testing on \\(\\beta\\)"},{"location":"supervised_learning/regression/Untitled/#t-statistics","text":"In statistics, the t-statistic is the ratio of the difference of the estimated value of a true population parameter from its hypothesized value to its standard error . A good intuitive of explanation of t-statistics can be read here . Let \\(\\hat{\\mathbf{\\beta}}\\) be an estimator of \\(\\mathbf{\\beta}\\) in some statistical model. Then a t-statistic for this parameter \\(\\mathbf{\\beta}\\) is any quantity of the form \\[t_{\\hat{\\mathbf{\\beta}}} = \\dfrac{\\hat{\\mathbf{\\beta}} - \\mathbf{\\beta}_H}{\\text{SE}(\\hat{\\mathbf{\\beta}})}\\] where \\(\\mathbf{\\beta}_H\\) is the value we want to test in the hypothesis. By default, statistical software sets \\(\\mathbf{\\beta}_H = 0\\) . In the regression setting, we further take note that the t-statistic for each individual coefficient \\(\\hat{\\beta}_i\\) is given by \\[t_{\\hat{\\mathbf{\\beta}}i} = [t_{\\hat{\\mathbf{\\beta}}}]_{(i+1) \\times (i+1)}\\] If our null hypothesis is really true, that \\(\u03b2_1=0\\) , then if we calculate our t-value to be 0, then we can understand it as the number of standard deviations that \\(\\hat{\u03b2}_1\\) is 0, which means that \\(\\hat{\\beta}_1\\) is 0. This might be hard to reconcile at first, but if we see the formula of the t-statistics, and that by definition we set \\(\\beta_H=0\\) , then it is apparent that if \\(t_{\\hat{\\beta}}=0\\) , it forces the formula to become \\(t_{\\hat{\\beta}}=0=\\dfrac{\\hat{\\beta}-0}{\\text{SE}(\\hat{\\beta})} \\Longrightarrow \\hat{\\beta}=0\\) ; even more concretely with an example, we replace \\(\\beta_{H}\\) with our favorite true population parameter \\(\\beta_1\\) and \\(\\hat{\\beta}\\) with \\(\\hat{\\beta}_1\\) , then it just means that if \\(\\beta_1\\) were really \\(0\\) , i.e. no relationship of \\(y\\) and \\(x_1\\) , and if we also get \\(t_{\\hat{\\beta}_1}\\) to be 0 as well (To re-explain this part as a bit cumbersome). In which case we accept the null hypothesis; on the other hand, if our t-value is none-zero, it means that \\(\\hat{\\beta}_1\u22600\\) ) Consequently, we can conclude that greater the magnitude of \\(|t|\\) ( \\(t\\) can be either positive or negative), the greater the evidence to reject the null hypothesis. The closer \\(t\\) is to 0, the more likely there isn\u2019t a significant evidence to reject the null hypothesis.","title":"T-Statistics"},{"location":"supervised_learning/regression/Untitled/#time-complexity","text":"Time Complexity is an important topic, you do not want your code to run for 1 billion years, and therefore, an efficient code will be important to businesses. That is also why Time Complexity questions are becoming increasingly popular in Machine Learning and Data Science interviews! The Linear Algorithm that we used here simply uses matrix multiplication. We will also ignore the codes that are of constant time O(1). For example, self.coef_=None in the constructor is O(1) and we do not really wish to consider this in the grand scheme of things. What is the really important ones are in code lines 37\u201340. Given X to be a m by n matrix/array, where m is the number of samples and n the number of features. In addition, y is a m by 1 vector. Refer to this Wikipedia Page for a handy helpsheet on the various time complexity for mathematical operations. Line 37: np.dot(X.T,X) In the dot product, we transpose the m \u00d7 n matrix to become n \u00d7 m, this operation takes O(m \u00d7 n) time because we are effectively performing two for loops. Next up is performing matrix multiplication, note carefully that np.dot between two 2-d arrays does not mean dot product , instead they are matrix multiplication, which takes O(m \u00d7 n\u00b2) time. The output matrix of this step is n\u00d7 n. Line 38: Inverting a n \u00d7 n matrix takes n\u00b3 time. The output matrix is n \u00d7 n. Line 39: Now we perform matrix multiplication of n \u00d7 n and n \u00d7 m, which gives O(m \u00d7 n\u00b2), the output matrix is n \u00d7 m. Line 40: Lastly, the time complexity is O(m \u00d7 n). Adding them all up gives you O(2mn+2mn\u00b2+n\u00b3) whereby simple triangle inequality of mn<mn\u00b2 implies we can remove the less dominant 2mn term. In the end, the run time complexity of this Linear Regression Algorithm using Normal Equation is O(n\u00b2(m+n)). However, you noticed that there are two variables in the bigO notation, and you wonder if we can further reduce the bigO notation to a single variable? Well, if the number of variables is small, which means n is kept small and maybe constant, we can reduce the time complexity to O(m), however, if your variables are increasing, then your time complexity will explode if n \u2192 \u221e. This ends the first series, and also the first article published by me. Stay tuned for updates and see me code various Machine Learning Algorithms from scratch.","title":"Time Complexity"},{"location":"supervised_learning/regression/Untitled/#preamble-for-the-next-series-on-linear-regression","text":"Just a heads up, I may not be doing part II of the series for Linear Regression just yet, as I want to cover a wide variety of algorithms on a surface level, just enough for beginners (or intermediate) learners. However, as a preamble, I will definitely include more and touch on the following topics that are not covered in today\u2019s session.","title":"Preamble for the next series on Linear Regression"},{"location":"supervised_learning/regression/Untitled/#orthogonalization","text":"We can speed up the Normal Equation\u2019s time complexity by using a technique called Orthogonalization, whereby we make use of QR Factorization so we do not need to invert the annoying \\(X^TX\\) where it took n\u00b3 time!","title":"Orthogonalization"},{"location":"supervised_learning/regression/Untitled/#regularization","text":"You basically cannot leave Linear Models without knowing L1\u20132 Regularization! The Ridge, Lasso, and the ElasticNet! Note that Regularization is a broad term that traverses through all Machine Learning Models. Stay tuned on understanding how Regularization can reduce overfitting. In addition, one caveat that I didn\u2019t mention is what if \\(X^TX\\) is not invertible in our Normal Equation? This can happen if some columns of X are linearly dependent (redundancy in our feature variables), or there are too many features whereby somehow\u2026 the number of training samples m is lesser than the number of features n. If you use say, Ridge Regression, then the Modified Normal Equation guarantees a solution. We will talk about it in Part II of Linear Regression.","title":"Regularization"},{"location":"supervised_learning/regression/Untitled/#statistical-and-interpretation-of-linear-regression","text":"I didn\u2019t mention much about how to interpret Linear Regression. This is important, even if you know how to code up a Linear Regression Algorithm from scratch, if you do not know how to interpret the results in a statistically rigorous way, then that is not meaningful! Learn more on Hypothesis Testing, Standard Errors, and Confidence Levels. I may delve a bit on Maximum Likelihood Estimators as well! Conclusion on what I learnt in this few days: Returning self by method chaining. Using Decorators in Python where I have to call raise xxx error multiple times throughout the classes, which is annoying. Reference from StackOverFlow . And Real Python .","title":"Statistical and Interpretation of Linear Regression"},{"location":"supervised_learning/regression/Untitled/#python-implementation","text":"One Hundred Page ML Book, CS229, ML Glossary, GeeksforGeeks. Take input \\(X\\) and \\(y\\) \u2192 Use either closed form solution or Gradient Descent. And remember \\(y = X\\beta\\) , use this everywhere for vectorization. Gradient Descent Define Cost Function to be MSE = \\(\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2\\) In order to compute the gradient, we can vectorize it as such: \\(\\nabla\\) MSE = \\(-\\frac{2}{N}(y_{true} - y_{pred}) @ X\\) ; This is because y_true - y_pred gives you a 1xN vector, whereby X gives you a N x (n+1) vector. Multiplying them give us 1x(n+1) vector, which is the gradient vector of MSE, looks like \\([\\beta_0, \\beta_1, ..., \\beta_n]\\) . Note \\(\\sum_{i=1}^{N}\\) is omitted due to vectorizing. Note y_pred is calculated by X @ B Question: Verify by hand that the above gradient vector is true and derive it by calculus.","title":"Python Implementation"},{"location":"supervised_learning/regression/Untitled/#references-and-citations","text":"Statistics by Jim - Regression Interpreting MLR Coefficients and P-values Goodness of Fit and R-Squared T-Test Normal Equation (ML Wiki) Wholesome and Mathematically Rigorous (This is a must read) Ordinary Least Squares Wikipedia Linear Regression Assumptions by Jeff Macaluso from Microsoft Stanford's STATS203 class - Consider downloading them before it's gone Kaggle Linear Regression Assumptions Linear Regression Additive Effects (PSU STATS462) Hands on Linear Regression Real Python Linear Regression Normal Equation Derivation II Feature Scaling does not affect Linear Regressions' validity of Coefficients Hypothesis Testing on Optimal Coefficients Conditional Mean and Expectation of Linear Regression","title":"References and Citations"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/","text":"Quick Navigation Dependencies and Configuration Stage 4: Modelling How EDA helped us? Modelling Spot Checking Algorithms Make Basic Pipeline (Say No to Data Leakage!) Define Metrics Comparison of Cross-Validated Models Out-of-Fold Confusion Matrix Hypothesis Testing Across Models Model Selection: Hyperparameter Tuning with GridSearchCV Retrain on the whole training set Retrain using Optimal Hyperparameters Interpretation of Results Interpretation of Coefficients Interpretation of Metric Scores on Train Set Evaluation on Test Set Bias-Variance Tradeoff Dependencies and Configuration import csv import random from functools import wraps from time import time from typing import Callable , Dict , List , Union , Optional , Any import matplotlib.pyplot as plt import copy import mlxtend import numpy as np import pandas as pd import seaborn as sns from mlxtend.evaluate import paired_ttest_5x2cv , bias_variance_decomp from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS #from statsmodels.stats.outliers_influence import variance_inflation_factor from dataclasses import dataclass import logging @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/regression/ %20ho use-sales-in-king-country-usa/data/raw/kc_house_data.csv\" train_size : float = 0.8 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"KFold\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # set logger logger = init_logger () # read data df = pd . read_csv ( config . raw_data ) How EDA helped us? Insights derived from EDA: To fill in. Cross-Validation Strategy Generalization: Ultimately, we are interested in the Generalization Error made by the model, that is, how well the model perform on unseen data that is not taken from our sample set $\\mathcal{D}$. In general, we use validation set for Model Selection and the test set for an estimate of generalization error on new data. - Refactored from Elements of Statistical Learning, Chapter 7.2 Step 1: Train-Test-Split: Since this dataset is relatively small, we will not use the train-validation-test split and only split into train and test in a ratio of 9:1, whereby the split is stratified on our target, using stratify=y parameter in train_test_split() to ensure that our target has equal representation in both train and test. We note that this is a relatively small dataset and in practice, we need a large sample size to get a reliable/stable split, it is also recommended to retrain the whole dataset (without the \"unseen\" test set) after we have done the model selection process (eg. finding best hyperparameters). Step 2: Resampling Stategy: Note that we will be performing StratifiedKFold as our resampling strategy. After our split in Step 1, we have a training set $X_{\\text{train}}$, we will then perform our resampling strategy on this $X_{\\text{train}}$. We will choose our choice of $K = 5$. The choice of $K$ is somewhat arbitrary, and is derived empirically . To recap, we have the following: Training Set ( \\(X_{\\text{train}}\\) ) : This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis \\(h \\in \\mathcal{H}\\) . Validation Set ( \\(X_{\\text{val}}\\) ) : This is split from our \\(X_{\\text{train}}\\) during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis \\(g \\in \\mathcal{H}\\) ). Test Set ( \\(X_{\\text{test}}\\) ) : This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model \\(g\\) , we will use \\(g\\) to predict on the test set to get an estimate of the generalization error (also called out-of-sample error). Courtesy of scikit-learn on a typical Cross-Validation workflow. predictor_cols = df . columns [ 3 :] . tolist () target_col = [ \"price\" ] logger . info ( f \" \\n The predictor columns are \\n { predictor_cols } \" ) 2021-11-10,12:09:44 - The predictor columns are ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'] X = df [ predictor_cols ] . copy () y = df [ target_col ] . copy () # Split train - test X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , y , train_size = config . train_size , shuffle = True , random_state = config . seed ) logger . info ( f \" \\n Shape of train: { X_train . shape } \\n Shape of test: { X_test . shape } \" ) 2021-11-10,12:09:44 - Shape of train: (17290, 18) Shape of test: (4323, 18) def make_folds ( df : pd . DataFrame , num_folds : int , cv_schema : str , seed : int , predictor_col : List , target_col : List , ) -> pd . DataFrame : \"\"\"Split the given dataframe into training folds. Args: df (pd.DataFrame): [description] num_folds (int): [description] cv_schema (str): [description] seed (int): [description] Returns: pd.DataFrame: [description] \"\"\" if cv_schema == \"KFold\" : df_folds = df . copy () kf = model_selection . KFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( kf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) elif cv_schema == \"StratifiedKFold\" : df_folds = df . copy () skf = model_selection . StratifiedKFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( skf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) print ( df_folds . groupby ([ \"fold\" , \"diagnosis\" ]) . size ()) return df_folds X_y_train = pd . concat ([ X_train , y_train ], axis = 1 ) . reset_index ( drop = True ) df_folds = make_folds ( X_y_train , num_folds = config . num_folds , cv_schema = config . cv_schema , seed = config . seed , predictor_col = predictor_cols , target_col = target_col ) Looks good! All our five folds are now in df_fold ! Modelling Spot Checking Algorithms Terminology Alert! This method is advocated by Jason Brownlee PhD and this serves as the first stage of my modelling process. We will rapidly test (spot check) different classifier algorithms, from DummyClassifier , to LinearModel to more sophisticated ensemble trees like RandomForest . I also note to the readers that we need to think of a few things when choosing the \"optimal\" machine learning algorithm: No Lunch Free Theorem intuitively says that no single optimization algorithm can work best in all situations. Therefore, spot checking can help us form a basis of which algorithm might work better in this particular scenario. Occam's Razor often appears in many Machine Learning textbook, and the narrative is that a simpler model more often times generalizes better than a complex model. This is not unfamiliar when we think of the bias-variance tradeoff, and that is why there is always a tradeoff that we must make. Make Basic Pipeline (Say No to Data Leakage!) Say No to Data Leakage: This has been emphasized throughout and we must be careful as we should never touch the test set when fitting the model. This means that preprocessing steps such as StandardScaling() should only be fitted on the training data, and then apply the same transformation (mean and std) on the test data. In other words, do not apply scaling on the whole dataset before splitting. However, it is also equally important to take note not to contaminate our validation set, which is often overlooked, resulting in over optimistic results from model selection phase, but perform badly on unseen test set. As a result, when we use a 5 fold cross validation, we should be careful during fitting that the preprocessing steps are only applied on the training folds, and not on all 5 folds. The same idea is also applied to our ReduceVIF() preprocessing step. We should also include this in our pipeline and not select the features outside the cross-validation loop. Scikit Learn's Pipeline object will prevent us from data leakage, as the steps in a pipeline is already pre-defined. There is also a lot of flexibility in this object, as you can even write custom functions in your pipeline! def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : print ( f \"Dropping { max_vif_col } with vif= { max_vif } \" ) column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names # create a feature preparation pipeline for a model def make_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # reduce VIF # steps.append((\"remove_multicollinearity\", ReduceVIF(thresh=10))) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline regressors = [ # baseline model dummy . DummyRegressor ( strategy = \"mean\" ), # linear model linear_model . LinearRegression ( fit_intercept = True ), linear_model . Ridge ( random_state = config . seed , alpha = 1 , fit_intercept = True ), linear_model . Lasso ( random_state = config . seed , alpha = 1 , fit_intercept = True , ), linear_model . ElasticNet ( random_state = config . seed , alpha = 1 , l1_ratio = 0.5 , fit_intercept = True , ), # tree tree . DecisionTreeRegressor ( random_state = config . seed , criterion = \"squared_error\" ), # ensemble # ensemble.RandomForestClassifier(random_state=config.seed), ] regressors = [ make_pipeline ( model ) for model in regressors ] Define Metrics default_result_names = [ \"y_true\" , \"y_pred\" , ] default_logit_names = [ \"y_true\" , \"y_pred\" , ] default_score_names = [ \"explained_variance_score\" , \"mean_squared_error\" , \"mean_absolute_error\" , \"root_mean_squared_error\" , \"r2_score\" , \"mean_absolute_percentage_error\" , ] custom_score_names = [ \"adjusted_r2\" ] def adjusted_r2 ( r2 : float , n : int , k : int ) -> float : \"\"\"Calculate adjusted R^2. Args: r2 (float): r2 score of the model/ n (int): number of samples. k (int): number of features minus the constant bias term. Returns: adjusted_r2_score (float): r2 * (n - 1) / (n - k - 1) \"\"\" adjusted_r2_score = r2 * ( n - 1 ) / ( n - k - 1 ) return adjusted_r2_score class Results : \"\"\"Stores results for model training in columnwise format.\"\"\" _result_dict : Dict logit_names : List [ str ] score_names : List [ str ] def __init__ ( self , logit_names : List [ str ] = default_logit_names , score_names : List [ str ] = default_score_names , existing_dict : Optional [ Dict ] = None , ): \"\"\"Construct a new results store.\"\"\" self . logit_names = logit_names self . score_names = score_names if existing_dict is not None : self . _result_dict = copy . deepcopy ( existing_dict ) return dict_keys = [ \"identifier\" , * logit_names , * score_names ] self . _result_dict = { key : [] for key in dict_keys } def add ( self , identifier : str , results : Dict , in_place = False ): \"\"\"Add a new results row.\"\"\" if not in_place : return Results ( self . logit_names , self . score_names , self . _result_dict ) . add ( identifier , results , in_place = True ) self . _result_dict [ \"identifier\" ] . append ( identifier ) for result_name in set ([ * results . keys (), * self . logit_names , * self . score_names ]): result_value = results . get ( result_name , np . nan ) self . _result_dict [ result_name ] . append ( result_value ) return self def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } def get_result_values ( self , result_name : str ) -> List [ Any ]: \"\"\"Get a list of values for a result.\"\"\" return self . _result_dict [ result_name ] def to_dataframe ( self ) -> pd . DataFrame : \"\"\"Get a Data Frame containing the results.\"\"\" return pd . DataFrame . from_dict ( self . _result_dict ) def to_dict ( self ) -> Dict : \"\"\"Get a dictionary containing the results. Returns: Dict[str, List[Any]]: Dictionary of result columns \"\"\" return self . _result_dict def compute_metrics ( logits : Dict [ str , np . ndarray ]) -> Dict [ str , Any ]: \"\"\"Compute metrics from logits.\"\"\" y_true , y_pred = logits [ \"y_true\" ], logits [ \"y_pred\" ] default_score_names = [ \"explained_variance_score\" , \"mean_squared_error\" , \"mean_absolute_error\" , \"root_mean_squared_error\" , \"r2_score\" , \"mean_absolute_percentage_error\" , ] default_metrics_dict : Dict [ str , float ] = {} custom_metrics_dict : Dict [ str , float ] = {} for metric_name in default_score_names : if hasattr ( metrics . _regression , metric_name ): # TODO: get metric score with default parameters, consider adding kwargs if you want to configure parameters metric_score = getattr ( metrics . _regression , metric_name )( y_true , y_pred ) else : # logger.info(f\"{metrics._regression} has no such attribute {metric_name}!\") # add custom metrics here rmse = metrics . _regression . mean_squared_error ( y_true , y_pred , squared = False ) custom_metrics_dict [ \"root_mean_squared_error\" ] = rmse if metric_name not in default_metrics_dict : default_metrics_dict [ metric_name ] = metric_score metrics_dict = { ** default_metrics_dict , ** custom_metrics_dict } return metrics_dict def mean_score ( score_values ) -> Union [ float , np . ndarray ]: \"\"\"Compute the mean score.\"\"\" score_values = np . array ( score_values ) shape = score_values . shape if len ( shape ) == 1 : return score_values . mean () return score_values . mean ( axis = 0 ) def mean_cv_results ( model_results : Results ) -> Dict : \"\"\"Add mean cross-validation results. This method computes the mean value for all score types in the model_results, including for scores (e.g., confusion matrix) where the mean value may contain decimal places. \"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result )) for y_result in model_results . logit_names } cv_scores = { score : mean_score ( model_results . get_result_values ( score ) ) for score in model_results . score_names } return { ** cv_logits , ** cv_scores , } def oof_cv_results ( model_results : Results ) -> Dict : \"\"\"Add OOF cross-validation results.\"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result ) ) for y_result in model_results . logit_names } cv_scores = compute_metrics ( cv_logits ) return { ** cv_logits , ** cv_scores , } def add_cv_results ( model_results : Results ): \"\"\"Add cross-validation results. This method returns a copy of the given model results with summary columns for mean and CV cross-validation. \"\"\" mean_cv = mean_cv_results ( model_results ) oof_cv = oof_cv_results ( model_results ) return ( model_results . add ( \"mean_cv\" , mean_cv ) . add ( \"oof_cv\" , oof_cv ) ) def train_on_fold ( df_folds : pd . DataFrame , models : List [ Callable ], num_folds : int , predictor_col : List , target_col : List , ) -> Dict [ str , List ]: \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results. Args: df_folds (pd.DataFrame): [description] model (Callable): [description] num_folds (int): [description] predictor_col (List): [description] target_col (List): [description] Returns: Dict[str, List]: [description] \"\"\" y_true = df_folds [ target_col ] . values . flatten () # test_pred_arr: np.ndarray = np.zeros(len(X_test)) model_dict = {} for model in models : model_results = Results () if isinstance ( model , pipeline . Pipeline ): model_name = model [ \"model\" ] . __class__ . __name__ else : model_name = model . __class__ . __name__ # out-of-fold validation predictions oof_pred_arr : np . ndarray = np . zeros ( len ( df_folds )) for fold in range ( 1 , num_folds + 1 ): train_df = df_folds [ df_folds [ \"fold\" ] != fold ] . reset_index ( drop = True ) val_df = df_folds [ df_folds [ \"fold\" ] == fold ] . reset_index ( drop = True ) val_idx = df_folds [ df_folds [ \"fold\" ] == fold ] . index . values X_train , y_train = train_df [ predictor_col ] . values , train_df [ target_col ] . values X_val , y_val = val_df [ predictor_col ] . values , val_df [ target_col ] . values model . fit ( X_train , y_train ) y_val_pred = model . predict ( X_val ) logits = { \"y_true\" : y_val , \"y_pred\" : y_val_pred , } metrics = compute_metrics ( logits ) model_results . add ( f \"fold { fold } \" , { ** logits , ** metrics }, in_place = True ) if model_name not in model_dict : model_dict [ model_name ] = model_results return model_dict model_dict = train_on_fold ( df_folds , models = regressors , num_folds = 5 , predictor_col = predictor_cols , target_col = target_col ) 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.676e+13, tolerance: 1.868e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:46 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+13, tolerance: 1.862e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:48 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.603e+13, tolerance: 1.760e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:49 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+13, tolerance: 1.897e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:50 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e+13, tolerance: 1.913e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:53 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:53 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! model_dict_with_summary = { model : add_cv_results ( model_results ) for model , model_results in model_dict . items () } 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! results_df = pd . concat ({ name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () }, axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv DummyRegressor identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [541073.7741469058, 541073.7741469058, 541073.... [539415.1803788317, 539415.1803788317, 539415.... [536694.9069548872, 536694.9069548872, 536694.... [539857.5267495662, 539857.5267495662, 539857.... [540990.9316078658, 540990.9316078658, 540990.... [541073.7741469058, 541073.7741469058, 541073.... [541073.7741469058, 541073.7741469058, 541073.... explained_variance_score -0.0 0.0 0.0 0.0 -0.0 -0.0 -0.000169 mean_squared_error 132247512137.06163 133951959536.310318 163296091978.429077 123686085780.066254 119223815039.600311 134481092894.293533 134481092894.293488 mean_absolute_error 229404.508989 232306.56185 241944.838637 232308.320526 230976.128917 233388.071784 233388.071784 root_mean_squared_error 363658.510332 365994.480199 404099.111578 351690.326538 345288.017515 366146.089233 366716.63842 r2_score -0.000407 -0.000007 -0.0013 -0.000013 -0.000402 -0.000426 -0.000169 mean_absolute_percentage_error 0.542257 0.530016 0.531438 0.534771 0.540719 0.53584 0.53584 LinearRegression identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [[667876.5428875468], [614410.8399372244], [73... [[694869.3745167988], [618367.5827104518], [46... [[1023189.3043934056], [304317.4518826463], [9... [[477332.04737439007], [492396.46923110134], [... [[853190.5779001702], [574292.3295129627], [77... [[667876.5428875468], [614410.8399372244], [73... [[667876.5428875468], [614410.8399372244], [73... explained_variance_score 0.680095 0.705119 0.685252 0.693963 0.721335 0.697153 0.696083 mean_squared_error 42290033057.306816 39515957195.400787 51430470051.222527 37873842125.074295 33210290838.463482 40864118653.493584 40864118653.493584 mean_absolute_error 127943.982537 124266.336776 130373.471382 126075.417537 122514.978064 126234.837259 126234.837259 root_mean_squared_error 205645.406118 198786.209772 226782.869836 194612.029754 182236.908552 201612.684806 202148.753777 r2_score 0.68009 0.704997 0.684638 0.693787 0.721334 0.696969 0.696083 mean_absolute_percentage_error 0.265815 0.257898 0.250641 0.251933 0.255471 0.256352 0.256352 Ridge identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [[667923.954879363], [614423.8176756387], [731... [[694094.2870407818], [617981.9933915635], [46... [[1023626.0773182933], [304085.70358412666], [... [[477416.1959023318], [492474.1026423527], [30... [[853485.3791880516], [574203.5276395974], [77... [[667923.954879363], [614423.8176756387], [731... [[667923.954879363], [614423.8176756387], [731... explained_variance_score 0.680109 0.70512 0.685333 0.693972 0.721308 0.697168 0.696102 mean_squared_error 42288266590.795799 39515834657.582001 51417532361.73008 37872817115.509499 33213482338.320721 40861586612.787613 40861586612.787621 mean_absolute_error 127934.770004 124362.619797 130426.951817 126064.678687 122549.911589 126267.786379 126267.786379 root_mean_squared_error 205641.11114 198785.901556 226754.343645 194609.396267 182245.6648 201607.283482 202142.490864 r2_score 0.680104 0.704998 0.684718 0.693795 0.721307 0.696984 0.696102 mean_absolute_percentage_error 0.265796 0.258152 0.250808 0.251901 0.255562 0.256444 0.256444 Lasso identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [667918.069528075, 614415.1560338883, 731173.9... [694081.9059207196, 617992.7838152755, 463279.... [1023674.2634054194, 304066.4170321575, 955604... [477415.1327845714, 492463.7396309793, 305057.... [853505.6953654164, 574205.7699892861, 771097.... [667918.069528075, 614415.1560338883, 731173.9... [667918.069528075, 614415.1560338883, 731173.9... explained_variance_score 0.680106 0.70512 0.685338 0.693969 0.721305 0.697168 0.696102 mean_squared_error 42288598880.705284 39515796822.947304 51416606662.838509 37873203587.611603 33213918899.943615 40861624970.809258 40861624970.809265 mean_absolute_error 127937.742862 124365.678646 130429.153983 126067.441635 122553.005403 126270.604506 126270.604506 root_mean_squared_error 205641.919075 198785.806392 226752.302442 194610.389208 182246.862524 201607.455928 202142.585743 r2_score 0.680101 0.704998 0.684723 0.693792 0.721303 0.696984 0.696102 mean_absolute_percentage_error 0.265806 0.258161 0.250816 0.25191 0.255571 0.256453 0.256453 ElasticNet identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [656773.6005291657, 614707.2479590292, 745914.... [685212.4217947914, 599436.3647276227, 483018.... [907890.7497500202, 369129.65691215475, 862824... [469736.2740988735, 539209.0261494196, 379320.... [804965.135403886, 571735.3944930851, 737716.4... [656773.6005291657, 614707.2479590292, 745914.... [656773.6005291657, 614707.2479590292, 745914.... explained_variance_score 0.65603 0.673131 0.638955 0.670126 0.695926 0.666834 0.664827 mean_squared_error 45475490348.208717 43800728719.55619 59005250118.409103 40812502573.376862 36239917129.190384 45066777777.748253 45066777777.748253 mean_absolute_error 124419.180393 122248.905619 131347.081356 124722.490988 120613.606068 124670.252885 124670.252885 root_mean_squared_error 213249.830828 209286.236336 242909.962987 202021.044877 190367.846889 211566.984383 212289.372739 r2_score 0.655993 0.673009 0.638191 0.670027 0.695912 0.666627 0.664827 mean_absolute_percentage_error 0.247878 0.245323 0.243383 0.244499 0.243172 0.244851 0.244851 DecisionTreeRegressor identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [915000.0, 300000.0, 316000.0, 643000.0, 28200... [535000.0, 630000.0, 410000.0, 335000.0, 72000... [1605000.0, 479000.0, 850000.0, 614000.0, 6505... [449000.0, 555000.0, 440000.0, 247500.0, 35000... [715000.0, 545000.0, 597000.0, 530000.0, 50000... [915000.0, 300000.0, 316000.0, 643000.0, 28200... [915000.0, 300000.0, 316000.0, 643000.0, 28200... explained_variance_score 0.752389 0.756145 0.770122 0.743783 0.767617 0.758011 0.758577 mean_squared_error 32751471474.735687 32664895113.423656 37510896623.824608 31693198492.168594 27707113797.312031 32465515100.292915 32465515100.292915 mean_absolute_error 101035.96819 99810.482071 105829.744939 101019.912377 96233.899075 100786.00133 100786.00133 root_mean_squared_error 180973.676193 180734.321902 193677.300229 178025.836586 166454.539732 179973.134928 180181.894485 r2_score 0.752246 0.756143 0.769991 0.743758 0.767511 0.75793 0.758546 mean_absolute_percentage_error 0.192566 0.186579 0.189517 0.183729 0.184638 0.187406 0.187406 Comparison of Cross-Validated Models The point of the following comparison is to check how different models are performing across folds. More specifically, if we have 5 folds, we will have a metric score for each fold, subsequently, we can find the standard error of model's performance. We need to be aware of models that have high variance across folds in terms of the metrics performance. This can indicate that the model is highly unstable, and may be a sign of overfitting. def summarize_metrics ( metric_name ): ls = [] for model_name , inner_dict in model_dict . items (): folds = inner_dict [ \"identifier\" ][: - 2 ] all_obs = [] for idx , obs in enumerate ( inner_dict [ metric_name ][: - 2 ]): ls . append (( model_name , folds [ idx ], obs )) all_obs . append ( obs ) ls . append (( model_name , \"SE\" , np . std ( all_obs , ddof = 1 ) / len ( all_obs ) ** 0.5 )) fig , ax = plt . subplots ( figsize = ( 15 , 8 )) summary_df = pd . DataFrame ( ls , columns = [ \"model\" , \"fold\" , metric_name ]) # summary_df.to_csv _ = sns . boxplot ( x = \"model\" , y = metric_name , data = summary_df [( summary_df [ 'model' ] != 'DummyClassifier' ) & ( summary_df [ 'fold' ] != 'SE' )], ax = ax ) fig . savefig ( config . spot_checking_boxplot , format = 'png' , dpi = 300 ) return summary_df summary_df = summarize_metrics ( \"roc\" ) display ( summary_df . tail ( 12 )) Out-of-Fold Confusion Matrix We do have information on the performance of each folds, we now look at the performance of all 5 folds together. Typicall there are two ways to do it, one is to simply take the average of the score of five folds, the other is to take a look at out of folds predictions. From the confusion matrix of the out of fold performance, Logistic Regression does seem to be a model we can explore on, although slightly lower in terms of overall AUROC score than SVC, it seems to have the quite low False Negatives amongst all. With further hyperparameter tuning and threshold optimization, we can make it better. model_names = [ model for model in model_dict . keys ()] fig , ax = plt . subplots ( 2 , 3 , figsize = ( 10 , 10 )) for axes , algo in zip ( ax . ravel (), model_names ): cf_mat = results_df . oof_cv [ algo ] . confusion_matrix #### scores auc = results_df . oof_cv [ algo ] . roc #### annotations labels = [ \"True Neg\" , \"False Pos\" , \"False Neg\" , \"True Pos\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in cf_mat . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in cf_mat . flatten () / np . sum ( cf_mat )] #### final annotations label = ( np . array ([ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )]) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = cf_mat , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = axes , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) axes . text ( 0 , - 0 , \" {} \" . format ( algo ), { \"size\" : 12 , \"color\" : \"black\" , \"weight\" : \"bold\" }) axes . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) axes . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( round ( auc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels axes . set_xticklabels ( \"\" ) axes . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Out Of Fold Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"This Visualization show the results of various classifiers and there respective results.\"\"\" , { \"size\" : 14 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) fig . savefig ( config . oof_confusion_matrix , format = 'png' , dpi = 300 ) Hypothesis Testing Across Models I am slightly shocked at the performance of plain LogisticRegression, I decide to use an idea from Hypothesis Testing Across Models to check if the difference is really by chance or not. Note that I will be modifying his code as his code does not split using StratifiedKFold. The basic idea is to test if two model's difference in scores (in this case roc), is statistically significant or not. However, we note that this method may violate an assumption of Student's t test. Null Hypothesis \\(H_0\\) : The difference in the performance score of two classifiers is Statistically Significant. Alternate Hypothesis \\(H_1\\) : The difference in the performance score of two classifiers is not Statistically Significant. def paired_ttest_skfold_cv ( estimator1 , estimator2 , X , y , cv = 10 , scoring = None , shuffle = False , random_seed = None ): \"\"\"Modified from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/ttest.py to accomodate StratifiedKFold\"\"\" if not shuffle : skf = model_selection . StratifiedKFold ( n_splits = cv , shuffle = shuffle ) else : skf = model_selection . StratifiedKFold ( n_splits = cv , random_state = random_seed , shuffle = shuffle ) if scoring is None : if estimator1 . _estimator_type == \"classifier\" : scoring = \"accuracy\" elif estimator1 . _estimator_type == \"regressor\" : scoring = \"r2\" else : raise AttributeError ( \"Estimator must \" \"be a Classifier or Regressor.\" ) if isinstance ( scoring , str ): scorer = metrics . get_scorer ( scoring ) else : scorer = scoring score_diff = [] for train_index , test_index in skf . split ( X = X , y = y ): X_train , X_test = X [ train_index ], X [ test_index ] y_train , y_test = y [ train_index ], y [ test_index ] estimator1 . fit ( X_train , y_train ) estimator2 . fit ( X_train , y_train ) est1_score = scorer ( estimator1 , X_test , y_test ) est2_score = scorer ( estimator2 , X_test , y_test ) score_diff . append ( est1_score - est2_score ) avg_diff = np . mean ( score_diff ) numerator = avg_diff * np . sqrt ( cv ) denominator = np . sqrt ( sum ([( diff - avg_diff ) ** 2 for diff in score_diff ]) / ( cv - 1 )) t_stat = numerator / denominator pvalue = stats . t . sf ( np . abs ( t_stat ), cv - 1 ) * 2.0 return float ( t_stat ), float ( pvalue ) # check if difference between algorithms is real X_tmp = X_y_train [ predictor_cols ] . values y_tmp = X_y_train [ 'diagnosis' ] . values t , p = paired_ttest_skfold_cv ( estimator1 = classifiers [ 1 ], estimator2 = classifiers [ - 1 ], shuffle = True , cv = 5 , X = X_tmp , y = y_tmp , scoring = 'roc_auc' , random_seed = config . seed ) print ( 'P-value: %.3f , t-Statistic: %.3f ' % ( p , t )) Since P value is quite high, and more the basic threshold of 0.05 or 0.1, we fail to reject the null hypothesis, and say that there is no significant difference between these two models. Model Selection: Hyperparameter Tuning with GridSearchCV Hyperparameter Tuning: We have done a quick spot checking on algorithms and realized that LogisticRegression is doing well for this task. For this purpose, I will just perform hyperparameter tuning on this single algorithm. However, in practice and if resources are allowed, I will also tune other models such as RandomForest() , or gradient boosting algorithms such as XGBoost , as I believe they will perform no worse than our Logistic Regression model given the right hyperparameters. Grid Search: We will use an old-fashioned way to search for hyperparameters, which is brute force method. The time complexity of Grid Search is high and if you have many hyperparameters to tune, I recommend trying out Random Grid Search or libraries like Optuna that uses Bayesian Optimization. def make_finetuning_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # reduce VIF steps . append (( 'remove_multicollinearity' , ReduceVIF ( thresh = 10 ))) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline Reconstruct our pipeline but now only taking in LogisticRegression . pipeline_logistic = make_finetuning_pipeline ( linear_model . LogisticRegression ( solver = \"saga\" , random_state = config . seed , max_iter = 10000 , n_jobs = None , fit_intercept = True ) ) Define our search space for the hyperparameters: param_grid = { model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 )} param_grid = dict ( model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 ), ) Run our hyperparameter search with cross-validation. For example, our param_grid has \\(2 \\times 10 = 20\\) combinations, and our cross validation has 5 folds, then there will be a total of 100 fits. Below details the pseudo code of what happens under the hood: Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\) grid = model_selection . GridSearchCV ( pipeline_logistic , param_grid = param_grid , cv = 5 , refit = True , verbose = 3 , scoring = \"roc_auc\" ) _ = grid . fit ( X_train , y_train ) We can save our results in a dataframe, we will also look at the top performing hyperparameter by querying the below: grid_cv_df = pd . DataFrame ( grid . cv_results_ ) grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ] grid_cv_df = pd . DataFrame ( grid . cv_results_ ) best_cv = grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ] display ( best_cv ) best_hyperparams = grid . best_params_ print ( f \"Best Hyperparameters found is { best_hyperparams } \" ) Our best performing set of hyperparameters {'model__C': 0.3593813663804626, 'model__penalty': 'l2'} gives rise to a mean cross validation score of \\(0.988739\\) , which is higher than the model with default hyperparameter scoring, \\(0.987136\\) . Room for Improvement: Apart from the other methods to search for the optimal hyperparameters, we can also include preprocessing step as a tunable hyperparameter. More specifically, in our ReduceVIF() step, we hard coded two manual criterion in which the algorithm will stop; if the threshold reaches 10, or if the number of features removed hit 20; we can include them in the search space. Retrain on the whole training set A common practice after the hyperparameter tuning phase is to retrain the model on the whole dataset \\(X_{\\text{train}}\\) where we will get the estimator's coefficients obtained from the retraining. This is actually already done as the scikit-learn's GridSearchCV has a parameter refit ; if we select it to be true, then after the model selection process is done (i.e. getting the best hyperparameters after cross validation with grid search), the grid search object will retrain on the whole \\(X_{\\text{train}}\\) with the best hyperparameters internally, and return us back an object in which we can call predict etc. Retrain using optimal hyperparameters However, to be extra careful, we can retrain manually using the best hyperparameters and check if scikit-learn is true to its documentation. We will just reconstruct the pipeline using the grid's best hyper parameters. We will then test if the retrained model's coefficients coincide with the grid's best estimator's coefficients. If there difference is 0, this means they are trained under the same circumstances and we can be sure that the refit parameter is behaving true to its words. grid_best_hyperparams = grid . best_params_ print ( grid_best_hyperparams ) -> { 'model__C' : 0.3593813663804626 , 'model__penalty' : 'l2' } retrain_pipeline = pipeline . Pipeline ( [ ( \"standardize\" , preprocessing . StandardScaler ()), ( 'remove_multicollinearity' , ReduceVIF ( thresh = 10 )), ( \"model\" , linear_model . LogisticRegression ( C = 0.3593813663804626 , max_iter = 10000 , random_state = 1992 , solver = \"saga\" , penalty = \"l1\" ), ), ] ) _ = retrain_pipeline . fit ( X_train , y_train ) coef_diff = retrain_pipeline [ 'model' ] . coef_ - grid . best_estimator_ [ 'model' ] . coef_ print ( \"...\" ) assert np . all ( coef_diff == 0 ) == True print ( \"Retraining Assertion Passed!\" ) Interpretation of Results Interpretation of Coefficients As shown in the figure below, all else being equal, for every square unit increase in mean cell area, the odds of the tumor being malignant increases by a factor of \\(e^{1.43} = 4.19\\) . The variation (standard error) of the characteristics of cells also are deemed important by the model, for example, area se played an important role in determining whether a cell is malignant; intuitively, if some cells are noticably larger than the rest, then it is also a good indicator of malignancy. selected_features_by_vif_index = grid . best_estimator_ [ 'remove_multicollinearity' ] . column_indices_kept_ selected_feature_names = np . asarray ( predictor_cols )[ selected_features_by_vif_index ] selected_features_coefficients = grid . best_estimator_ [ 'model' ] . coef_ . flatten () # assertion #assert grid.best_estimator_['remove_multicollinearity'].feature_names_ == retrain_pipeline['remove_multicollinearity'].feature_names_ fig , ax = plt . subplots ( figsize = ( 15 , 15 )) # .abs() _ = pd . Series ( selected_features_coefficients , index = selected_feature_names ) . sort_values () . plot ( ax = ax , kind = 'barh' ) fig . savefig ( config . feature_importance , format = \"png\" , dpi = 300 ) Interpretation of Metric Scores on Train Set We are also interested in choosing an optimal threshold for the model such that it gives the lowest recall, or False Negatives. We note that the default threshold when calling predict() from a model is \\(0.5\\) . In this section, we will explore one way to get the best tradeoff we can when choosing a high recall, while maintaining a reasonable score for precision. def evaluate_train_test_set ( estimator : Callable , X : Union [ pd . DataFrame , np . ndarray ], y : Union [ pd . DataFrame , np . ndarray ] ) -> Dict [ str , Union [ float , np . ndarray ]]: \"\"\"This function takes in X and y and returns a dictionary of scores. Args: estimator (Callable): [description] X (Union[pd.DataFrame, np.ndarray]): [description] y (Union[pd.DataFrame, np.ndarray]): [description] Returns: Dict[str, Union[float, np.ndarray]]: [description] \"\"\" test_results = {} y_pred = estimator . predict ( X ) # This is the probability array of class 1 (malignant) y_prob = estimator . predict_proba ( X )[:, 1 ] test_brier = metrics . brier_score_loss ( y , y_prob ) test_roc = metrics . roc_auc_score ( y , y_prob ) test_results [ \"brier\" ] = test_brier test_results [ \"roc\" ] = test_roc test_results [ \"y\" ] = np . asarray ( y ) . flatten () test_results [ \"y_pred\" ] = y_pred . flatten () test_results [ \"y_prob\" ] = y_prob . flatten () return test_results def plot_precision_recall_vs_threshold ( precisions , recalls , thresholds ): \"\"\" Modified from: Hands-On Machine learning with Scikit-Learn and TensorFlow; p.89 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( \"Precision and Recall Scores as a function of the decision threshold\" ) plt . plot ( thresholds , precisions [: - 1 ], \"b--\" , label = \"Precision\" ) plt . plot ( thresholds , recalls [: - 1 ], \"g-\" , label = \"Recall\" ) plt . ylabel ( \"Score\" ) plt . xlabel ( \"Decision Threshold\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . precision_recall_threshold_plot , format = \"png\" , dpi = 300 ) def plot_roc_curve ( fpr , tpr , label = None ): \"\"\" The ROC curve, modified from Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( 'ROC Curve' ) plt . plot ( fpr , tpr , linewidth = 2 , label = label ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . axis ([ - 0.005 , 1 , 0 , 1.005 ]) plt . xticks ( np . arange ( 0 , 1 , 0.05 ), rotation = 90 ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate (Recall)\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . roc_plot , format = \"png\" , dpi = 300 ) def adjusted_classes ( y_scores , t ): \"\"\" This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" return [ 1 if y >= t else 0 for y in y_scores ] The plots below show the tradeoffs between precision and recall, recall and false positive rate. The confusion matrix on the train set tells us that there is still more false negatives than false positives. We can choose a particular threshold in order to minimize false negatives, at some expense of false positive. train_results = evaluate_train_test_set ( grid , X_train , y_train ) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) # CM cm_train = metrics . confusion_matrix ( train_results [ 'y' ], train_results [ 'y_pred' ]) #### scores auc = metrics . roc_auc_score ( train_results [ 'y' ], train_results [ 'y_prob' ]) #### annotations labels = [ \"True Neg\" , \"False Pos\" , \"False Neg\" , \"True Pos\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in cm_train . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in cm_train . flatten () / np . sum ( cm_train )] #### final annotations label = ( np . array ([ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )]) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = cm_train , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = ax , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) ax . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) ax . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( round ( auc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels ax . set_xticklabels ( \"\" ) ax . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Train Set Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"Training Set Confusion Matrix.\"\"\" , { \"size\" : 12 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) fig . savefig ( config . final_train_confusion_matrix , format = 'png' , dpi = 300 ) # generate the precision recall curve precision , recall , pr_thresholds = metrics . precision_recall_curve ( train_results [ 'y' ], train_results [ 'y_prob' ]) fpr , tpr , roc_thresholds = metrics . roc_curve ( train_results [ 'y' ], train_results [ 'y_prob' ], pos_label = 1 ) # use the same p, r, thresholds that were previously calculated plot_precision_recall_vs_threshold ( precision , recall , pr_thresholds ) Based on the tradeoff plot above, a good threshold can be set at \\(t = 0.35\\) , let us see how it performs with this threshold. y_pred_adj = adjusted_classes ( train_results [ \"y_prob\" ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( train_results [ \"y\" ], y_pred_adj ), columns = [ \"pred_neg\" , \"pred_pos\" ], index = [ \"neg\" , \"pos\" ], ) ) print ( metrics . classification_report ( y_true = train_results [ \"y\" ], y_pred = y_pred_adj )) train_brier = train_results [ 'brier' ] print ( f \"train brier: { train_brier } \" ) The False Negatives reduced from 15 to 9, at the expense of increase False Positives from 6 to 14. We should take comfort that less patients are falsely classified as benign when in fact they need treatment. This is a tradeoff that we have to choose. The ROC curve below also paints a similar story, in order for you to have high recall, one must accept that there will more False Positives. plot_roc_curve ( fpr , tpr , 'recall_optimized' ) Evaluation on Test Set Ultimately, we are interested in finding the estimate of the generalization error of the model, and thus it is time for us to evaluate our model's performance on the \"unseen\" test set \\(X_{\\text{test}}\\) to get a good gauge on how well the model generalizes to unseen data. Take note that now everything has settled, we will use the exact hyperparameters to predict on test set, with the pre-defined threshold of 0.35. test_results = evaluate_train_test_set ( grid , X_test , y_test ) y_test_pred_adj = adjusted_classes ( test_results [ 'y_prob' ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( test_results [ 'y' ], y_test_pred_adj ), columns = [ 'pred_neg' , 'pred_pos' ], index = [ 'neg' , 'pos' ])) test_roc = test_results [ 'roc' ] test_brier = test_results [ 'brier' ] print ( test_roc ) print ( test_brier ) print ( metrics . classification_report ( y_true = test_results [ \"y\" ], y_pred = y_test_pred_adj )) Using the same threshold we used on training set, we see that the False Negative is quite low. The overall ROC score is 0.9828, and the corresponding Brier Score is 0.04136, both seem reasonably well performing. Bias-Variance Tradeoff avg_expected_loss , avg_bias , avg_var = bias_variance_decomp ( grid . best_estimator_ [ 'model' ], X_train . values , y_train . values , X_test . values , y_test . values , loss = '0-1_loss' , random_seed = 123 ) print ( 'Average expected loss: %.3f ' % avg_expected_loss ) print ( 'Average bias: %.3f ' % avg_bias ) print ( 'Average variance: %.3f ' % avg_var ) We use the mlxtend library to estimate the Bias-Variance Tradeoff in our Logistic Regression model. The core idea behind this function is to use bagging and repeatedly sample from our training set so as to simulate that we are actually drawing samples from the \"true\" population over a distribution \\(\\mathcal{P}\\) . As expected, Logistic Regression being a linear model, its simplicity contributes to its high bias and low variance.","title":"House sales in king country usa"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#dependencies-and-configuration","text":"import csv import random from functools import wraps from time import time from typing import Callable , Dict , List , Union , Optional , Any import matplotlib.pyplot as plt import copy import mlxtend import numpy as np import pandas as pd import seaborn as sns from mlxtend.evaluate import paired_ttest_5x2cv , bias_variance_decomp from scipy import stats from sklearn import ( base , decomposition , dummy , ensemble , feature_selection , linear_model , metrics , model_selection , neighbors , pipeline , preprocessing , svm , tree ) from statsmodels.regression.linear_model import OLS #from statsmodels.stats.outliers_influence import variance_inflation_factor from dataclasses import dataclass import logging @dataclass class config : raw_data : str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/regression/ %20ho use-sales-in-king-country-usa/data/raw/kc_house_data.csv\" train_size : float = 0.8 seed : int = 1992 num_folds : int = 5 cv_schema : str = \"KFold\" def set_seeds ( seed : int = 1234 ) -> None : \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) def init_logger ( log_file : str = \"info.log\" ): \"\"\" Initialize logger. \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( logging . INFO ) stream_handler = logging . StreamHandler () stream_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) file_handler = logging . FileHandler ( filename = log_file ) file_handler . setFormatter ( logging . Formatter ( \" %(asctime)s - %(message)s \" , datefmt = \"%Y-%m- %d ,%H:%M:%S\" )) logger . addHandler ( stream_handler ) logger . addHandler ( file_handler ) return logger # set seeding for reproducibility _ = set_seeds ( seed = config . seed ) # set logger logger = init_logger () # read data df = pd . read_csv ( config . raw_data )","title":"Dependencies and Configuration"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#how-eda-helped-us","text":"Insights derived from EDA: To fill in.","title":"How EDA helped us?"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#cross-validation-strategy","text":"Generalization: Ultimately, we are interested in the Generalization Error made by the model, that is, how well the model perform on unseen data that is not taken from our sample set $\\mathcal{D}$. In general, we use validation set for Model Selection and the test set for an estimate of generalization error on new data. - Refactored from Elements of Statistical Learning, Chapter 7.2 Step 1: Train-Test-Split: Since this dataset is relatively small, we will not use the train-validation-test split and only split into train and test in a ratio of 9:1, whereby the split is stratified on our target, using stratify=y parameter in train_test_split() to ensure that our target has equal representation in both train and test. We note that this is a relatively small dataset and in practice, we need a large sample size to get a reliable/stable split, it is also recommended to retrain the whole dataset (without the \"unseen\" test set) after we have done the model selection process (eg. finding best hyperparameters). Step 2: Resampling Stategy: Note that we will be performing StratifiedKFold as our resampling strategy. After our split in Step 1, we have a training set $X_{\\text{train}}$, we will then perform our resampling strategy on this $X_{\\text{train}}$. We will choose our choice of $K = 5$. The choice of $K$ is somewhat arbitrary, and is derived empirically . To recap, we have the following: Training Set ( \\(X_{\\text{train}}\\) ) : This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis \\(h \\in \\mathcal{H}\\) . Validation Set ( \\(X_{\\text{val}}\\) ) : This is split from our \\(X_{\\text{train}}\\) during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis \\(g \\in \\mathcal{H}\\) ). Test Set ( \\(X_{\\text{test}}\\) ) : This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model \\(g\\) , we will use \\(g\\) to predict on the test set to get an estimate of the generalization error (also called out-of-sample error). Courtesy of scikit-learn on a typical Cross-Validation workflow. predictor_cols = df . columns [ 3 :] . tolist () target_col = [ \"price\" ] logger . info ( f \" \\n The predictor columns are \\n { predictor_cols } \" ) 2021-11-10,12:09:44 - The predictor columns are ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'] X = df [ predictor_cols ] . copy () y = df [ target_col ] . copy () # Split train - test X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , y , train_size = config . train_size , shuffle = True , random_state = config . seed ) logger . info ( f \" \\n Shape of train: { X_train . shape } \\n Shape of test: { X_test . shape } \" ) 2021-11-10,12:09:44 - Shape of train: (17290, 18) Shape of test: (4323, 18) def make_folds ( df : pd . DataFrame , num_folds : int , cv_schema : str , seed : int , predictor_col : List , target_col : List , ) -> pd . DataFrame : \"\"\"Split the given dataframe into training folds. Args: df (pd.DataFrame): [description] num_folds (int): [description] cv_schema (str): [description] seed (int): [description] Returns: pd.DataFrame: [description] \"\"\" if cv_schema == \"KFold\" : df_folds = df . copy () kf = model_selection . KFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( kf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) elif cv_schema == \"StratifiedKFold\" : df_folds = df . copy () skf = model_selection . StratifiedKFold ( n_splits = num_folds , shuffle = True , random_state = seed ) for fold , ( train_idx , val_idx ) in enumerate ( skf . split ( X = df_folds [ predictor_col ], y = df_folds [ target_col ]) ): df_folds . loc [ val_idx , \"fold\" ] = int ( fold + 1 ) df_folds [ \"fold\" ] = df_folds [ \"fold\" ] . astype ( int ) print ( df_folds . groupby ([ \"fold\" , \"diagnosis\" ]) . size ()) return df_folds X_y_train = pd . concat ([ X_train , y_train ], axis = 1 ) . reset_index ( drop = True ) df_folds = make_folds ( X_y_train , num_folds = config . num_folds , cv_schema = config . cv_schema , seed = config . seed , predictor_col = predictor_cols , target_col = target_col ) Looks good! All our five folds are now in df_fold !","title":"Cross-Validation Strategy"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#modelling","text":"","title":"Modelling"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#spot-checking-algorithms","text":"Terminology Alert! This method is advocated by Jason Brownlee PhD and this serves as the first stage of my modelling process. We will rapidly test (spot check) different classifier algorithms, from DummyClassifier , to LinearModel to more sophisticated ensemble trees like RandomForest . I also note to the readers that we need to think of a few things when choosing the \"optimal\" machine learning algorithm: No Lunch Free Theorem intuitively says that no single optimization algorithm can work best in all situations. Therefore, spot checking can help us form a basis of which algorithm might work better in this particular scenario. Occam's Razor often appears in many Machine Learning textbook, and the narrative is that a simpler model more often times generalizes better than a complex model. This is not unfamiliar when we think of the bias-variance tradeoff, and that is why there is always a tradeoff that we must make.","title":"Spot Checking Algorithms"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#make-basic-pipeline-say-no-to-data-leakage","text":"Say No to Data Leakage: This has been emphasized throughout and we must be careful as we should never touch the test set when fitting the model. This means that preprocessing steps such as StandardScaling() should only be fitted on the training data, and then apply the same transformation (mean and std) on the test data. In other words, do not apply scaling on the whole dataset before splitting. However, it is also equally important to take note not to contaminate our validation set, which is often overlooked, resulting in over optimistic results from model selection phase, but perform badly on unseen test set. As a result, when we use a 5 fold cross validation, we should be careful during fitting that the preprocessing steps are only applied on the training folds, and not on all 5 folds. The same idea is also applied to our ReduceVIF() preprocessing step. We should also include this in our pipeline and not select the features outside the cross-validation loop. Scikit Learn's Pipeline object will prevent us from data leakage, as the steps in a pipeline is already pre-defined. There is also a lot of flexibility in this object, as you can even write custom functions in your pipeline! def variance_inflation_factor ( exog , idx_kept , vif_idx ): \"\"\"Compute VIF for one feature. Args: exog (np.ndarray): Observations idx_kept (List[int]): Indices of features to consider vif_idx (int): Index of feature for which to compute VIF Returns: float: VIF for the selected feature \"\"\" exog = np . asarray ( exog ) x_i = exog [:, vif_idx ] mask = [ col for col in idx_kept if col != vif_idx ] x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) return vif class ReduceVIF ( base . BaseEstimator , base . TransformerMixin ): \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class; I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method. \"\"\" def __init__ ( self , thresh = 10 , max_drop = 20 ): self . thresh = thresh self . max_drop = max_drop self . column_indices_kept_ = [] self . feature_names_kept_ = None def reset ( self ): \"\"\"Resets the state of predictor columns after each fold.\"\"\" self . column_indices_kept_ = [] self . feature_names_kept_ = None def fit ( self , X , y = None ): \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" self . column_indices_kept_ , self . feature_names_kept_ = self . calculate_vif ( X ) return self def transform ( self , X , y = None ): \"\"\"Transforms the Validation Set according to the selected feature names. Args: X ([type]): [description] y ([type], optional): [description]. Defaults to None. Returns: [type]: [description] \"\"\" return X [:, self . column_indices_kept_ ] def calculate_vif ( self , X : Union [ np . ndarray , pd . DataFrame ]): \"\"\"Implements a VIF function that recursively eliminates features. Args: X (Union[np.ndarray, pd.DataFrame]): [description] Returns: [type]: [description] \"\"\" feature_names = None column_indices_kept = list ( range ( X . shape [ 1 ])) if isinstance ( X , pd . DataFrame ): feature_names = X . columns dropped = True count = 0 while dropped and count <= self . max_drop : dropped = False max_vif , max_vif_col = None , None for col in column_indices_kept : vif = variance_inflation_factor ( X , column_indices_kept , col ) if max_vif is None or vif > max_vif : max_vif = vif max_vif_col = col if max_vif > self . thresh : print ( f \"Dropping { max_vif_col } with vif= { max_vif } \" ) column_indices_kept . remove ( max_vif_col ) if feature_names is not None : feature_names . pop ( max_vif_col ) dropped = True count += 1 return column_indices_kept , feature_names # create a feature preparation pipeline for a model def make_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # reduce VIF # steps.append((\"remove_multicollinearity\", ReduceVIF(thresh=10))) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline regressors = [ # baseline model dummy . DummyRegressor ( strategy = \"mean\" ), # linear model linear_model . LinearRegression ( fit_intercept = True ), linear_model . Ridge ( random_state = config . seed , alpha = 1 , fit_intercept = True ), linear_model . Lasso ( random_state = config . seed , alpha = 1 , fit_intercept = True , ), linear_model . ElasticNet ( random_state = config . seed , alpha = 1 , l1_ratio = 0.5 , fit_intercept = True , ), # tree tree . DecisionTreeRegressor ( random_state = config . seed , criterion = \"squared_error\" ), # ensemble # ensemble.RandomForestClassifier(random_state=config.seed), ] regressors = [ make_pipeline ( model ) for model in regressors ]","title":"Make Basic Pipeline (Say No to Data Leakage!)"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#define-metrics","text":"default_result_names = [ \"y_true\" , \"y_pred\" , ] default_logit_names = [ \"y_true\" , \"y_pred\" , ] default_score_names = [ \"explained_variance_score\" , \"mean_squared_error\" , \"mean_absolute_error\" , \"root_mean_squared_error\" , \"r2_score\" , \"mean_absolute_percentage_error\" , ] custom_score_names = [ \"adjusted_r2\" ] def adjusted_r2 ( r2 : float , n : int , k : int ) -> float : \"\"\"Calculate adjusted R^2. Args: r2 (float): r2 score of the model/ n (int): number of samples. k (int): number of features minus the constant bias term. Returns: adjusted_r2_score (float): r2 * (n - 1) / (n - k - 1) \"\"\" adjusted_r2_score = r2 * ( n - 1 ) / ( n - k - 1 ) return adjusted_r2_score class Results : \"\"\"Stores results for model training in columnwise format.\"\"\" _result_dict : Dict logit_names : List [ str ] score_names : List [ str ] def __init__ ( self , logit_names : List [ str ] = default_logit_names , score_names : List [ str ] = default_score_names , existing_dict : Optional [ Dict ] = None , ): \"\"\"Construct a new results store.\"\"\" self . logit_names = logit_names self . score_names = score_names if existing_dict is not None : self . _result_dict = copy . deepcopy ( existing_dict ) return dict_keys = [ \"identifier\" , * logit_names , * score_names ] self . _result_dict = { key : [] for key in dict_keys } def add ( self , identifier : str , results : Dict , in_place = False ): \"\"\"Add a new results row.\"\"\" if not in_place : return Results ( self . logit_names , self . score_names , self . _result_dict ) . add ( identifier , results , in_place = True ) self . _result_dict [ \"identifier\" ] . append ( identifier ) for result_name in set ([ * results . keys (), * self . logit_names , * self . score_names ]): result_value = results . get ( result_name , np . nan ) self . _result_dict [ result_name ] . append ( result_value ) return self def get_result ( self , result_name : str ) -> Dict [ str , Any ]: \"\"\"Get a map of identifiers to result values for a result.\"\"\" return { identifier : result_value for identifier , result_value in zip ( self . _result_dict [ \"identifier\" ], self . _result_dict [ result_name ]) } def get_result_values ( self , result_name : str ) -> List [ Any ]: \"\"\"Get a list of values for a result.\"\"\" return self . _result_dict [ result_name ] def to_dataframe ( self ) -> pd . DataFrame : \"\"\"Get a Data Frame containing the results.\"\"\" return pd . DataFrame . from_dict ( self . _result_dict ) def to_dict ( self ) -> Dict : \"\"\"Get a dictionary containing the results. Returns: Dict[str, List[Any]]: Dictionary of result columns \"\"\" return self . _result_dict def compute_metrics ( logits : Dict [ str , np . ndarray ]) -> Dict [ str , Any ]: \"\"\"Compute metrics from logits.\"\"\" y_true , y_pred = logits [ \"y_true\" ], logits [ \"y_pred\" ] default_score_names = [ \"explained_variance_score\" , \"mean_squared_error\" , \"mean_absolute_error\" , \"root_mean_squared_error\" , \"r2_score\" , \"mean_absolute_percentage_error\" , ] default_metrics_dict : Dict [ str , float ] = {} custom_metrics_dict : Dict [ str , float ] = {} for metric_name in default_score_names : if hasattr ( metrics . _regression , metric_name ): # TODO: get metric score with default parameters, consider adding kwargs if you want to configure parameters metric_score = getattr ( metrics . _regression , metric_name )( y_true , y_pred ) else : # logger.info(f\"{metrics._regression} has no such attribute {metric_name}!\") # add custom metrics here rmse = metrics . _regression . mean_squared_error ( y_true , y_pred , squared = False ) custom_metrics_dict [ \"root_mean_squared_error\" ] = rmse if metric_name not in default_metrics_dict : default_metrics_dict [ metric_name ] = metric_score metrics_dict = { ** default_metrics_dict , ** custom_metrics_dict } return metrics_dict def mean_score ( score_values ) -> Union [ float , np . ndarray ]: \"\"\"Compute the mean score.\"\"\" score_values = np . array ( score_values ) shape = score_values . shape if len ( shape ) == 1 : return score_values . mean () return score_values . mean ( axis = 0 ) def mean_cv_results ( model_results : Results ) -> Dict : \"\"\"Add mean cross-validation results. This method computes the mean value for all score types in the model_results, including for scores (e.g., confusion matrix) where the mean value may contain decimal places. \"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result )) for y_result in model_results . logit_names } cv_scores = { score : mean_score ( model_results . get_result_values ( score ) ) for score in model_results . score_names } return { ** cv_logits , ** cv_scores , } def oof_cv_results ( model_results : Results ) -> Dict : \"\"\"Add OOF cross-validation results.\"\"\" cv_logits = { y_result : np . concatenate ( model_results . get_result_values ( y_result ) ) for y_result in model_results . logit_names } cv_scores = compute_metrics ( cv_logits ) return { ** cv_logits , ** cv_scores , } def add_cv_results ( model_results : Results ): \"\"\"Add cross-validation results. This method returns a copy of the given model results with summary columns for mean and CV cross-validation. \"\"\" mean_cv = mean_cv_results ( model_results ) oof_cv = oof_cv_results ( model_results ) return ( model_results . add ( \"mean_cv\" , mean_cv ) . add ( \"oof_cv\" , oof_cv ) ) def train_on_fold ( df_folds : pd . DataFrame , models : List [ Callable ], num_folds : int , predictor_col : List , target_col : List , ) -> Dict [ str , List ]: \"\"\"Take in a dataframe with fold number as column, and a models which holds a list of callable models, we will loop through and return a dictionary of cv results. Args: df_folds (pd.DataFrame): [description] model (Callable): [description] num_folds (int): [description] predictor_col (List): [description] target_col (List): [description] Returns: Dict[str, List]: [description] \"\"\" y_true = df_folds [ target_col ] . values . flatten () # test_pred_arr: np.ndarray = np.zeros(len(X_test)) model_dict = {} for model in models : model_results = Results () if isinstance ( model , pipeline . Pipeline ): model_name = model [ \"model\" ] . __class__ . __name__ else : model_name = model . __class__ . __name__ # out-of-fold validation predictions oof_pred_arr : np . ndarray = np . zeros ( len ( df_folds )) for fold in range ( 1 , num_folds + 1 ): train_df = df_folds [ df_folds [ \"fold\" ] != fold ] . reset_index ( drop = True ) val_df = df_folds [ df_folds [ \"fold\" ] == fold ] . reset_index ( drop = True ) val_idx = df_folds [ df_folds [ \"fold\" ] == fold ] . index . values X_train , y_train = train_df [ predictor_col ] . values , train_df [ target_col ] . values X_val , y_val = val_df [ predictor_col ] . values , val_df [ target_col ] . values model . fit ( X_train , y_train ) y_val_pred = model . predict ( X_val ) logits = { \"y_true\" : y_val , \"y_pred\" : y_val_pred , } metrics = compute_metrics ( logits ) model_results . add ( f \"fold { fold } \" , { ** logits , ** metrics }, in_place = True ) if model_name not in model_dict : model_dict [ model_name ] = model_results return model_dict model_dict = train_on_fold ( df_folds , models = regressors , num_folds = 5 , predictor_col = predictor_cols , target_col = target_col ) 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:44 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:45 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.676e+13, tolerance: 1.868e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:46 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+13, tolerance: 1.862e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:48 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.603e+13, tolerance: 1.760e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:49 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+13, tolerance: 1.897e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:50 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! C:\\Users\\reighns\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e+13, tolerance: 1.913e+11 model = cd_fast.enet_coordinate_descent( 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:52 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:53 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:09:53 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! model_dict_with_summary = { model : add_cv_results ( model_results ) for model , model_results in model_dict . items () } 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! 2021-11-10,12:19:12 - <module 'sklearn.metrics._regression' from 'C:\\\\Users\\\\reighns\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_regression.py'> has no such attribute root_mean_squared_error! results_df = pd . concat ({ name : results . to_dataframe () . T for name , results in model_dict_with_summary . items () }, axis = 0 ) results_df . columns = [ 'fold 1' , 'fold 2' , 'fold 3' , 'fold 4' , 'fold 5' , 'mean_cv' , 'oof_cv' ] results_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv DummyRegressor identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [541073.7741469058, 541073.7741469058, 541073.... [539415.1803788317, 539415.1803788317, 539415.... [536694.9069548872, 536694.9069548872, 536694.... [539857.5267495662, 539857.5267495662, 539857.... [540990.9316078658, 540990.9316078658, 540990.... [541073.7741469058, 541073.7741469058, 541073.... [541073.7741469058, 541073.7741469058, 541073.... explained_variance_score -0.0 0.0 0.0 0.0 -0.0 -0.0 -0.000169 mean_squared_error 132247512137.06163 133951959536.310318 163296091978.429077 123686085780.066254 119223815039.600311 134481092894.293533 134481092894.293488 mean_absolute_error 229404.508989 232306.56185 241944.838637 232308.320526 230976.128917 233388.071784 233388.071784 root_mean_squared_error 363658.510332 365994.480199 404099.111578 351690.326538 345288.017515 366146.089233 366716.63842 r2_score -0.000407 -0.000007 -0.0013 -0.000013 -0.000402 -0.000426 -0.000169 mean_absolute_percentage_error 0.542257 0.530016 0.531438 0.534771 0.540719 0.53584 0.53584 LinearRegression identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [[667876.5428875468], [614410.8399372244], [73... [[694869.3745167988], [618367.5827104518], [46... [[1023189.3043934056], [304317.4518826463], [9... [[477332.04737439007], [492396.46923110134], [... [[853190.5779001702], [574292.3295129627], [77... [[667876.5428875468], [614410.8399372244], [73... [[667876.5428875468], [614410.8399372244], [73... explained_variance_score 0.680095 0.705119 0.685252 0.693963 0.721335 0.697153 0.696083 mean_squared_error 42290033057.306816 39515957195.400787 51430470051.222527 37873842125.074295 33210290838.463482 40864118653.493584 40864118653.493584 mean_absolute_error 127943.982537 124266.336776 130373.471382 126075.417537 122514.978064 126234.837259 126234.837259 root_mean_squared_error 205645.406118 198786.209772 226782.869836 194612.029754 182236.908552 201612.684806 202148.753777 r2_score 0.68009 0.704997 0.684638 0.693787 0.721334 0.696969 0.696083 mean_absolute_percentage_error 0.265815 0.257898 0.250641 0.251933 0.255471 0.256352 0.256352 Ridge identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [[667923.954879363], [614423.8176756387], [731... [[694094.2870407818], [617981.9933915635], [46... [[1023626.0773182933], [304085.70358412666], [... [[477416.1959023318], [492474.1026423527], [30... [[853485.3791880516], [574203.5276395974], [77... [[667923.954879363], [614423.8176756387], [731... [[667923.954879363], [614423.8176756387], [731... explained_variance_score 0.680109 0.70512 0.685333 0.693972 0.721308 0.697168 0.696102 mean_squared_error 42288266590.795799 39515834657.582001 51417532361.73008 37872817115.509499 33213482338.320721 40861586612.787613 40861586612.787621 mean_absolute_error 127934.770004 124362.619797 130426.951817 126064.678687 122549.911589 126267.786379 126267.786379 root_mean_squared_error 205641.11114 198785.901556 226754.343645 194609.396267 182245.6648 201607.283482 202142.490864 r2_score 0.680104 0.704998 0.684718 0.693795 0.721307 0.696984 0.696102 mean_absolute_percentage_error 0.265796 0.258152 0.250808 0.251901 0.255562 0.256444 0.256444 Lasso identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [667918.069528075, 614415.1560338883, 731173.9... [694081.9059207196, 617992.7838152755, 463279.... [1023674.2634054194, 304066.4170321575, 955604... [477415.1327845714, 492463.7396309793, 305057.... [853505.6953654164, 574205.7699892861, 771097.... [667918.069528075, 614415.1560338883, 731173.9... [667918.069528075, 614415.1560338883, 731173.9... explained_variance_score 0.680106 0.70512 0.685338 0.693969 0.721305 0.697168 0.696102 mean_squared_error 42288598880.705284 39515796822.947304 51416606662.838509 37873203587.611603 33213918899.943615 40861624970.809258 40861624970.809265 mean_absolute_error 127937.742862 124365.678646 130429.153983 126067.441635 122553.005403 126270.604506 126270.604506 root_mean_squared_error 205641.919075 198785.806392 226752.302442 194610.389208 182246.862524 201607.455928 202142.585743 r2_score 0.680101 0.704998 0.684723 0.693792 0.721303 0.696984 0.696102 mean_absolute_percentage_error 0.265806 0.258161 0.250816 0.25191 0.255571 0.256453 0.256453 ElasticNet identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [656773.6005291657, 614707.2479590292, 745914.... [685212.4217947914, 599436.3647276227, 483018.... [907890.7497500202, 369129.65691215475, 862824... [469736.2740988735, 539209.0261494196, 379320.... [804965.135403886, 571735.3944930851, 737716.4... [656773.6005291657, 614707.2479590292, 745914.... [656773.6005291657, 614707.2479590292, 745914.... explained_variance_score 0.65603 0.673131 0.638955 0.670126 0.695926 0.666834 0.664827 mean_squared_error 45475490348.208717 43800728719.55619 59005250118.409103 40812502573.376862 36239917129.190384 45066777777.748253 45066777777.748253 mean_absolute_error 124419.180393 122248.905619 131347.081356 124722.490988 120613.606068 124670.252885 124670.252885 root_mean_squared_error 213249.830828 209286.236336 242909.962987 202021.044877 190367.846889 211566.984383 212289.372739 r2_score 0.655993 0.673009 0.638191 0.670027 0.695912 0.666627 0.664827 mean_absolute_percentage_error 0.247878 0.245323 0.243383 0.244499 0.243172 0.244851 0.244851 DecisionTreeRegressor identifier fold 1 fold 2 fold 3 fold 4 fold 5 mean_cv oof_cv y_true [[945000.0], [352500.0], [560000.0], [500000.0... [[850000.0], [653000.0], [532000.0], [385000.0... [[1274950.0], [392137.0], [850000.0], [520000.... [[450000.0], [495000.0], [395000.0], [280000.0... [[754999.0], [588500.0], [525000.0], [525000.0... [[945000.0], [352500.0], [560000.0], [500000.0... [[945000.0], [352500.0], [560000.0], [500000.0... y_pred [915000.0, 300000.0, 316000.0, 643000.0, 28200... [535000.0, 630000.0, 410000.0, 335000.0, 72000... [1605000.0, 479000.0, 850000.0, 614000.0, 6505... [449000.0, 555000.0, 440000.0, 247500.0, 35000... [715000.0, 545000.0, 597000.0, 530000.0, 50000... [915000.0, 300000.0, 316000.0, 643000.0, 28200... [915000.0, 300000.0, 316000.0, 643000.0, 28200... explained_variance_score 0.752389 0.756145 0.770122 0.743783 0.767617 0.758011 0.758577 mean_squared_error 32751471474.735687 32664895113.423656 37510896623.824608 31693198492.168594 27707113797.312031 32465515100.292915 32465515100.292915 mean_absolute_error 101035.96819 99810.482071 105829.744939 101019.912377 96233.899075 100786.00133 100786.00133 root_mean_squared_error 180973.676193 180734.321902 193677.300229 178025.836586 166454.539732 179973.134928 180181.894485 r2_score 0.752246 0.756143 0.769991 0.743758 0.767511 0.75793 0.758546 mean_absolute_percentage_error 0.192566 0.186579 0.189517 0.183729 0.184638 0.187406 0.187406","title":"Define Metrics"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#comparison-of-cross-validated-models","text":"The point of the following comparison is to check how different models are performing across folds. More specifically, if we have 5 folds, we will have a metric score for each fold, subsequently, we can find the standard error of model's performance. We need to be aware of models that have high variance across folds in terms of the metrics performance. This can indicate that the model is highly unstable, and may be a sign of overfitting. def summarize_metrics ( metric_name ): ls = [] for model_name , inner_dict in model_dict . items (): folds = inner_dict [ \"identifier\" ][: - 2 ] all_obs = [] for idx , obs in enumerate ( inner_dict [ metric_name ][: - 2 ]): ls . append (( model_name , folds [ idx ], obs )) all_obs . append ( obs ) ls . append (( model_name , \"SE\" , np . std ( all_obs , ddof = 1 ) / len ( all_obs ) ** 0.5 )) fig , ax = plt . subplots ( figsize = ( 15 , 8 )) summary_df = pd . DataFrame ( ls , columns = [ \"model\" , \"fold\" , metric_name ]) # summary_df.to_csv _ = sns . boxplot ( x = \"model\" , y = metric_name , data = summary_df [( summary_df [ 'model' ] != 'DummyClassifier' ) & ( summary_df [ 'fold' ] != 'SE' )], ax = ax ) fig . savefig ( config . spot_checking_boxplot , format = 'png' , dpi = 300 ) return summary_df summary_df = summarize_metrics ( \"roc\" ) display ( summary_df . tail ( 12 ))","title":"Comparison of Cross-Validated Models"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#out-of-fold-confusion-matrix","text":"We do have information on the performance of each folds, we now look at the performance of all 5 folds together. Typicall there are two ways to do it, one is to simply take the average of the score of five folds, the other is to take a look at out of folds predictions. From the confusion matrix of the out of fold performance, Logistic Regression does seem to be a model we can explore on, although slightly lower in terms of overall AUROC score than SVC, it seems to have the quite low False Negatives amongst all. With further hyperparameter tuning and threshold optimization, we can make it better. model_names = [ model for model in model_dict . keys ()] fig , ax = plt . subplots ( 2 , 3 , figsize = ( 10 , 10 )) for axes , algo in zip ( ax . ravel (), model_names ): cf_mat = results_df . oof_cv [ algo ] . confusion_matrix #### scores auc = results_df . oof_cv [ algo ] . roc #### annotations labels = [ \"True Neg\" , \"False Pos\" , \"False Neg\" , \"True Pos\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in cf_mat . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in cf_mat . flatten () / np . sum ( cf_mat )] #### final annotations label = ( np . array ([ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )]) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = cf_mat , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = axes , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) axes . text ( 0 , - 0 , \" {} \" . format ( algo ), { \"size\" : 12 , \"color\" : \"black\" , \"weight\" : \"bold\" }) axes . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) axes . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( round ( auc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels axes . set_xticklabels ( \"\" ) axes . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Out Of Fold Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"This Visualization show the results of various classifiers and there respective results.\"\"\" , { \"size\" : 14 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) fig . savefig ( config . oof_confusion_matrix , format = 'png' , dpi = 300 )","title":"Out-of-Fold Confusion Matrix"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#hypothesis-testing-across-models","text":"I am slightly shocked at the performance of plain LogisticRegression, I decide to use an idea from Hypothesis Testing Across Models to check if the difference is really by chance or not. Note that I will be modifying his code as his code does not split using StratifiedKFold. The basic idea is to test if two model's difference in scores (in this case roc), is statistically significant or not. However, we note that this method may violate an assumption of Student's t test. Null Hypothesis \\(H_0\\) : The difference in the performance score of two classifiers is Statistically Significant. Alternate Hypothesis \\(H_1\\) : The difference in the performance score of two classifiers is not Statistically Significant. def paired_ttest_skfold_cv ( estimator1 , estimator2 , X , y , cv = 10 , scoring = None , shuffle = False , random_seed = None ): \"\"\"Modified from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/ttest.py to accomodate StratifiedKFold\"\"\" if not shuffle : skf = model_selection . StratifiedKFold ( n_splits = cv , shuffle = shuffle ) else : skf = model_selection . StratifiedKFold ( n_splits = cv , random_state = random_seed , shuffle = shuffle ) if scoring is None : if estimator1 . _estimator_type == \"classifier\" : scoring = \"accuracy\" elif estimator1 . _estimator_type == \"regressor\" : scoring = \"r2\" else : raise AttributeError ( \"Estimator must \" \"be a Classifier or Regressor.\" ) if isinstance ( scoring , str ): scorer = metrics . get_scorer ( scoring ) else : scorer = scoring score_diff = [] for train_index , test_index in skf . split ( X = X , y = y ): X_train , X_test = X [ train_index ], X [ test_index ] y_train , y_test = y [ train_index ], y [ test_index ] estimator1 . fit ( X_train , y_train ) estimator2 . fit ( X_train , y_train ) est1_score = scorer ( estimator1 , X_test , y_test ) est2_score = scorer ( estimator2 , X_test , y_test ) score_diff . append ( est1_score - est2_score ) avg_diff = np . mean ( score_diff ) numerator = avg_diff * np . sqrt ( cv ) denominator = np . sqrt ( sum ([( diff - avg_diff ) ** 2 for diff in score_diff ]) / ( cv - 1 )) t_stat = numerator / denominator pvalue = stats . t . sf ( np . abs ( t_stat ), cv - 1 ) * 2.0 return float ( t_stat ), float ( pvalue ) # check if difference between algorithms is real X_tmp = X_y_train [ predictor_cols ] . values y_tmp = X_y_train [ 'diagnosis' ] . values t , p = paired_ttest_skfold_cv ( estimator1 = classifiers [ 1 ], estimator2 = classifiers [ - 1 ], shuffle = True , cv = 5 , X = X_tmp , y = y_tmp , scoring = 'roc_auc' , random_seed = config . seed ) print ( 'P-value: %.3f , t-Statistic: %.3f ' % ( p , t )) Since P value is quite high, and more the basic threshold of 0.05 or 0.1, we fail to reject the null hypothesis, and say that there is no significant difference between these two models.","title":"Hypothesis Testing Across Models"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#model-selection-hyperparameter-tuning-with-gridsearchcv","text":"Hyperparameter Tuning: We have done a quick spot checking on algorithms and realized that LogisticRegression is doing well for this task. For this purpose, I will just perform hyperparameter tuning on this single algorithm. However, in practice and if resources are allowed, I will also tune other models such as RandomForest() , or gradient boosting algorithms such as XGBoost , as I believe they will perform no worse than our Logistic Regression model given the right hyperparameters. Grid Search: We will use an old-fashioned way to search for hyperparameters, which is brute force method. The time complexity of Grid Search is high and if you have many hyperparameters to tune, I recommend trying out Random Grid Search or libraries like Optuna that uses Bayesian Optimization. def make_finetuning_pipeline ( model ): \"\"\"Make a Pipeline for Training. Args: model ([type]): [description] Returns: [type]: [description] \"\"\" steps = list () # standardization steps . append (( 'standardize' , preprocessing . StandardScaler ())) # reduce VIF steps . append (( 'remove_multicollinearity' , ReduceVIF ( thresh = 10 ))) # the model steps . append (( 'model' , model )) # create pipeline _pipeline = pipeline . Pipeline ( steps = steps ) return _pipeline Reconstruct our pipeline but now only taking in LogisticRegression . pipeline_logistic = make_finetuning_pipeline ( linear_model . LogisticRegression ( solver = \"saga\" , random_state = config . seed , max_iter = 10000 , n_jobs = None , fit_intercept = True ) ) Define our search space for the hyperparameters: param_grid = { model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 )} param_grid = dict ( model__penalty = [ \"l1\" , \"l2\" ], model__C = np . logspace ( - 4 , 4 , 10 ), ) Run our hyperparameter search with cross-validation. For example, our param_grid has \\(2 \\times 10 = 20\\) combinations, and our cross validation has 5 folds, then there will be a total of 100 fits. Below details the pseudo code of what happens under the hood: Define \\(G\\) as the set of combination of hyperparamters. Define number of splits to be \\(K\\) . For each set of hyperparameter \\(z \\in Z\\) : for fold \\(j\\) in K: Set \\(F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}\\) Set \\(F_{\\text{val}} = F_{j}\\) as the validation set Perform Standard Scaling on \\(F_{\\text{train}}\\) and find the mean and std Perform VIF recursively on \\(F_{\\text{train}}\\) and find the selected features Transform \\(F_{\\text{val}}\\) using the mean and std found using \\(F_{\\text{train}}\\) Transform \\(F_{\\text{val}}\\) to have only the selected features from \\(F_{\\text{train}}\\) Train and fit on \\(F_{\\text{train}}\\) Evaluate the fitted parameters on \\(F_{\\text{val}}\\) to obtain \\(\\mathcal{M}\\) grid = model_selection . GridSearchCV ( pipeline_logistic , param_grid = param_grid , cv = 5 , refit = True , verbose = 3 , scoring = \"roc_auc\" ) _ = grid . fit ( X_train , y_train ) We can save our results in a dataframe, we will also look at the top performing hyperparameter by querying the below: grid_cv_df = pd . DataFrame ( grid . cv_results_ ) grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ] grid_cv_df = pd . DataFrame ( grid . cv_results_ ) best_cv = grid_cv_df . loc [ grid_cv_df [ 'rank_test_score' ] == 1 ] display ( best_cv ) best_hyperparams = grid . best_params_ print ( f \"Best Hyperparameters found is { best_hyperparams } \" ) Our best performing set of hyperparameters {'model__C': 0.3593813663804626, 'model__penalty': 'l2'} gives rise to a mean cross validation score of \\(0.988739\\) , which is higher than the model with default hyperparameter scoring, \\(0.987136\\) . Room for Improvement: Apart from the other methods to search for the optimal hyperparameters, we can also include preprocessing step as a tunable hyperparameter. More specifically, in our ReduceVIF() step, we hard coded two manual criterion in which the algorithm will stop; if the threshold reaches 10, or if the number of features removed hit 20; we can include them in the search space.","title":"Model Selection: Hyperparameter Tuning with GridSearchCV"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#retrain-on-the-whole-training-set","text":"A common practice after the hyperparameter tuning phase is to retrain the model on the whole dataset \\(X_{\\text{train}}\\) where we will get the estimator's coefficients obtained from the retraining. This is actually already done as the scikit-learn's GridSearchCV has a parameter refit ; if we select it to be true, then after the model selection process is done (i.e. getting the best hyperparameters after cross validation with grid search), the grid search object will retrain on the whole \\(X_{\\text{train}}\\) with the best hyperparameters internally, and return us back an object in which we can call predict etc.","title":"Retrain on the whole training set"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#retrain-using-optimal-hyperparameters","text":"However, to be extra careful, we can retrain manually using the best hyperparameters and check if scikit-learn is true to its documentation. We will just reconstruct the pipeline using the grid's best hyper parameters. We will then test if the retrained model's coefficients coincide with the grid's best estimator's coefficients. If there difference is 0, this means they are trained under the same circumstances and we can be sure that the refit parameter is behaving true to its words. grid_best_hyperparams = grid . best_params_ print ( grid_best_hyperparams ) -> { 'model__C' : 0.3593813663804626 , 'model__penalty' : 'l2' } retrain_pipeline = pipeline . Pipeline ( [ ( \"standardize\" , preprocessing . StandardScaler ()), ( 'remove_multicollinearity' , ReduceVIF ( thresh = 10 )), ( \"model\" , linear_model . LogisticRegression ( C = 0.3593813663804626 , max_iter = 10000 , random_state = 1992 , solver = \"saga\" , penalty = \"l1\" ), ), ] ) _ = retrain_pipeline . fit ( X_train , y_train ) coef_diff = retrain_pipeline [ 'model' ] . coef_ - grid . best_estimator_ [ 'model' ] . coef_ print ( \"...\" ) assert np . all ( coef_diff == 0 ) == True print ( \"Retraining Assertion Passed!\" )","title":"Retrain using optimal hyperparameters"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#interpretation-of-results","text":"","title":"Interpretation of Results"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#interpretation-of-coefficients","text":"As shown in the figure below, all else being equal, for every square unit increase in mean cell area, the odds of the tumor being malignant increases by a factor of \\(e^{1.43} = 4.19\\) . The variation (standard error) of the characteristics of cells also are deemed important by the model, for example, area se played an important role in determining whether a cell is malignant; intuitively, if some cells are noticably larger than the rest, then it is also a good indicator of malignancy. selected_features_by_vif_index = grid . best_estimator_ [ 'remove_multicollinearity' ] . column_indices_kept_ selected_feature_names = np . asarray ( predictor_cols )[ selected_features_by_vif_index ] selected_features_coefficients = grid . best_estimator_ [ 'model' ] . coef_ . flatten () # assertion #assert grid.best_estimator_['remove_multicollinearity'].feature_names_ == retrain_pipeline['remove_multicollinearity'].feature_names_ fig , ax = plt . subplots ( figsize = ( 15 , 15 )) # .abs() _ = pd . Series ( selected_features_coefficients , index = selected_feature_names ) . sort_values () . plot ( ax = ax , kind = 'barh' ) fig . savefig ( config . feature_importance , format = \"png\" , dpi = 300 )","title":"Interpretation of Coefficients"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#interpretation-of-metric-scores-on-train-set","text":"We are also interested in choosing an optimal threshold for the model such that it gives the lowest recall, or False Negatives. We note that the default threshold when calling predict() from a model is \\(0.5\\) . In this section, we will explore one way to get the best tradeoff we can when choosing a high recall, while maintaining a reasonable score for precision. def evaluate_train_test_set ( estimator : Callable , X : Union [ pd . DataFrame , np . ndarray ], y : Union [ pd . DataFrame , np . ndarray ] ) -> Dict [ str , Union [ float , np . ndarray ]]: \"\"\"This function takes in X and y and returns a dictionary of scores. Args: estimator (Callable): [description] X (Union[pd.DataFrame, np.ndarray]): [description] y (Union[pd.DataFrame, np.ndarray]): [description] Returns: Dict[str, Union[float, np.ndarray]]: [description] \"\"\" test_results = {} y_pred = estimator . predict ( X ) # This is the probability array of class 1 (malignant) y_prob = estimator . predict_proba ( X )[:, 1 ] test_brier = metrics . brier_score_loss ( y , y_prob ) test_roc = metrics . roc_auc_score ( y , y_prob ) test_results [ \"brier\" ] = test_brier test_results [ \"roc\" ] = test_roc test_results [ \"y\" ] = np . asarray ( y ) . flatten () test_results [ \"y_pred\" ] = y_pred . flatten () test_results [ \"y_prob\" ] = y_prob . flatten () return test_results def plot_precision_recall_vs_threshold ( precisions , recalls , thresholds ): \"\"\" Modified from: Hands-On Machine learning with Scikit-Learn and TensorFlow; p.89 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( \"Precision and Recall Scores as a function of the decision threshold\" ) plt . plot ( thresholds , precisions [: - 1 ], \"b--\" , label = \"Precision\" ) plt . plot ( thresholds , recalls [: - 1 ], \"g-\" , label = \"Recall\" ) plt . ylabel ( \"Score\" ) plt . xlabel ( \"Decision Threshold\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . precision_recall_threshold_plot , format = \"png\" , dpi = 300 ) def plot_roc_curve ( fpr , tpr , label = None ): \"\"\" The ROC curve, modified from Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91 and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" plt . figure ( figsize = ( 8 , 8 )) plt . title ( 'ROC Curve' ) plt . plot ( fpr , tpr , linewidth = 2 , label = label ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . axis ([ - 0.005 , 1 , 0 , 1.005 ]) plt . xticks ( np . arange ( 0 , 1 , 0.05 ), rotation = 90 ) plt . xlabel ( \"False Positive Rate\" ) plt . ylabel ( \"True Positive Rate (Recall)\" ) plt . legend ( loc = 'best' ) plt . savefig ( config . roc_plot , format = \"png\" , dpi = 300 ) def adjusted_classes ( y_scores , t ): \"\"\" This function adjusts class predictions based on the prediction threshold (t). Will only work for binary classification problems. and courtesy of https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 \"\"\" return [ 1 if y >= t else 0 for y in y_scores ] The plots below show the tradeoffs between precision and recall, recall and false positive rate. The confusion matrix on the train set tells us that there is still more false negatives than false positives. We can choose a particular threshold in order to minimize false negatives, at some expense of false positive. train_results = evaluate_train_test_set ( grid , X_train , y_train ) fig , ax = plt . subplots ( figsize = ( 10 , 10 )) # CM cm_train = metrics . confusion_matrix ( train_results [ 'y' ], train_results [ 'y_pred' ]) #### scores auc = metrics . roc_auc_score ( train_results [ 'y' ], train_results [ 'y_prob' ]) #### annotations labels = [ \"True Neg\" , \"False Pos\" , \"False Neg\" , \"True Pos\" ] counts = [ \" {0:0.0f} \" . format ( value ) for value in cm_train . flatten ()] percentages = [ \" {0:.2%} \" . format ( value ) for value in cm_train . flatten () / np . sum ( cm_train )] #### final annotations label = ( np . array ([ f \" { v1 } \\n { v2 } \\n { v3 } \" for v1 , v2 , v3 in zip ( labels , counts , percentages )]) ) . reshape ( 2 , 2 ) # heatmap sns . heatmap ( data = cm_train , vmin = 0 , vmax = 330 , cmap = [ \"#fe4a49\" , \"#2ab7ca\" , \"#fed766\" , \"#59981A\" ], linewidth = 2 , linecolor = \"white\" , square = True , ax = ax , annot = label , fmt = \"\" , cbar = False , annot_kws = { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" , \"alpha\" : 0.8 }, alpha = 1 , ) ax . scatter ( 1 , 1 , s = 3500 , c = \"white\" ) ax . text ( 0.72 , 1.0 , \"AUC: {} \" . format ( round ( auc , 3 )), { \"size\" : 10 , \"color\" : \"black\" , \"weight\" : \"bold\" }, ) ## ticks and labels ax . set_xticklabels ( \"\" ) ax . set_yticklabels ( \"\" ) ## titles and text fig . text ( 0 , 1.05 , \"Train Set Confusion Matrix\" , { \"size\" : 22 , \"weight\" : \"bold\" }, alpha = 1 ) fig . text ( 0 , 1 , \"\"\"Training Set Confusion Matrix.\"\"\" , { \"size\" : 12 , \"weight\" : \"normal\" }, alpha = 0.98 , ) fig . tight_layout ( pad = 2.5 , w_pad = 2.5 , h_pad = 2.5 ) fig . savefig ( config . final_train_confusion_matrix , format = 'png' , dpi = 300 ) # generate the precision recall curve precision , recall , pr_thresholds = metrics . precision_recall_curve ( train_results [ 'y' ], train_results [ 'y_prob' ]) fpr , tpr , roc_thresholds = metrics . roc_curve ( train_results [ 'y' ], train_results [ 'y_prob' ], pos_label = 1 ) # use the same p, r, thresholds that were previously calculated plot_precision_recall_vs_threshold ( precision , recall , pr_thresholds ) Based on the tradeoff plot above, a good threshold can be set at \\(t = 0.35\\) , let us see how it performs with this threshold. y_pred_adj = adjusted_classes ( train_results [ \"y_prob\" ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( train_results [ \"y\" ], y_pred_adj ), columns = [ \"pred_neg\" , \"pred_pos\" ], index = [ \"neg\" , \"pos\" ], ) ) print ( metrics . classification_report ( y_true = train_results [ \"y\" ], y_pred = y_pred_adj )) train_brier = train_results [ 'brier' ] print ( f \"train brier: { train_brier } \" ) The False Negatives reduced from 15 to 9, at the expense of increase False Positives from 6 to 14. We should take comfort that less patients are falsely classified as benign when in fact they need treatment. This is a tradeoff that we have to choose. The ROC curve below also paints a similar story, in order for you to have high recall, one must accept that there will more False Positives. plot_roc_curve ( fpr , tpr , 'recall_optimized' )","title":"Interpretation of Metric Scores on Train Set"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#evaluation-on-test-set","text":"Ultimately, we are interested in finding the estimate of the generalization error of the model, and thus it is time for us to evaluate our model's performance on the \"unseen\" test set \\(X_{\\text{test}}\\) to get a good gauge on how well the model generalizes to unseen data. Take note that now everything has settled, we will use the exact hyperparameters to predict on test set, with the pre-defined threshold of 0.35. test_results = evaluate_train_test_set ( grid , X_test , y_test ) y_test_pred_adj = adjusted_classes ( test_results [ 'y_prob' ], t = 0.35 ) print ( pd . DataFrame ( metrics . confusion_matrix ( test_results [ 'y' ], y_test_pred_adj ), columns = [ 'pred_neg' , 'pred_pos' ], index = [ 'neg' , 'pos' ])) test_roc = test_results [ 'roc' ] test_brier = test_results [ 'brier' ] print ( test_roc ) print ( test_brier ) print ( metrics . classification_report ( y_true = test_results [ \"y\" ], y_pred = y_test_pred_adj )) Using the same threshold we used on training set, we see that the False Negative is quite low. The overall ROC score is 0.9828, and the corresponding Brier Score is 0.04136, both seem reasonably well performing.","title":"Evaluation on Test Set"},{"location":"supervised_learning/regression/house_sales_in_king_country_usa/#bias-variance-tradeoff","text":"avg_expected_loss , avg_bias , avg_var = bias_variance_decomp ( grid . best_estimator_ [ 'model' ], X_train . values , y_train . values , X_test . values , y_test . values , loss = '0-1_loss' , random_seed = 123 ) print ( 'Average expected loss: %.3f ' % avg_expected_loss ) print ( 'Average bias: %.3f ' % avg_bias ) print ( 'Average variance: %.3f ' % avg_var ) We use the mlxtend library to estimate the Bias-Variance Tradeoff in our Logistic Regression model. The core idea behind this function is to use bagging and repeatedly sample from our training set so as to simulate that we are actually drawing samples from the \"true\" population over a distribution \\(\\mathcal{P}\\) . As expected, Logistic Regression being a linear model, its simplicity contributes to its high bias and low variance.","title":"Bias-Variance Tradeoff"}]}